[["index.html", "Advanced Data Skills, Open Science and Reproducibility 2024/25 Welcome", " Advanced Data Skills, Open Science and Reproducibility 2024/25 Welcome Unit Lead: George Farmer TA: Lana Bojanić Welcome to the Advanced Data Skills, Open Science and Reproducibility M.Res. unit PCHN63101. This unit will be taught via a flipped classroom model. Make sure you go through the content on this site in advance of the in-person sessions. These are held on Mondays from 1 to 3pm in the Engineering A building (also known as the Nancy Rothwell building) in room room 3A.012. For questions about the course you can contact me via email (george.farmer@manchester.ac.uk). If you have any questions or technical problems relating to each week’s content, please post these on the Blackboard discussion board. There are two assignments associated with this unit. The full details of these (plus hand in dates) can be found on the Blackboard page for this unit. "],["open-science.html", "Workshop 1 Open Science 1.1 Open Research and Reproducibility 1.2 Experimental Power 1.3 Open Source Software", " Workshop 1 Open Science In this workshop I will first introduce you to the key concepts in open research, and talk about the so-called “replication crisis” in the Psychological, Biomedical, and Life Sciences that has resulted in the Open Research movement. I will also discuss the importance of adopting reproducible research practices in your own research, and provide an introduction to various tools and processes you can incorporate into your own research workflows that will allow you to conduct reproducible research. Like the other workshops in this series, this one involves a mix of recorded videos, narrative, and links to various resources for you to explore and to read. 1.1 Open Research and Reproducibility First I’d like you to watch the following video on the history of the replication crisis and the issues which have motivated a move towards the adoption of open and reproducible research practices. The video covers the so-called replication crisis in the biomedical sciences, issues around open research, and summarise some of the initiatives (including the UK Reproducibility Network) that have been established to address the fundamental problems around open research, transparency, and reproducibility in science.       Before watching the next video, please have a read through the following paper by Ioannidis (2005) which arguably started the conversation around reproducibility that has had such an impact on research in Psychology and across the Biomedical sciences for the last few years. Clicking on the image below will take you to the paper. Ioannidis (2005)       Bishop (2019) This post by Dorothy Bishop in 2019 nicely captures the situation a number of years later. Clicking on the image below will take you to the paper.    1.1.1 How to do Reproducible Research One of the biggest challenges facing researchers who are used to the old way of conducting research is that they feel that they don’t have the knowledge or technical skills to adopt open and reproducible research practices. But it’s not that hard! Before you run your experiment, you can pre-register your hypotheses so that when you come to analyse and write-up your results, you can demonstrate that your predictions really were made in advance of data collection. You can also make your research data open (and FAIR) alongside your code so that others can recreate your analyses. And by posting your research article on a pre-print server (such as PsyArXiv or bioRxiv) before submission to a journal, you make your research findings available to all. The adoption of open source software such as R, also means that any research findings you produce can be re-produced by others who can access your data and code. This principle of using open tools to allow us to produce open (and reusable) data and code is the fundamental philosophy behind all of the workshops in this unit. Take the time to read the following paper, it’s a great guide to the various things you can do to make your own research more open. Just click on the image to open the paper.    Haeffel (2022) This thought provoking paper by Gerald Haeffel suggests that psychology needs to focus more on theory development (and encourage the publication of results that refute theories). 1.2 Experimental Power We are now going to look at the issues around experimental power (and why it is important). One of the insights revealed by the “replication crisis” is that very often research is underpowered for the effect size of interest (i.e., even if the effect is there, your experiment is unlikely to find it). Even when underpowered studies do reveal the effect of interest, the effect size itself will be over-estimated (thus causing problems for future work that might base their power estimates on this incorrect effect size estimate). One solution to the challenge is to conduct data simulation as part of the experimental design process. There are many ways to do this using R, and there are several packages on CRAN (the Comprehensive R Archive Network) that provide functions to simulate data for different kinds of designs.       If you’re interested in reading more about power, you might like to take a look at this classic “Power Primer” paper by Jacob Cohen.    1.3 Open Source Software If you want to produce open and reproducible research, you should be using open source software in your workflow. Research produced using proprietary software cannot be easily reproduced by others. Open source software is software that is licensed to be free to modify, remix, and improve. It is usually free to use, and is centred on the principles of open exchange, collaborative participation, rapid prototyping, transparency, meritocracy, and community-oriented development. The move towards open source began in the early 1980s partly because of a printer and developed further that decade in the form of the Free Software Foundation established by Richard Stallman. In the late 1990s, the Open Source Initiative was launched to raise awareness and adoption of open source software, and build bridges between open source communities of practice. Open source software is made by many people and distributed under an OSD-compliant license which grants all the rights to use, study, change, and share the software in modified and unmodified form. Software freedom is essential to enabling community development of open source software. There is a huge amount of open source software available - some of which you will find useful both in the context of this unit, but also in the context of how you study, and in how you conduct your research. Below is an interesting CNBC video discussing the rise of Open Source Software - it ends with mention of the need to collaborate in an open manner on global challenges such as the environment, cancer, and Alzheimer’s disease.       1.3.1 Statistical and Scientific Computing R and RStudio Desktop It goes without saying that R and RStudio Desktop are the two most obvious examples of open source software that are relevant to this course. In terms of other open source languages used for data analysis and statistical modelling, you might also be interested in Python and Julia.              Python While R tends to be the go-to language for people who are interested in data wrangling, data visualisation, and statistical modelling, Python is arguably the ‘better’ language in the sense it is more general purpose. Python is used widely by the machine learning community (to name just one example). Octave You may have heard of - or even used - MATLAB for numerical computing. There is an open source equivalent, called GNU Octave, that you may be interested in checking out. 1.3.2 Document Creation Up until this time you’ve probably mainly used Microsoft Word for writing documents. LibreOffice is a great open source equivalent to the Microsoft Office suite and offers a huge range of applications for document writing, working with spreadsheets, and the creation of presentations. If you’re interested in writing using Markdown (which is really easy to get to grips with), you might be interested in using HackMD. HackMD is a Markdown editor that critically allows you to write collaborative documents and presentations with others. 1.3.3 Building Experiments PsychoPy offers a great open source solution to build experiments to collect human data, and via the companion hosting site Pavlovia provides an easy to use method for running the PsychoPy experiments online. PsychoPy has been around for a number of years and has lots of pre-built experiment templates that you can adapt as needs be. There is a great reference that describes the PsychoPy environment here.              1.3.4 Linux One of the most widely used pieces of open source software is the Linux operating system. Rather than running your computer on Windows, or Mac OS, you could choose to run it using Linux. Linux runs the majority of the internet servers in world is becoming increasingly popular in academic settings. Linux refers to a bunch of open source Unix-like operating systems. It was developed and released by Linus Torvalds in 1991. Some of the most popular distributions of Linux are Ubuntu, Fedora, and Debian. If you really want to get into the computational side of research, it’s important to discover the world of Linux. 1.3.5 More Reading If you’re interested in other examples of open source software, you might be interested in having a look at this list of open source alternatives on the Tech Radar site. End of workshop 1 materials "],["r-and-rstudio.html", "Workshop 2 R and RStudio 2.1 Why R? 2.2 Getting Started 2.3 Keeping Things Organised 2.4 Good Coding Style 2.5 Your First R Script", " Workshop 2 R and RStudio 2.1 Why R? In this workshop we will cover the language, R, and RStudio Desktop, the integrated development environment (IDE) that you will use to write reproducible code involving the wrangling, visualization, summary, and statistical modelling of your data. Both R and RStudio Desktop are examples of Open Source Software. The video below introduces you to R and talks about the importance of adopting such tools in our research analysis workflows.       Below is a video of a great talk by J.J. Allaire, entrepreneur and founder of RStudio (and other organizations). This video is from rstudio::conf 2020 and in it J.J. talks about his journey from being a Political Scientist, how he got involved in R, and the importance of Open Source in the context of Reproducible Data Science. If you click on the image, you’ll be taken to the RStudio website where you can watch the recording. If you’re interested, you might like to look at some of the other videos on the RStudio site.       2.2 Getting Started In this next video you’ll see how to install R (the language) and RStudio Desktop (the IDE for working with the language). You can download R from here for a variety of platforms including Mac OS, Windows, and Ubuntu. To download free RStudio Desktop just go to here. If you are using a Chromebook, or have a tablet, or are having difficulties installing R and RStudio Desktop on your computer, you can use Posit cloud to run the RStudio environment in your browser. You’ll need to sign up - there is a free plan available.       2.3 Keeping Things Organised When you are doing data analysis using R, it’s important to use a sensible structure for your folders and files. As you saw in the video above, creating a new project with a .Rproj file is the easiest way to this is using RStudio Desktop. Good file management is as important as good coding style (which we’ll come to next). There’s a great paper on project management that you can read by clicking the image below.          2.4 Good Coding Style In the following video you will learn a little about good coding style. It’s important when you’re writing analysis scripts that your code is understandable by others, and by future you. If you get into the habit of good coding style early on, it will make things a lot easier in the long run - and you’ll find it easier to work collaboratively as others will find it easier to work with you.       You can have a look at the helpful Tidyverse Style Guide here. If you want to make your code and data open (and you really should unless there’s a good reason not to do so), it’s important to license it properly to allow others to (re)use and remix it. It’s often good to use the most permissive license that you can. Some good licenses are the MIT License, and the Creative Commons License CC-BY 4.0. You can use this handy guide if you need help choosing the right license for your own work.    2.5 Your First R Script You’re now going to run your first R script. We will create three visualisations of UFO sightings in the US using a database of more than 80,000 UFO sightings over the years. Before you run the code, you will need to install two packages onto your computer - they are tidyverse and patchwork.       Once you have installed the packages, paste the following code into a new R script. Run the code in the same way shown in the video. Does your visualisation look like the one in the video? library(tidyverse) # load the tidyverse library(patchwork) # needed to combine our 4 plots at the end # read in data ufo_sightings &lt;- read_csv(&quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-06-25/ufo_sightings.csv&quot;) # plot of top 10 US states with number of sightings in each state plot1 &lt;- ufo_sightings %&gt;% filter(!is.na(state)) %&gt;% mutate(state = str_to_upper(state)) %&gt;% group_by(state) %&gt;% tally() %&gt;% top_n(10) %&gt;% ggplot(aes(x = reorder(state, n), y = n, fill = state)) + geom_col() + coord_flip() + guides(fill = &quot;none&quot;) + labs(title = &quot;Top 10 States for UFO Sightings&quot;, x = NULL, y = NULL) + ylim(0, 11000) + theme_minimal() + theme(text = element_text(size = 15)) # work out states within lat and long limits (i.e., exclude Alaska) tidied_ufo &lt;- ufo_sightings %&gt;% filter(country == &quot;us&quot;) %&gt;% filter(latitude &gt; 24 &amp; latitude &lt; 50) # plot all sightings on a map of the US plot2 &lt;- tidied_ufo %&gt;% ggplot(aes(x = longitude, y = latitude)) + geom_point(size = .5, alpha = .25) + theme_void() + coord_cartesian() + labs(title = &quot;Sites of UFO Sightings in the US&quot;) + theme(text = element_text(size = 15)) # plot of top 10 UFO shapes spotted in California plot3 &lt;- tidied_ufo %&gt;% filter(state == &quot;ca&quot;) %&gt;% filter(ufo_shape != &quot;other&quot;) %&gt;% filter(ufo_shape != &quot;unknown&quot;) %&gt;% group_by(ufo_shape) %&gt;% tally() %&gt;% top_n(10) %&gt;% mutate(ufo_shape = str_to_title(ufo_shape)) %&gt;% ggplot(aes(x = reorder(ufo_shape, n), y = n, fill = ufo_shape)) + geom_col() + coord_flip() + guides(fill = &quot;none&quot;) + labs(title = &quot;Top 10 UFO Shapes spotted in California&quot;, x = NULL, y = NULL) + theme_minimal() + theme(text = element_text(size = 15)) # Put plots together my_plot &lt;- (plot1 + plot3) / (plot2) ggsave(&quot;ufo_plot.jpg&quot;, plot = my_plot, width = 12, height = 10) End of workshop 2 materials "],["data-wrangling-and-data-summarising.html", "Workshop 3 Data Wrangling and Data Summarising 3.1 Wrangling your data 3.2 Summarising your data", " Workshop 3 Data Wrangling and Data Summarising In this workshop we shall take our first look at some key tools in the Tidyverse that will allow us to wrangle and tidy our data so that it’s in the format that we need in order to visualize and model it. By making our data wrangling reproducible (i.e., by coding it in R), we can easily re-run this stage of our analysis pipeline as new data gets added. Reproducibility of the data wrangling stage is a key part of the analysis process and often gets overlooked in terms of needing to ensure it is reproducible. The Tidyverse is a collection of packages that all ‘play nicely’ with each other. They are based on a common philosophy where data are represented in rectangular format (i.e., with rows and columns). These rectangular structures are known in the Tidyverse as tibbles. If you’re interested, you can read more about tibbles in the R4DS book here. 3.1 Wrangling your data Have a look at the following video where I walk you through this worksheet. Then I want you to work through the content by writing (and running) the script on your own machine.       3.1.1 Loading the Tidyverse Let’s take our first look at data wrangling. We are going to start with a dataset that comes with the Tidyverse. The dataset is called mpg and comprises fuel economy data from 1999 to 2008 for 38 popular models of cars in the US. First, we need to load the tidyverse library with the following: library(tidyverse) If you run this line without having first installed the Tidyverse on your computer, you will encounter an error. R packages only need to be installed once, so if you want to load one into your library for the first time, you need to install it with install.packages(*packagename*). For the tidyverse we need to install it with: install.packages(&quot;tidyverse&quot;) Once you have installed the tidyverse, you can then load it into your llbrary with the library() function. You only ever need to install a package once on your machine (unless you have updated R or you want to install the most up-to-date version of a particular package). When you are writing your R scripts, you never want to have the install.packages() function in the body of the script as if someone else were to run your script, this would update packages on their computer (which they might not want). The mpg dataset The mpg dataset is loaded as part of the Tidyverse. In the help file, which you can access by typing help(mpg) or ?mpg we see the following: Description This dataset contains a subset of the fuel economy data that the EPA makes available on http://fueleconomy.gov. It contains only models which had a new release every year between 1999 and 2008 - this was used as a proxy for the popularity of the car. A data frame with 234 rows and 11 variables. manufacturer - manufacturer model - model name displ - engine displacement, in litres year - year of manufacture cyl - number of cylinders trans -type of transmission drv -f = front-wheel drive, r = rear wheel drive, 4 = 4wd cty - city miles per gallon hwy - highway miles per gallon fl - fuel type class - “type” of car Note you can also use help to inspect functions, for example: Typing ?sd will show you the documentation for R’s Standard Deviation function. 3.1.2 Using head() and str() We can explore the mpg dataset that is loaded with the Tidyverse in a number of ways. If we want to look at the first 6 lines of the dataset, we can use the head() function. head(mpg) ## # A tibble: 6 × 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 audi a4 1.8 1999 4 auto(l5) f 18 29 p compa… ## 2 audi a4 1.8 1999 4 manual(m5) f 21 29 p compa… ## 3 audi a4 2 2008 4 manual(m6) f 20 31 p compa… ## 4 audi a4 2 2008 4 auto(av) f 21 30 p compa… ## 5 audi a4 2.8 1999 6 auto(l5) f 16 26 p compa… ## 6 audi a4 2.8 1999 6 manual(m5) f 18 26 p compa… We see that it is a tibble - or a rectangular data frame - made up of rows and columns. This is tidy format where each observation corresponds to a row. Most of the analyses we will run in R involve tidy data. Within the Tidyverse, the tibble is the standard way to represent data. You’ll spend a lot of time tidying and wrangling your data to get it into this format! By doing this in R using a script that you write, you are making this key stage reproducible. You can run the script again on an updated or different dataset - thus likely saving you lots of time! We can also ask for information about the structure of our dataset with str(). This will tell us about the columns, what type of variable each is, the number of rows etc. str(mpg) ## tibble [234 × 11] (S3: tbl_df/tbl/data.frame) ## $ manufacturer: chr [1:234] &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; ... ## $ model : chr [1:234] &quot;a4&quot; &quot;a4&quot; &quot;a4&quot; &quot;a4&quot; ... ## $ displ : num [1:234] 1.8 1.8 2 2 2.8 2.8 3.1 1.8 1.8 2 ... ## $ year : int [1:234] 1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 ... ## $ cyl : int [1:234] 4 4 4 4 6 6 6 4 4 4 ... ## $ trans : chr [1:234] &quot;auto(l5)&quot; &quot;manual(m5)&quot; &quot;manual(m6)&quot; &quot;auto(av)&quot; ... ## $ drv : chr [1:234] &quot;f&quot; &quot;f&quot; &quot;f&quot; &quot;f&quot; ... ## $ cty : int [1:234] 18 21 20 21 16 18 18 18 16 20 ... ## $ hwy : int [1:234] 29 29 31 30 26 26 27 26 25 28 ... ## $ fl : chr [1:234] &quot;p&quot; &quot;p&quot; &quot;p&quot; &quot;p&quot; ... ## $ class : chr [1:234] &quot;compact&quot; &quot;compact&quot; &quot;compact&quot; &quot;compact&quot; ... 3.1.3 Use select() to select columns If we want to, we could just select one of the columns using the select() function. Below we are just selecing the column entitled manufacturer. mpg %&gt;% select(manufacturer) ## # A tibble: 234 × 1 ## manufacturer ## &lt;chr&gt; ## 1 audi ## 2 audi ## 3 audi ## 4 audi ## 5 audi ## 6 audi ## 7 audi ## 8 audi ## 9 audi ## 10 audi ## # ℹ 224 more rows Related to the select() function is rename(). It does exactly what you think it might; it renames a column. We can also look at the different car manufacturers in the dataset by using the distinct() function. This gives us the unique manufacturer names. This function can be quite handy if you want to check a dataset for duplicates of (e.g.) participant IDs. mpg %&gt;% distinct(manufacturer) ## # A tibble: 15 × 1 ## manufacturer ## &lt;chr&gt; ## 1 audi ## 2 chevrolet ## 3 dodge ## 4 ford ## 5 honda ## 6 hyundai ## 7 jeep ## 8 land rover ## 9 lincoln ## 10 mercury ## 11 nissan ## 12 pontiac ## 13 subaru ## 14 toyota ## 15 volkswagen 3.1.4 Use filter() to select rows Sometimes we might want to select only a subset of rows in our dataset. We can do that using the filter() function. For example, here we filter our dataset to include only cars made by ‘honda’. mpg %&gt;% filter(manufacturer == &quot;honda&quot;) ## # A tibble: 9 × 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 honda civic 1.6 1999 4 manual(m5) f 28 33 r subco… ## 2 honda civic 1.6 1999 4 auto(l4) f 24 32 r subco… ## 3 honda civic 1.6 1999 4 manual(m5) f 25 32 r subco… ## 4 honda civic 1.6 1999 4 manual(m5) f 23 29 p subco… ## 5 honda civic 1.6 1999 4 auto(l4) f 24 32 r subco… ## 6 honda civic 1.8 2008 4 manual(m5) f 26 34 r subco… ## 7 honda civic 1.8 2008 4 auto(l5) f 25 36 r subco… ## 8 honda civic 1.8 2008 4 auto(l5) f 24 36 c subco… ## 9 honda civic 2 2008 4 manual(m6) f 21 29 p subco… Note, we use the operator == which means ‘is equal to’. This is a logical operator - other logical operators include less than &lt;, greater than &gt;, less than or equal to &lt;=, greater then or equal to &gt;=, and is not equal to !=. We can also filter using a combination of possibilities via logical OR | or logical AND &amp;. The first code chunk below filters the dataset for cases where the manufacturer is ‘honda’ OR ‘toyota’. mpg %&gt;% filter(manufacturer == &quot;honda&quot; | manufacturer == &quot;toyota&quot;) ## # A tibble: 43 × 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 honda civic 1.6 1999 4 manu… f 28 33 r subc… ## 2 honda civic 1.6 1999 4 auto… f 24 32 r subc… ## 3 honda civic 1.6 1999 4 manu… f 25 32 r subc… ## 4 honda civic 1.6 1999 4 manu… f 23 29 p subc… ## 5 honda civic 1.6 1999 4 auto… f 24 32 r subc… ## 6 honda civic 1.8 2008 4 manu… f 26 34 r subc… ## 7 honda civic 1.8 2008 4 auto… f 25 36 r subc… ## 8 honda civic 1.8 2008 4 auto… f 24 36 c subc… ## 9 honda civic 2 2008 4 manu… f 21 29 p subc… ## 10 toyota 4runner 4… 2.7 1999 4 manu… 4 15 20 r suv ## # ℹ 33 more rows While below we filter for cases where the manufacturer is ‘honda’ and the year of manufacture is ‘1999’. mpg %&gt;% filter(manufacturer == &quot;honda&quot; &amp; year == &quot;1999&quot;) ## # A tibble: 5 × 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 honda civic 1.6 1999 4 manual(m5) f 28 33 r subco… ## 2 honda civic 1.6 1999 4 auto(l4) f 24 32 r subco… ## 3 honda civic 1.6 1999 4 manual(m5) f 25 32 r subco… ## 4 honda civic 1.6 1999 4 manual(m5) f 23 29 p subco… ## 5 honda civic 1.6 1999 4 auto(l4) f 24 32 r subco… Combining functions We can combine the use of filter() with select() to filter for case where the manufacturer is ‘honda’, the year of manufacture is ‘1999’ and we only want to display these two columns plus those telling us about fuel economy - cty and hwy. mpg %&gt;% filter(manufacturer == &quot;honda&quot; &amp; year == &quot;1999&quot;) %&gt;% select(manufacturer, year, cty, hwy) ## # A tibble: 5 × 4 ## manufacturer year cty hwy ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 honda 1999 28 33 ## 2 honda 1999 24 32 ## 3 honda 1999 25 32 ## 4 honda 1999 23 29 ## 5 honda 1999 24 32 By combining just a few functions, you can imagine that we can build some quite complex data wrangling rules quite straightforwardly. 3.1.5 The pipe %&gt;% Note that in these examples above we used the %&gt;% operator - this is called the pipe and allows us to pass information from one side of the pipe to the other. You can read it out load as ‘and then’. All of the functions (such as select(), filter() etc.) in the Tidyverse are known as verbs, and they describe what they do. The pipe is one of the most commonly used operators in the Tidyverse and allows us to chain together different lines of code - with the output of each line being passed on as input into the next. In this example, the dataset mpg is passed along to the distinct() function where we ask for a list of the distinct (i.e., unique) manufacturers. This output itself is a vector. Vectors are a basic data structure and contain elements of the same type - for example, a bunch of numbers. We can add another line to our piped chain to tell us how many elements are in this vector. We could read this out loud as ‘take the dataset mpg, and then work out the distinct manufacturer names, and then count them’. mpg %&gt;% distinct(manufacturer) %&gt;% count() ## # A tibble: 1 × 1 ## n ## &lt;int&gt; ## 1 15 3.1.6 Tidying up a dataset Tidying variable names At the moment, the car manufacturer names are all in lower case. It would look a lot nicer if they were in title case (i.e., with capitalisation on the first letter of each word). We can use the mutate() function to create a new column - this time, the name of the new column is also the name of the old column that we’re wanting to modify using the function str_to_title(). What this will do is overwrite the column manufacturer and replace it with the new version with the car manufacturer names in title case. mpg %&gt;% mutate(manufacturer = str_to_title(manufacturer)) ## # A tibble: 234 × 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Audi a4 1.8 1999 4 auto… f 18 29 p comp… ## 2 Audi a4 1.8 1999 4 manu… f 21 29 p comp… ## 3 Audi a4 2 2008 4 manu… f 20 31 p comp… ## 4 Audi a4 2 2008 4 auto… f 21 30 p comp… ## 5 Audi a4 2.8 1999 6 auto… f 16 26 p comp… ## 6 Audi a4 2.8 1999 6 manu… f 18 26 p comp… ## 7 Audi a4 3.1 2008 6 auto… f 18 27 p comp… ## 8 Audi a4 quattro 1.8 1999 4 manu… 4 18 26 p comp… ## 9 Audi a4 quattro 1.8 1999 4 auto… 4 16 25 p comp… ## 10 Audi a4 quattro 2 2008 4 manu… 4 20 28 p comp… ## # ℹ 224 more rows The column model is also lowercase. Let’s make that title case too. We can use the mutate() function to work over more than one column at the same time like this: mpg %&gt;% mutate(manufacturer = str_to_title(manufacturer), model = str_to_title(model)) ## # A tibble: 234 × 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Audi A4 1.8 1999 4 auto… f 18 29 p comp… ## 2 Audi A4 1.8 1999 4 manu… f 21 29 p comp… ## 3 Audi A4 2 2008 4 manu… f 20 31 p comp… ## 4 Audi A4 2 2008 4 auto… f 21 30 p comp… ## 5 Audi A4 2.8 1999 6 auto… f 16 26 p comp… ## 6 Audi A4 2.8 1999 6 manu… f 18 26 p comp… ## 7 Audi A4 3.1 2008 6 auto… f 18 27 p comp… ## 8 Audi A4 Quattro 1.8 1999 4 manu… 4 18 26 p comp… ## 9 Audi A4 Quattro 1.8 1999 4 auto… 4 16 25 p comp… ## 10 Audi A4 Quattro 2 2008 4 manu… 4 20 28 p comp… ## # ℹ 224 more rows There are quite a few columns there, so how about we select just the manufacturer, model, year, transmission, and hwy columns: mpg %&gt;% mutate(manufacturer = str_to_title(manufacturer), model = str_to_title(model)) %&gt;% select(manufacturer, model, year, trans, hwy) ## # A tibble: 234 × 5 ## manufacturer model year trans hwy ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 Audi A4 1999 auto(l5) 29 ## 2 Audi A4 1999 manual(m5) 29 ## 3 Audi A4 2008 manual(m6) 31 ## 4 Audi A4 2008 auto(av) 30 ## 5 Audi A4 1999 auto(l5) 26 ## 6 Audi A4 1999 manual(m5) 26 ## 7 Audi A4 2008 auto(av) 27 ## 8 Audi A4 Quattro 1999 manual(m5) 26 ## 9 Audi A4 Quattro 1999 auto(l5) 25 ## 10 Audi A4 Quattro 2008 manual(m6) 28 ## # ℹ 224 more rows Recoding variables In the real world, data frames do not always arrive on our computer in tidy format. Very often you need to engage in some data tidying before you can do anything useful with them. We’re going to look at an example of how we go from messy data to tidy data. my_messy_data &lt;- read_csv(&quot;https://raw.githubusercontent.com/ajstewartlang/03_data_wrangling/master/data/my_data.csv&quot;) We ran a reaction time experiment with 24 participants and 4 conditions - they are numbered 1-4 in our data file. head(my_messy_data) ## # A tibble: 6 × 3 ## participant condition rt ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 879 ## 2 1 2 1027 ## 3 1 3 1108 ## 4 1 4 765 ## 5 2 1 1042 ## 6 2 2 1050 This is a repeated measures design where we had one factor (Prime Type) with two levels (A vs. B) and a second factor (Target Type) with two levels (A vs. B). We want to recode our data frame so it better matches our experimental design. First we need to recode our 4 conditions like this: Recode condition columns follows: Condition 1 = Prime A, Target A Condition 2 = Prime A, Target B Condition 3 = Prime B, Target A Condition 4 = Prime B, Target B my_messy_data %&gt;% mutate(condition = recode(condition, &quot;1&quot; = &quot;PrimeA_TargetA&quot;, &quot;2&quot; = &quot;PrimeA_TargetB&quot;, &quot;3&quot; = &quot;PrimeB_TargetA&quot;, &quot;4&quot; = &quot;PrimeB_TargetB&quot;)) %&gt;% head() ## # A tibble: 6 × 3 ## participant condition rt ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 PrimeA_TargetA 879 ## 2 1 PrimeA_TargetB 1027 ## 3 1 PrimeB_TargetA 1108 ## 4 1 PrimeB_TargetB 765 ## 5 2 PrimeA_TargetA 1042 ## 6 2 PrimeA_TargetB 1050 We now need to separate out our Condition column into two - one for our first factor (Prime), and one for our second factor (Target). The separate() function does just this - when used in conjunction with a piped tibble, it needs to know which column we want to separate, what new columns to create by separating that original column, and on what basis we want to do the separation. In the example below we tell separate() that we want to separate the column labeled condition into two new columns called Prime and Target and we want to do this at any points where a _ is present in the column to be separated. my_messy_data %&gt;% mutate(condition = recode(condition, &quot;1&quot; = &quot;PrimeA_TargetA&quot;, &quot;2&quot; = &quot;PrimeA_TargetB&quot;, &quot;3&quot; = &quot;PrimeB_TargetA&quot;, &quot;4&quot; = &quot;PrimeB_TargetB&quot;)) %&gt;% separate(col = &quot;condition&quot;, into = c(&quot;Prime&quot;, &quot;Target&quot;), sep = &quot;_&quot;) ## # A tibble: 96 × 4 ## participant Prime Target rt ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 PrimeA TargetA 879 ## 2 1 PrimeA TargetB 1027 ## 3 1 PrimeB TargetA 1108 ## 4 1 PrimeB TargetB 765 ## 5 2 PrimeA TargetA 1042 ## 6 2 PrimeA TargetB 1050 ## 7 2 PrimeB TargetA 942 ## 8 2 PrimeB TargetB 945 ## 9 3 PrimeA TargetA 943 ## 10 3 PrimeA TargetB 910 ## # ℹ 86 more rows my_messy_data %&gt;% mutate(condition = recode(condition, &quot;1&quot; = &quot;PrimeA_TargetA&quot;, &quot;2&quot; = &quot;PrimeA_TargetB&quot;, &quot;3&quot; = &quot;PrimeB_TargetA&quot;, &quot;4&quot; = &quot;PrimeB_TargetB&quot;)) %&gt;% separate(col = &quot;condition&quot;, into = c(&quot;Prime&quot;, &quot;Target&quot;), sep = &quot;_&quot;) %&gt;% mutate(Prime = factor(Prime), Target = factor(Target)) ## # A tibble: 96 × 4 ## participant Prime Target rt ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 1 PrimeA TargetA 879 ## 2 1 PrimeA TargetB 1027 ## 3 1 PrimeB TargetA 1108 ## 4 1 PrimeB TargetB 765 ## 5 2 PrimeA TargetA 1042 ## 6 2 PrimeA TargetB 1050 ## 7 2 PrimeB TargetA 942 ## 8 2 PrimeB TargetB 945 ## 9 3 PrimeA TargetA 943 ## 10 3 PrimeA TargetB 910 ## # ℹ 86 more rows The pivot functions Most of the analysis we will conduct in R requires our data to be in tidy, or long, format. In such data sets, one row corresponds to one observation. In the real world, data are rarely in the right format for analysis. In R, the pivot_wider() and pivot_longer() functions are designed to reshape our data files. First, let’s load a datafile that is in wide format (i.e., multiple observations per row). It is from an experiment where we had four conditions (labelled Condition1, Condition2, Condition3, and Condition4). In addition to there being a column for each of the 4 conditions, we also have a column corresponding to participant ID. Each cell in the data set corresponds to a reaction time (measured in milliseconds). my_wide_data &lt;- read_csv(&quot;https://raw.githubusercontent.com/ajstewartlang/03_data_wrangling/master/data/my_wide_data.csv&quot;) The pivot_longer() function head(my_wide_data) ## # A tibble: 6 × 5 ## ID Condition1 Condition2 Condition3 Condition4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 487 492 499 488 ## 2 2 502 494 517 508 ## 3 3 510 483 488 509 ## 4 4 476 488 513 521 ## 5 5 504 478 513 504 ## 6 6 505 486 503 495 So, we can see the data file is in wide format. We want to reshape it to long format. We can do that using the pivot_longer() function. Minimally, we need to specify the data frame that we want to reshape, the columns that we want to ‘pivot’ into longer format, the name of the new column that we are creating, and the name of the column that will hold the values of our reshaped data frame. We are going to map the output to a variable I’m calling my_longer_data. my_longer_data &lt;- my_wide_data %&gt;% pivot_longer(cols = c(Condition1, Condition2, Condition3, Condition4), names_to = &quot;Condition&quot;, values_to = &quot;RT&quot;) Now let’s have a look at what our reshaped data frame looks like. head(my_longer_data) ## # A tibble: 6 × 3 ## ID Condition RT ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Condition1 487 ## 2 1 Condition2 492 ## 3 1 Condition3 499 ## 4 1 Condition4 488 ## 5 2 Condition1 502 ## 6 2 Condition2 494 So you can see our data are now in long - or tidy - format with one observation per row. Note that our Condition column isn’t coded as a factor. It’s important that our data set reflects the structure of our experiment so let’s convert that column to a factor - note that in the following code we are now ‘saving’ the change as we are not mapping the output onto a variable name. my_longer_data %&gt;% mutate(Condition = factor(Condition)) %&gt;% head() ## # A tibble: 6 × 3 ## ID Condition RT ## &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 1 Condition1 487 ## 2 1 Condition2 492 ## 3 1 Condition3 499 ## 4 1 Condition4 488 ## 5 2 Condition1 502 ## 6 2 Condition2 494 The pivot_wider() function We can use the pivot_wider() function to reshape a long data frame so that it goes from long to wide format. It works similarly to pivot_longer(). Let’s take our new, long, data frame and turn it back into wide format. With pivot_wider() we minimally need to specify the data frame that we want to reshape, and a pair or arguments (names_from and values_from) that describe from which column to get the name of the output column, and from which column to get the cell values. my_wider_data &lt;- my_longer_data %&gt;% pivot_wider(names_from = &quot;Condition&quot;, values_from = &quot;RT&quot;) We can check that our data set is back in wide format. head(my_wider_data) ## # A tibble: 6 × 5 ## ID Condition1 Condition2 Condition3 Condition4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 487 492 499 488 ## 2 2 502 494 517 508 ## 3 3 510 483 488 509 ## 4 4 476 488 513 521 ## 5 5 504 478 513 504 ## 6 6 505 486 503 495 3.1.7 Joining Two Datasets Sometimes you might need to combine two datasets. For example, you might have one dataset that contains reading time data (like the one above) and another than contains individual difference measures for the participants in the first dataset. How would we go about combining these two datasets so that we end up with one that includes both the reading time data and the individual difference measures (that perhaps we want to covary out later)? Luckily, the dplyr package contains a number of join functions that allows you to join together different tibbles. First, let’s load the data that contains the individual different measures. individual_diffs &lt;- read_csv(&quot;https://raw.githubusercontent.com/ajstewartlang/03_data_wrangling/master/data/individual_diffs.csv&quot;) Let’s look at the first few rows of the individual differences data. This dataset contains the ID numbers of our participants plus measures of IQ (the iq column) and Working Memory (the wm column). head(individual_diffs) ## # A tibble: 6 × 3 ## ID iq wm ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 100 9 ## 2 2 108 8 ## 3 3 116 9 ## 4 4 95 9 ## 5 5 83 11 ## 6 6 73 10 We want to combine this dataset with our reading time dataset from above my_longer_data which looks like this: head(my_longer_data) ## # A tibble: 6 × 3 ## ID Condition RT ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Condition1 487 ## 2 1 Condition2 492 ## 3 1 Condition3 499 ## 4 1 Condition4 488 ## 5 2 Condition1 502 ## 6 2 Condition2 494 Full Join We can combine using one of the join functions. There are a variety of options including full_join() which includes all of the rows from both tibbles that we want to join. Other options include inner_join() which keeps only the rows in tibble one that have a matching key in tibble 2, as well as left_join() and right_join(). combined_data &lt;- full_join(my_longer_data, individual_diffs, by = &quot;ID&quot;) We now see that our dataset are combined as we’d expect. combined_data ## # A tibble: 128 × 5 ## ID Condition RT iq wm ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Condition1 487 100 9 ## 2 1 Condition2 492 100 9 ## 3 1 Condition3 499 100 9 ## 4 1 Condition4 488 100 9 ## 5 2 Condition1 502 108 8 ## 6 2 Condition2 494 108 8 ## 7 2 Condition3 517 108 8 ## 8 2 Condition4 508 108 8 ## 9 3 Condition1 510 116 9 ## 10 3 Condition2 483 116 9 ## # ℹ 118 more rows Left Join Of course, you may be thinking that we could just do a quick bit of Excel cut and paste of the columns we want from one dataset to the other. But what about the case where our individual differences file contains 10,000 participant IDs (in random order) and we’re only interested in combining the two datasets where there is a match? large_ind_diffs &lt;- read_csv(&quot;https://raw.githubusercontent.com/ajstewartlang/03_data_wrangling/master/data/large_ind_diffs.csv&quot;) head(large_ind_diffs) ## # A tibble: 6 × 3 ## ID iq wm ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 6057 93 7 ## 2 2723 86 7 ## 3 1088 97 9 ## 4 8687 87 8 ## 5 4223 77 11 ## 6 369 95 9 We can actually use another join function (left_join()) to combine these two datasets, but only where there is a match of ID with the first of the two datasets (my_longer_data) in the function call. left_join(my_longer_data, large_ind_diffs, by = &quot;ID&quot;) ## # A tibble: 128 × 5 ## ID Condition RT iq wm ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Condition1 487 100 9 ## 2 1 Condition2 492 100 9 ## 3 1 Condition3 499 100 9 ## 4 1 Condition4 488 100 9 ## 5 2 Condition1 502 108 8 ## 6 2 Condition2 494 108 8 ## 7 2 Condition3 517 108 8 ## 8 2 Condition4 508 108 8 ## 9 3 Condition1 510 116 9 ## 10 3 Condition2 483 116 9 ## # ℹ 118 more rows 3.1.8 Your challenge (complete this before the in-person session) Have a go at recreating the join above using RStudio Desktop. Use read_csv to create the my_wide_data tibble (as shown above) and use pivot_longer to make my my_longer_data. Then use read_csv to create large_ind_diffs and use left_join to combine it with my_longer_data 3.2 Summarising your data We’ll be using the mpg dataset that is built into the tidyverse for this workshop. This dataset contains information about cars (such as engine size, fuel economy) produced by a number of different manufacturers. Once a dataset has been tidied, often one of the first things we want to do is generate summary statistics, e.g. the means and standard deviations for one of our variables grouped by car manufacturer. Have a look at the following video where I walk you through this worksheet. Then I want you to work through the content by writing (and running) the script on your own machine.       Remember to set up a .Rproj file for this workshop before continuing. In your script, you’ll first need to load the tidyverse. library(tidyverse) 3.2.1 Using group_by() and summarise() We are going to use the group_by() function to group the dataset, and then the summarise() function to calculate the mean and sd of the hwy variable. The summarise() function can take a lot of different functions to give us summary statistics. To read more about the different options, type ?summarise in the Console window. Commonly used ones are mean(), median(), sd(). mpg %&gt;% group_by(manufacturer) %&gt;% summarise(mean_hwy = mean(hwy), sd_hwy = sd(hwy), number = n()) ## # A tibble: 15 × 4 ## manufacturer mean_hwy sd_hwy number ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 audi 26.4 2.18 18 ## 2 chevrolet 21.9 5.11 19 ## 3 dodge 17.9 3.57 37 ## 4 ford 19.4 3.33 25 ## 5 honda 32.6 2.55 9 ## 6 hyundai 26.9 2.18 14 ## 7 jeep 17.6 3.25 8 ## 8 land rover 16.5 1.73 4 ## 9 lincoln 17 1 3 ## 10 mercury 18 1.15 4 ## 11 nissan 24.6 5.09 13 ## 12 pontiac 26.4 1.14 5 ## 13 subaru 25.6 1.16 14 ## 14 toyota 24.9 6.17 34 ## 15 volkswagen 29.2 5.32 27 Note that this output is currently ordered alphabetically by the first column manufacturer. What if we wanted to order this out by mean highway fuel economy highest (best) to lowest (worst)? We can use the arrange function. Re-ordering the output with arrange() mpg %&gt;% group_by(manufacturer) %&gt;% summarise(mean_hwy = mean(hwy), sd_hwy = sd(hwy), number = n()) %&gt;% arrange(mean_hwy) ## # A tibble: 15 × 4 ## manufacturer mean_hwy sd_hwy number ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 land rover 16.5 1.73 4 ## 2 lincoln 17 1 3 ## 3 jeep 17.6 3.25 8 ## 4 dodge 17.9 3.57 37 ## 5 mercury 18 1.15 4 ## 6 ford 19.4 3.33 25 ## 7 chevrolet 21.9 5.11 19 ## 8 nissan 24.6 5.09 13 ## 9 toyota 24.9 6.17 34 ## 10 subaru 25.6 1.16 14 ## 11 pontiac 26.4 1.14 5 ## 12 audi 26.4 2.18 18 ## 13 hyundai 26.9 2.18 14 ## 14 volkswagen 29.2 5.32 27 ## 15 honda 32.6 2.55 9 Hmm, so that isn’t what we want - this is going from lowest to highest which is the default in R. We can change that by putting a - sign in from of the parameter we can to order by. mpg %&gt;% group_by(manufacturer) %&gt;% summarise(mean_hwy = mean(hwy), sd_hwy = sd(hwy), number = n()) %&gt;% arrange(-mean_hwy) ## # A tibble: 15 × 4 ## manufacturer mean_hwy sd_hwy number ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 honda 32.6 2.55 9 ## 2 volkswagen 29.2 5.32 27 ## 3 hyundai 26.9 2.18 14 ## 4 audi 26.4 2.18 18 ## 5 pontiac 26.4 1.14 5 ## 6 subaru 25.6 1.16 14 ## 7 toyota 24.9 6.17 34 ## 8 nissan 24.6 5.09 13 ## 9 chevrolet 21.9 5.11 19 ## 10 ford 19.4 3.33 25 ## 11 mercury 18 1.15 4 ## 12 dodge 17.9 3.57 37 ## 13 jeep 17.6 3.25 8 ## 14 lincoln 17 1 3 ## 15 land rover 16.5 1.73 4 This is looking better. The summarise_at() variant As well as using summarise(), you can use related functions such as summarise_at(). This is a scoped version of the summarise() function that can be applied across multiple columns. Note, when using summarise_at() you need to put the columns you want to summarise over in quotes. You also need to provide the summary function - in this case mean. Finally, in case our dataset contains any missing values (indicated by NA), we set the parameter na.rm = TRUE. This will ensure that missing data points are removed before the operation is applied. If we had missing data, but didn’t tell R what we wanted to do with it, it would have thrown an error. mpg %&gt;% group_by(manufacturer) %&gt;% summarise_at(c(&quot;displ&quot;, &quot;cty&quot;, &quot;hwy&quot;), mean, na.rm = TRUE) ## # A tibble: 15 × 4 ## manufacturer displ cty hwy ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 audi 2.54 17.6 26.4 ## 2 chevrolet 5.06 15 21.9 ## 3 dodge 4.38 13.1 17.9 ## 4 ford 4.54 14 19.4 ## 5 honda 1.71 24.4 32.6 ## 6 hyundai 2.43 18.6 26.9 ## 7 jeep 4.58 13.5 17.6 ## 8 land rover 4.3 11.5 16.5 ## 9 lincoln 5.4 11.3 17 ## 10 mercury 4.4 13.2 18 ## 11 nissan 3.27 18.1 24.6 ## 12 pontiac 3.96 17 26.4 ## 13 subaru 2.46 19.3 25.6 ## 14 toyota 2.95 18.5 24.9 ## 15 volkswagen 2.26 20.9 29.2 The summarise_if() variant Imagine we had a really big dataset and wanted to summarise all columns that were of a certain type. We can use the summarise_if() function to work out the mean for each of our car manufactures as follows: mpg %&gt;% group_by(manufacturer) %&gt;% summarise_if(is.numeric, mean, na.rm = TRUE) ## # A tibble: 15 × 6 ## manufacturer displ year cyl cty hwy ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 audi 2.54 2004. 5.22 17.6 26.4 ## 2 chevrolet 5.06 2005. 7.26 15 21.9 ## 3 dodge 4.38 2004. 7.08 13.1 17.9 ## 4 ford 4.54 2003. 7.2 14 19.4 ## 5 honda 1.71 2003 4 24.4 32.6 ## 6 hyundai 2.43 2004. 4.86 18.6 26.9 ## 7 jeep 4.58 2006. 7.25 13.5 17.6 ## 8 land rover 4.3 2004. 8 11.5 16.5 ## 9 lincoln 5.4 2002 8 11.3 17 ## 10 mercury 4.4 2004. 7 13.2 18 ## 11 nissan 3.27 2004. 5.54 18.1 24.6 ## 12 pontiac 3.96 2003. 6.4 17 26.4 ## 13 subaru 2.46 2004. 4 19.3 25.6 ## 14 toyota 2.95 2003. 5.12 18.5 24.9 ## 15 volkswagen 2.26 2003. 4.59 20.9 29.2 The first parameter in summarise_if() is the logical test applied to each column - in this case, if a column is numeric (i.e., a number) - then the test evaluates to TRUE and the second function, mean, is applied. Again, we tell R to ignore missing (NA) data values with the na.rm = TRUE parameter. R functions differ in terms of what arguments they take. I often forget them - if you start typing a function name, you’ll get a little bubble above where you’re typing to remind you what parameters are needed. And if you can’t remember the details, you can just type help(function_name) or ?function_name in the console for any function that you need help with. A lot of data analysis with R (or Python or any other language really) involves a fair bit of Googling. This is normal. There are some things I can never remember and am always having to look up! 3.2.2 Adding columns using mutate() We can add a new column that I’m calling mean_hwy using the mutate() function like this. mpg %&gt;% group_by(manufacturer) %&gt;% mutate(mean_hwy = mean(hwy), sd_hwy = sd(hwy)) ## # A tibble: 234 × 13 ## # Groups: manufacturer [15] ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 audi a4 1.8 1999 4 auto… f 18 29 p comp… ## 2 audi a4 1.8 1999 4 manu… f 21 29 p comp… ## 3 audi a4 2 2008 4 manu… f 20 31 p comp… ## 4 audi a4 2 2008 4 auto… f 21 30 p comp… ## 5 audi a4 2.8 1999 6 auto… f 16 26 p comp… ## 6 audi a4 2.8 1999 6 manu… f 18 26 p comp… ## 7 audi a4 3.1 2008 6 auto… f 18 27 p comp… ## 8 audi a4 quattro 1.8 1999 4 manu… 4 18 26 p comp… ## 9 audi a4 quattro 1.8 1999 4 auto… 4 16 25 p comp… ## 10 audi a4 quattro 2 2008 4 manu… 4 20 28 p comp… ## # ℹ 224 more rows ## # ℹ 2 more variables: mean_hwy &lt;dbl&gt;, sd_hwy &lt;dbl&gt; We have too many columns to display on this page so we can drop a couple by using the select() function slightly differently. By putting a - sign in front of a column names in select() we end up dropping it. mpg %&gt;% group_by(manufacturer) %&gt;% mutate(mean_hwy = mean(hwy), sd_hwy = sd(hwy)) %&gt;% select(-class, -trans) ## # A tibble: 234 × 11 ## # Groups: manufacturer [15] ## manufacturer model displ year cyl drv cty hwy fl mean_hwy sd_hwy ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 audi a4 1.8 1999 4 f 18 29 p 26.4 2.18 ## 2 audi a4 1.8 1999 4 f 21 29 p 26.4 2.18 ## 3 audi a4 2 2008 4 f 20 31 p 26.4 2.18 ## 4 audi a4 2 2008 4 f 21 30 p 26.4 2.18 ## 5 audi a4 2.8 1999 6 f 16 26 p 26.4 2.18 ## 6 audi a4 2.8 1999 6 f 18 26 p 26.4 2.18 ## 7 audi a4 3.1 2008 6 f 18 27 p 26.4 2.18 ## 8 audi a4 qu… 1.8 1999 4 4 18 26 p 26.4 2.18 ## 9 audi a4 qu… 1.8 1999 4 4 16 25 p 26.4 2.18 ## 10 audi a4 qu… 2 2008 4 4 20 28 p 26.4 2.18 ## # ℹ 224 more rows Note that this doesn’t change the mpg dataset permanently - the changes won’t be saved unless we map the output of this code onto a new variable. Below I am doing this by using the assignment operator &lt;- to map it onto a new variable I’m calling mpg_with_mean. Note that we remove the grouping at the end as we don’t want our grouping rule to remain in our new data frame. mpg_with_mean &lt;- mpg %&gt;% group_by(manufacturer) %&gt;% mutate(mean_hwy = mean(hwy), sd_hyw = sd(hwy)) %&gt;% ungroup() %&gt;% select(-class, -trans) We can then inspect this new variable using head() and str(). head(mpg_with_mean) ## # A tibble: 6 × 11 ## manufacturer model displ year cyl drv cty hwy fl mean_hwy sd_hyw ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 audi a4 1.8 1999 4 f 18 29 p 26.4 2.18 ## 2 audi a4 1.8 1999 4 f 21 29 p 26.4 2.18 ## 3 audi a4 2 2008 4 f 20 31 p 26.4 2.18 ## 4 audi a4 2 2008 4 f 21 30 p 26.4 2.18 ## 5 audi a4 2.8 1999 6 f 16 26 p 26.4 2.18 ## 6 audi a4 2.8 1999 6 f 18 26 p 26.4 2.18 str(mpg_with_mean) ## tibble [234 × 11] (S3: tbl_df/tbl/data.frame) ## $ manufacturer: chr [1:234] &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; ... ## $ model : chr [1:234] &quot;a4&quot; &quot;a4&quot; &quot;a4&quot; &quot;a4&quot; ... ## $ displ : num [1:234] 1.8 1.8 2 2 2.8 2.8 3.1 1.8 1.8 2 ... ## $ year : int [1:234] 1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 ... ## $ cyl : int [1:234] 4 4 4 4 6 6 6 4 4 4 ... ## $ drv : chr [1:234] &quot;f&quot; &quot;f&quot; &quot;f&quot; &quot;f&quot; ... ## $ cty : int [1:234] 18 21 20 21 16 18 18 18 16 20 ... ## $ hwy : int [1:234] 29 29 31 30 26 26 27 26 25 28 ... ## $ fl : chr [1:234] &quot;p&quot; &quot;p&quot; &quot;p&quot; &quot;p&quot; ... ## $ mean_hwy : num [1:234] 26.4 26.4 26.4 26.4 26.4 ... ## $ sd_hyw : num [1:234] 2.18 2.18 2.18 2.18 2.18 ... 3.2.3 Your challenge (do this during the in-person session) The Tidyverse has a number of other built-in data sets. Another one is the starwars data set. You can have a look at it by typing starwars or by typing view(starwars). This second option will open the data set in a new window. Have a go playing around with it. Work out the mean height of humans in the Star Wars universe. There might be some missing data (indicated by NA). You can use the na.rm = TRUE parameter in your summarise() function to ignore these values when generating your summary statistics. Another way to filter out NA values is to use the filter() function in your pipeline. The function is.na() returns a logical value of TRUE of FALSE. The operator ! means NOT, so the expression !is.na(height) will return TRUE when the height value is present, and FALSE if absent. By combining this with filter() we have the line filter(!is.na(height)) which will filter only the cases where we have height data (i.e., !is.na(height) is TRUE). So your code might look like this: starwars %&gt;% filter(!is.na(height)) %&gt;% filter(species == &quot;Human&quot;) %&gt;% summarise(mean_height = mean(height)) Replace the word mean in the summarise() line with median. What other things can you replace it with? Hint: type ?summarise in the console. What other summary information can you extract from this dataset? End of workshop 3 materials "],["data-visualisation.html", "Workshop 4 Data Visualisation 4.1 The Basics of ggplot2 4.2 Your First Visualisation 4.3 Scatterplots 4.4 Plotting Histograms 4.5 The NHANES Dataset 4.6 Your Challenge (do this during the in-person session) 4.7 Additional Resources", " Workshop 4 Data Visualisation Being able to build clear visualisations is key to the successful communication of your data to your intended audience. There are a couple of great recent books focused on data visualisation that I suggest you have a look it. They both provide great perspectives on data visualisations and are full of wonderful examples of different kinds of data visualisations, some of which you’ll learn how to build in this workshop. If you click on the image of the Claus Wilke book, you’ll be taken to the online version of the book (written in R, obviously!)       First I’d like you to watch this brief video where I give some examples of the kinds of data visualisations you can build in R, and why you probably want to avoid building bar graphs.          4.1 The Basics of ggplot2 First we need to load the ggplot2 package. As it is part of the Tidyverse (and we’re likely to be using other Tidyverse packages alongside ggplot2), we load it into our library using the library(tidyverse) line of code. library(tidyverse) In the video, I mention that using ggplot2 requires us to specify some core information in order to build our visualisations. These include the raw data that you want to plot, geometries (or geoms) that are the geometric shapes that will represent the data, and the aesthetics of the geometric and objects, such as color, size, shape and position. 4.2 Your First Visualisation Below is an example where we’re using the mpg dataset (which is a dataset that contains information about cars) to build a visualisation that plots the points corresponding to city fuel economy (cty) on the y-axis and manufacturer on the x-axis. mpg %&gt;% ggplot(aes(x = manufacturer, y = cty)) + geom_point() So, this is not great. The x-axis labels are hard to read, and the individual points don’t look that pleasing. We can use the str_to_title() function to change the manufacturer labels to title case, and adjust the axis labels easily using the theme() function. Note, the + symbol between the lines of ggplot() code is equivalent to the %&gt;% operator. For historical reasons (basically, because ggplot() came before the other packages in the Tidyverse), you need to use the + when adding layers to your ggplot() visualisations. mpg %&gt;% mutate(manufacturer = str_to_title(manufacturer)) %&gt;% ggplot(aes(x = manufacturer, y = cty)) + geom_point() + theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust = .5)) 4.2.1 Improving the Plot So let’s do some more tidying - we’re going to jitter the points slightly (so they’re not stacked vertically) using the geom_jitter() function, and tidy up the axis titles using the labs() function to explicitly add axis labels (rather than just use the labels in our dataset). We’re also adding a few other tweaks - can you spot them? mpg %&gt;% mutate(manufacturer = str_to_title(manufacturer)) %&gt;% ggplot(aes(x = manufacturer, y = cty)) + geom_jitter(width = .2, alpha = .75, size = 2) + theme_minimal() + theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust = .5)) + theme(text = element_text(size = 13)) + labs(title = &quot;City Fuel Economy by Car Manufacturer&quot;, x = &quot;Manufacturer&quot;, y = &quot;City Fuel Economy (mpg)&quot;) It might be helpful for us to add summary statistical information such as the mean fuel economy and confidence intervals around the mean for each car manufacturer. 4.2.2 Adding Summary Statistics We need to add the Hmisc package to allow us to use the stat_summary() function. library(Hmisc) mpg %&gt;% mutate(manufacturer = str_to_title(manufacturer)) %&gt;% ggplot(aes(x = manufacturer, y = cty)) + stat_summary(fun.data = mean_cl_boot, colour = &quot;black&quot;, size = 1) + theme_minimal() + theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust = .5)) + theme(text = element_text(size = 13)) + labs(title = &quot;City Fuel Economy by Car Manufacturer&quot;, x = &quot;Manufacturer&quot;, y = &quot;City Fuel Economy (mpg)&quot;) 4.2.3 The Finished(?) Plot At the moment, the x-axis is ordered alphabetically. How about we order it so that it goes from manufacturer with the hightest mean fuel economy, to the lowest. Also, how about we flip the visualisation so that the axes swap and add a few other tweaks? mpg %&gt;% mutate(manufacturer = str_to_title(manufacturer)) %&gt;% ggplot(aes(x = fct_reorder(manufacturer, .fun = mean, cty), y = cty, colour = manufacturer)) + stat_summary(fun.data = mean_cl_boot, size = 1) + geom_jitter(alpha = .25) + theme_minimal() + theme(text = element_text(size = 13)) + labs(title = &quot;Manufacturer by City Fuel Economy&quot;, x = &quot;Manufacturer&quot;, y = &quot;City Fuel Economy (mpg)&quot;) + guides(colour = &#39;none&#39;) + coord_flip() This looks pretty good. Can you tell what the other bits of code do that I added? Have a go changing some of the numbers to see what happens. You can prevent a line of code being run by adding a # in front of it. So if you need to temporarily not run a line, just add a # rather than delete the line. Plots are rarely completely “finished” as you’ll often think of a minor aesthetic tweak that might make some improvement. 4.2.4 Using facet_wrap() We might think that fuel economy varies as a function of the type of vehicle (e.g., sports cars may be more fuel hungry than midsize cars) and by the number size of engine (e.g., cars with bigger engines may be more fuel hungry). In the visualisation below we’re going to use the facet_wrap() function to build a separate visualisation for each level of the factor we are facetting over (ignoring SUVs). mpg %&gt;% filter(class != &quot;suv&quot;) %&gt;% mutate(class = str_to_title(class)) %&gt;% ggplot(aes(x = displ, y = cty, colour = class)) + geom_jitter(width = .2) + theme_minimal() + theme(text = element_text(size = 13)) + labs(title = &quot;City Fuel Economy by Engine Displacement&quot;, x = &quot;Engine Displacement (litres)&quot;, y = &quot;City Fuel Economy (mpg)&quot;) + guides(colour = &#39;none&#39;) + facet_wrap(~ class) Can you tell what each bit of code is doing? Again, edit the numbers and put a # before lines you want to temporarily ignore to see what happens. 4.3 Scatterplots Above we focused on plotting a numerical variable on one axis, and a categorical variable on the other. There will be cases where we want to create scatterplots, allowing us to plot two numerical variables against each other - possibly to determine whether there might be a relationship between the two. Below we are plotting Engine Displacement on the y-axis, and City Fuel Economy on the x-axis. mpg %&gt;% mutate(class = str_to_upper(class)) %&gt;% ggplot(aes(x = cty, y = displ)) + geom_point(aes(colour = class)) + geom_smooth(se = FALSE) + theme(text = element_text(size = 13)) + theme_minimal() + labs(x = &quot;City Fuel Economy (mpg)&quot;, y = &quot;Engine Displacement (litres)&quot;, colour = &quot;Vehicle Class&quot;) In the above example, we used the geom_smooth() function to add a layer corresponding to fitting a curve to our data. We can see a fairly clear negative correlation between Engine Displacement and Fuel Economy for Fuel Economy values that are less than 25 mpg, but little relationship between the two for values that are great than 25 mpg. These seems to suggest there are some cars with relatively small engines that have great fuel economy, and others with similar engine sizes that have much worse fuel economy. 4.4 Plotting Histograms We might want to plot a histogram of engine sizes (measured in litres and captured in the variable displ in the mpg dataset) to get a feel for how this variable is distributed. mpg %&gt;% ggplot(aes(x = displ)) + geom_histogram(binwidth = .5, fill = &quot;grey&quot;) + labs(title = &quot;Histogram of Engine Displacement&quot;, x = &quot;Engine Displacement (litres)&quot;, y = &quot;Count&quot;) 4.4.1 The ggridges Package Given in a previous visualisation we saw that there seemed to be variability between vehicle classes, wouldn’t it be great if we could compare the distributions of engine size separated by vehicle class? We’re now going to use the ggridges package to do just that… library(ggridges) mpg %&gt;% mutate(class = str_to_title(class)) %&gt;% ggplot(aes(x = displ, y = fct_reorder(class, .fun = mean, displ))) + geom_density_ridges(height = .5, aes(fill = class)) + theme_minimal() + theme(text = element_text(size = 13)) + guides(fill = &#39;none&#39;) + labs(x = &quot;Engine Displacement (litres)&quot;, y = NULL) We can see from the above visualisation that SUVs seem to have quite a lot of variability in engine size while compact cars have relatively little variability. 4.5 The NHANES Dataset We’re now going to visualise aspects of the NHANES dataset.    This is survey data collected by the US National Center for Health Statistics (NCHS) which has conducted a series of health and nutrition surveys since the early 1960’s. Since 1999 approximately 5,000 individuals of all ages are interviewed in their homes every year and complete the health examination component of the survey. The health examination is conducted in a mobile examination centre (MEC). The NHANES target population is “the non-institutionalized civilian resident population of the United States”. NHANES, (American National Health and Nutrition Examination surveys), use complex survey designs (see http://www.cdc.gov/nchs/data/series/sr_02/sr02_162.pdf) that oversample certain subpopulations like racial minorities. Naive analysis of the original NHANES data can lead to mistaken conclusions. The percentages of people from each racial group in the data, for example, are quite different from the way they are in the population.    We need to load the NHANES package as this is where the dataset is contained. library(NHANES) If running the above command generated an error, is it because you haven’t installed the package on your machine with install.packages(\"NHANES\")? First we’re going to explore the NHANES dataset. ncol(NHANES) ## [1] 76 nrow(NHANES) ## [1] 10000 We see there are 76 columns and 10,000 rows. If we use the function head() we can see the first few rows of the dataframe. head(NHANES) ## # A tibble: 6 × 76 ## ID SurveyYr Gender Age AgeDecade AgeMonths Race1 Race3 Education ## &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; ## 1 51624 2009_10 male 34 &quot; 30-39&quot; 409 White &lt;NA&gt; High School ## 2 51624 2009_10 male 34 &quot; 30-39&quot; 409 White &lt;NA&gt; High School ## 3 51624 2009_10 male 34 &quot; 30-39&quot; 409 White &lt;NA&gt; High School ## 4 51625 2009_10 male 4 &quot; 0-9&quot; 49 Other &lt;NA&gt; &lt;NA&gt; ## 5 51630 2009_10 female 49 &quot; 40-49&quot; 596 White &lt;NA&gt; Some College ## 6 51638 2009_10 male 9 &quot; 0-9&quot; 115 White &lt;NA&gt; &lt;NA&gt; ## # ℹ 67 more variables: MaritalStatus &lt;fct&gt;, HHIncome &lt;fct&gt;, HHIncomeMid &lt;int&gt;, ## # Poverty &lt;dbl&gt;, HomeRooms &lt;int&gt;, HomeOwn &lt;fct&gt;, Work &lt;fct&gt;, Weight &lt;dbl&gt;, ## # Length &lt;dbl&gt;, HeadCirc &lt;dbl&gt;, Height &lt;dbl&gt;, BMI &lt;dbl&gt;, ## # BMICatUnder20yrs &lt;fct&gt;, BMI_WHO &lt;fct&gt;, Pulse &lt;int&gt;, BPSysAve &lt;int&gt;, ## # BPDiaAve &lt;int&gt;, BPSys1 &lt;int&gt;, BPDia1 &lt;int&gt;, BPSys2 &lt;int&gt;, BPDia2 &lt;int&gt;, ## # BPSys3 &lt;int&gt;, BPDia3 &lt;int&gt;, Testosterone &lt;dbl&gt;, DirectChol &lt;dbl&gt;, ## # TotChol &lt;dbl&gt;, UrineVol1 &lt;int&gt;, UrineFlow1 &lt;dbl&gt;, UrineVol2 &lt;int&gt;, … 4.5.1 Tidying the Data It looks like some participants appear more than once in the dataset - this could be due to the oversampling mentioned in the description - the first few rows are all for participant ID 51624. We can use the select() function alongwith the n_distinct() function to tell us the unique number of IDs in the dataset. NHANES %&gt;% select(ID) %&gt;% n_distinct() ## [1] 6779 We see we have 6,779 unique individuals. Let’s tidy our data to remove duplicate IDs. Note that below we’re using the pipe operator %&gt;% You can read it as ‘and then’ so it means we’re taking the NHANES dataset and then filtering it keeping just rows with distinct ID numbers. The pipe operator really helps with the readability of your data wrangling code and is an integral part of the tidyverse philosophy - tidy data and tidy code. NHANES_tidied &lt;- NHANES %&gt;% distinct(ID, .keep_all = TRUE) ncol(NHANES_tidied) ## [1] 76 nrow(NHANES_tidied) ## [1] 6779 OK, so our tidied dataset is assigned to the variable NHANES_tidied and has 6,779 rows (but still 76 columns) - as we’d expect given we have 6,779 unique individuals. 4.5.2 Plotting a Histogram Let’s start exploring the data. We have lots of potential variables and relationships to explore. I see we have one labelled Education which is coded as a factor. We also have information related to health such as BMI - first of all lets plot a histogram of BMI. NHANES_tidied %&gt;% ggplot(aes(x = BMI)) + geom_histogram(bins = 100, na.rm = TRUE) We see a pretty right skewed distribution here. Note our use of the na.rm parameter - this parameter appears in many tidyverse functions and by setting it to TRUE we tell R to ignore any parts of our data frame where we have missing data (which is indicated by NA). 4.5.3 Summary Statistics Does BMI vary as a function of Education level? In the code below we are using the data stored in the variable NHANES_tidied, grouping it by Education, then summarising to generate the median BMI for each of our groups. Again, we use the na.rm = TRUE parameter with the summarise() function this time to remove any missing values (NA) from our calculation. NHANES_tidied %&gt;% group_by(Education) %&gt;% summarise(median = median(BMI, na.rm = TRUE)) ## # A tibble: 6 × 2 ## Education median ## &lt;fct&gt; &lt;dbl&gt; ## 1 8th Grade 28.6 ## 2 9 - 11th Grade 28.2 ## 3 High School 28.2 ## 4 Some College 28.4 ## 5 College Grad 26.5 ## 6 &lt;NA&gt; 18.9 So it looks like those with College eduction have the lowest median BMI (ignoring the NA category which corresponds to cases where we don’t have Education level recorded). 4.5.4 geom_violin() Let’s graph it! Note here we’re filtering out cases where we don’t have BMI value recorded. The function is.na() returns TRUE when applied to a case of missing data (NA) - we use the ! operator to negate this and combine several of these expressions together using the logical AND operator &amp;. The line of code below starting with filter() means filter cases where Education is not missing AND BMI is not missing. This means that the NHANES_tidied data that gets passed to the ggplot() call has no missing data for the key variables we’re interested in. I then add a geom_violin() layer to capture the shape of the distribution for each level of Education and geom_boxplot() layer to create a boxplot for each level of our Education factor. The guides(colour = 'none') call suppresses displaying the colour legend - place a # in front of it and rerun the code to see what changes. NHANES_tidied %&gt;% filter(!is.na(Education) &amp; !is.na(BMI)) %&gt;% ggplot(aes(x = Education, y = BMI, colour = Education)) + geom_violin() + geom_jitter(alpha = .2, width = .1) + geom_boxplot(alpha = .5) + guides(colour = &#39;none&#39;) + labs(title = &quot;Examining the effect of education level on BMI&quot;, x = &quot;Education Level&quot;, y = &quot;BMI&quot;) 4.5.5 Plotting Interaction Effects We can also plot how two factors interact with each other. For the plot above, we’ll now add the factor Diabetes (which has two levels - Yes vs. No) to see how that might interact with Education level. To capture the nature of this interaction, we use the expression Education:Diabetes when we specify the x-axis aesthetic. Note, I have rotated the x-axis labels 45 degrees to make them easier to read. NHANES_tidied %&gt;% filter(!is.na(Education) &amp; !is.na(BMI) &amp; !is.na(Diabetes)) %&gt;% ggplot(aes(x = Education:Diabetes, y = BMI, colour = Education)) + geom_violin() + geom_jitter(alpha = .2, width = .1) + geom_boxplot(alpha = .5) + guides(colour = &#39;none&#39;) + theme(axis.text.x = element_text(angle = 45, vjust = 0.5)) + labs(title = &quot;Examining the effect of education level and diabetes on BMI&quot;, x = &quot;Education Level x Diabetes&quot;, y = &quot;BMI&quot;) We can see from the above plot that those with Diabetes seem to also have higher BMI scores for each level of Education. 4.5.6 Histograms with facet_wrap() We can also plot histograms of BMI separately for each Education level - we use the facet_wrap() function to do this. NHANES_tidied %&gt;% filter(!is.na(Education) &amp; !is.na(BMI)) %&gt;% group_by(Education) %&gt;% ggplot(aes(x = BMI, fill = Education)) + geom_histogram() + guides(fill = &#39;none&#39;) + labs(title = &quot;Examining the effect of education level on BMI&quot;, x = &quot;BMI&quot;, y = &quot;Number of cases&quot;) + facet_wrap(~ Education) In the above graph, notice that the same y-axis scale is used for each plot - this makes comparisons a little tricky as there are different numbers of cases for each Eduction level. Add the following scales = \"free\" after Education in the facet_wrap() line. What changes? Instead of generating the histograms using a count, we could generate them using a density function. Let’s also add a density curve. NHANES_tidied %&gt;% filter(!is.na(Education) &amp; !is.na(BMI)) %&gt;% group_by(Education) %&gt;% ggplot(aes(x = BMI, fill = Education)) + geom_histogram(aes(y = ..density..)) + geom_density(aes(y = ..density..)) + guides(fill = &#39;none&#39;) + labs( title = &quot;Examining the effect of education level on BMI&quot;, x = &quot;BMI&quot;, y = &quot;Density&quot;) + facet_wrap(~ Education) 4.6 Your Challenge (do this during the in-person session) Create some new visualisations with either the mpg or NHANES datasets. There are many possible visualisations you could build with either dataset. 4.7 Additional Resources If you are interested in understanding more about ggplot2, you might be interested in watching these two (very) detailed webinars by Thomas Lin Pedersen, one of the developers working on ggplot2 and related packages. End of workshop 4 materials "],["r-markdown.html", "Workshop 5 R Markdown 5.1 Overview 5.2 Your Challenge (do this during the in-person session)", " Workshop 5 R Markdown In this workshop we will see how to generate a report in .html format using R Markdown. Reports written using R Markdown allow you to combine narrative that you’ve written along with R code chunks, and the output associated with those code chunks all in one knitted document. The assignments for this unit need to be produced using R Markdown. 5.1 Overview In the following video I will give you a brief overview of how you can turn a script you have written in R into an R Markdown document that you can ‘knit’ and share with others.       There are many resources available to help you explore the full range of possibilities in R Markdown. A good starting point is the “R Markdown: The Definitive Guide” by Yihui Xie, J. J. Allaire, and Garrett Grolemund. Just click on the image below to be taken to the online version of the book.       You may also be interested in the “R Markdown Cookbook” by Yihui Xie, Christophe Dervieux, and Emily Riederer. Again, just click on the image below to be taken to the online version of the book.       5.2 Your Challenge (do this during the in-person session) Take one of the scripts that you’ve already written - maybe one of the visualisation scripts that you’ve developed - and use it to create an R Markdown document. Add the code from your script to a new R Markdown document in meaningful small code chunks (maybe just a few lines each) - you might have a code chunk that loads your libraries, a chunk that reads in your data, another chunk for wrangling your data, and then separate chunks for each of the visualisations you have written. Before each small chunk of code, add some narrative explaining what the following code chunk does. Tweak the message and warning parameters at the start of each code chunk so that warnings and messages aren’t displayed in the final R Markdown generated .html file. Remember that when you’re knitting an R Markdown document it is working in its own R session - and so can’t access anything in the main R session in which you’ve been writing the R Markdown document itself. This means that your R Markdown must load the libraries needed and read in the data within the document itself. End of workshop 5 materials "],["regression-part-1.html", "Workshop 6 Regression Part 1 6.1 Overview 6.2 Simple Linear Regression", " Workshop 6 Regression Part 1 In this workshop we will explore Simple Regression in the context of the General Linear Model (GLM). You will also have the opportunity to build some regression models where you predict an outcome variable on the basis of one predictor. You will also learn how to run model diagnostics to ensure you are not violating any key assumptions of regression. 6.1 Overview First off I’d like you to watch the following video which starts off by revising the basics of correlation, before examining how we build regressions models.          6.2 Simple Linear Regression After having watched the video above, I’d like you to work through the following simple linear regression example in R. Remember to create a new .Rproj file to keep things organised. 6.2.1 The Packages We Need First we need to install the packages we need. We’re going to install the tidyverse set of packages plus a few others. The package Hmisc allows us to use the rcorr() function for calculating Pearson’s r, and the performance package so we can test our model assumptions. Remember, if you haven’t previously installed these packages on your laptop you first need to type install.packages(\"packagename\") in the console before you can call the library() function for that package. You may also need to install the package see to get the performance package working. If so, do that in the console by typing install.packages(\"see\"). library(tidyverse) library(Hmisc) library(performance) 6.2.2 Import the Data Import the dataset called crime_dataset.csv - this dataset contains population data, housing price index data and crime data for cities in the US. We can use the function head() to display the first few rows of our dataset called “crime”. crime &lt;- read_csv(&quot;https://raw.githubusercontent.com/george-farmer/PCHN63101/refs/heads/main/reg_pt1.csv&quot;) head(crime) ## # A tibble: 6 × 9 ## Year index_nsa `City, State` Population `Violent Crimes` Homicides Rapes ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1975 41.1 Atlanta, GA 490584 8033 185 443 ## 2 1975 30.8 Chicago, IL 3150000 37160 818 1657 ## 3 1975 36.4 Cleveland, OH 659931 10403 288 491 ## 4 1975 20.9 Oakland, CA 337748 5900 111 316 ## 5 1975 20.4 Seattle, WA 503500 3971 52 324 ## 6 NA NA &lt;NA&gt; NA NA NA NA ## # ℹ 2 more variables: Assaults &lt;dbl&gt;, Robberies &lt;dbl&gt; 6.2.3 Tidy the Data First let’s do some wrangling. There is one column that combines both City and State information. Let’s separate that information out into two new columns called “City” and “State” using the function separate(). We’ll also rename the columns to change the name of the “index_nsa” column to “House_price” and get rid of the space in the “Violent Crimes” heading. crime_tidied &lt;- crime %&gt;% separate(col = &quot;City, State&quot;, into = c(&quot;City&quot;, &quot;State&quot;)) %&gt;% rename(House_price = index_nsa) %&gt;% rename(Violent_Crimes = &quot;Violent Crimes&quot;) head(crime_tidied) ## # A tibble: 6 × 10 ## Year House_price City State Population Violent_Crimes Homicides Rapes ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1975 41.1 Atlanta GA 490584 8033 185 443 ## 2 1975 30.8 Chicago IL 3150000 37160 818 1657 ## 3 1975 36.4 Cleveland OH 659931 10403 288 491 ## 4 1975 20.9 Oakland CA 337748 5900 111 316 ## 5 1975 20.4 Seattle WA 503500 3971 52 324 ## 6 NA NA &lt;NA&gt; &lt;NA&gt; NA NA NA NA ## # ℹ 2 more variables: Assaults &lt;dbl&gt;, Robberies &lt;dbl&gt; 6.2.4 Plot the Data We might first think that as population size increases, crime rate also increases. Let’s first build a scatter plot. crime_tidied %&gt;% ggplot(aes(x = Population, y = Violent_Crimes)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_minimal() + theme(text = element_text(size = 13)) + labs(x = &quot;Population&quot;, y = &quot;Violent Crimes&quot;) 6.2.5 Pearson’s r This plot looks pretty interesting. How about calculating Pearson’s r? rcorr(crime_tidied$Population, crime_tidied$Violent_Crimes) ## x y ## x 1.00 0.81 ## y 0.81 1.00 ## ## n ## x y ## x 1714 1708 ## y 1708 1708 ## ## P ## x y ## x 0 ## y 0 Look at the r and p-values - r is =.81 and p &lt; .001. So ~66% of the variance in our Violent_Crimes variable is explained by our Population size variable. Clearly there is a positive relationship between population size and the rate of violent crime. From the plot, we might conclude that the relationship is being overly influenced by crime in a small number of very large cities (top right of the plot above). Let’s exclude cities with populations greater than 2,000,000 crime_filtered &lt;- filter(crime_tidied, Population &lt; 2000000) Now let’s redo the plot. As there are still likely to be quite a lot of points (and thus overplotting with many points appearing roughly in the same place), we can set the alpha parameter to be &lt; 1 in the geom_point() line of code. This parameter corresponds to the translucency of each point. Change it to other values to see what happens. crime_filtered %&gt;% ggplot(aes(x = Population, y = Violent_Crimes)) + geom_point(alpha = .25) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_minimal() + theme(text = element_text(size = 13)) + labs(x = &quot;Population&quot;, y = &quot;Violent Crimes&quot;) And calculate Pearson’s r. rcorr(crime_filtered$Population, crime_filtered$Violent_Crimes) ## x y ## x 1.00 0.69 ## y 0.69 1.00 ## ## n ## x y ## x 1659 1653 ## y 1653 1653 ## ## P ## x y ## x 0 ## y 0 There is still a clear positive relationship (r=.69). Let’s build a linear model. The dataset contains a lot of data and each city appears a number of times (once each year). For our linear model, our observations need to be independent of each other so let’s just focus on the year 2015. That way each city will just appear once. First we apply our filter. crime_filtered &lt;- filter(crime_filtered, Year == 2015) Then we build a plot. I’m using the layer geom_text() to plot the City names and set the check_overlap parameter to TRUE to ensure the labels don’t overlap. crime_filtered %&gt;% ggplot(aes(x = Population, y = Violent_Crimes, label = City)) + geom_point() + geom_text(nudge_y = 500, check_overlap = TRUE) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + xlim(0, 1800000) + theme_minimal() + theme(text = element_text(size = 13)) + labs(x = &quot;Population&quot;, y = &quot;Violent Crimes&quot;) This shows a clear positive linear relationship so let’s work out Pearson’s r. rcorr(crime_filtered$Population, crime_filtered$Violent_Crimes) ## x y ## x 1.00 0.65 ## y 0.65 1.00 ## ## n ## x y ## x 42 40 ## y 40 40 ## ## P ## x y ## x 0 ## y 0 6.2.6 Model the Data Imagine we are a city planner, and we want to know by how much we think violent crimes might increase as a function of population size. In other words, we want to work out how the violent crime rate is predicted by population size. We’re going to build two linear models - one model1 where we’re using the mean of our outcome variable as the predictor, and a second model2 where we are using Population size to predict the Violent Crimes outcome. model1 &lt;- lm(Violent_Crimes ~ 1, data = crime_filtered) model2 &lt;- lm(Violent_Crimes ~ Population, data = crime_filtered) 6.2.7 Checking Our Assumptions Let’s use the check_model() function from the performance package to check the assumptions of our model. check_model(model2, check = c(&quot;homogeneity&quot;,&quot;outliers&quot;,&quot;qq&quot;)) Our dataset is small and so some of our diagnostic plots don’t look great. But for now let’s use the anova() function to see if our model with Population as the predictor is better than the one using just the mean. anova(model1, model2) ## Analysis of Variance Table ## ## Model 1: Violent_Crimes ~ 1 ## Model 2: Violent_Crimes ~ Population ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 39 445568991 ## 2 38 257690819 1 187878173 27.705 5.813e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 It is - the models differ and you’ll see the residual sum of squares (or the error) is less in the second model (which has Population as the predictor). This means the deviation between our observed data and the regression line model model2 is significantly less than the deviation between our observed data and the mean as a model of our data model1. So let’s get the parameter estimates of model2. 6.2.8 Interpreting Our Model summary(model2) ## ## Call: ## lm(formula = Violent_Crimes ~ Population, data = crime_filtered) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5465.8 -1633.4 -809.1 684.3 6213.8 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.443e+02 9.216e+02 1.025 0.312 ## Population 6.963e-03 1.323e-03 5.264 5.81e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2604 on 38 degrees of freedom ## (2 observations deleted due to missingness) ## Multiple R-squared: 0.4217, Adjusted R-squared: 0.4064 ## F-statistic: 27.71 on 1 and 38 DF, p-value: 5.813e-06 The intercept corresponds to where our regression line intercepts the y-axis, and the Population parameter corresponds to the slope of our line. We see that for every increase in population by 1 there is an extra 0.006963 increase in violent crime. For a city with a population of about a million, there will be about 7907 Violent Crimes. We calculate this by multiplying the estimate of our predictor (0.006963) by 1,000,000 and then adding the intercept (944.3). This gives us 7907.3 crimes - which tallys with what you see in our regression line above. We may have a few outliers - how would you figure out what those were? Try excluding any outliers you find and re-building your model. End of workshop 6 materials "],["regression-part-2.html", "Workshop 7 Regression Part 2 7.1 Overview 7.2 Multiple Regression 7.3 Stepwise regression", " Workshop 7 Regression Part 2 In this workshop we will explore Multiple Regression in the context of the General Linear Model (GLM). Multiple Regression builds on Simple Regression, except that instead of having one predictor (as is the case with Simple Regression) we will be dealing with multiple predictors. Again, you will have the opportunity to build some regression models and use various methods to decide which one is ‘best’. You will also learn how to run model diagnostics for these models as you did in the case of Simple Regression. 7.1 Overview First off I’d like you to watch the following video which builds on the first regression workshop. We explore how to build regression models with more than one predictor in R using the lm() function, test our model assumptions, and interpret the output. We also look at different ways of building stepwise regression models with multiple predictors.       7.2 Multiple Regression In standard multiple regression all the independent variables (IVs) are entered into the equation and evaluated for their contribution at the same time. Let’s work through a specific example. As you read through the analysis below run the code on your machine. An educational psychologist conducted a study that investigated the psycholinguistic variables that contribute to spelling performance in primary school children aged between 7- and 9-years. The researcher presented children with 48 words that varied systematically according to certain features such as age of acquisition, word frequency, word length, and imageability. The psychologist wants to check whether performance on the test accurately reflected children’s spelling ability as estimated by a standardised spelling test. That is, the psychologist wants to check whether her test was appropriate. Children’s chronological age (in months) (age), their reading age (RA), their standardised reading age (std_RA), and their standardised spelling score (std_SPELL) were chosen as predictor variables. The criterion variable (Y) was the percentage correct spelling (corr_spell) score attained by each child using the list of 48 words. First we need to load the packages we need - the require function assumes they are already on your machine. If they are not, then you need to install.packages (“packagename”) first: 7.2.1 The Packages We Need library(tidyverse) # Load the tidyverse packages library(Hmisc) # Needed for correlation library(MASS) # Needed for maths functions library(car) # Needed for VIF calculation library(olsrr) # Needed for stepwise regression ## Warning: package &#39;olsrr&#39; was built under R version 4.3.3 library(performance) # Needed to check model assumptions 7.2.2 Import the Data You now need to read in the data file. MRes_tut2 &lt;- read_csv(&quot;https://raw.githubusercontent.com/ajstewartlang/10_glm_regression_pt2/master/data/MRes_tut2.csv&quot;) Examining Possible Relationships Before we start, let’s look at the relationships between our IVs (predictors) and our DV (outcome). We can plot graphs depicting the correlations. We’ll plot test performance against each of our four predictors in turn: ggplot(MRes_tut2, aes(x = age, y = corr_spell)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_minimal() + theme(text = element_text(size = 13)) ggplot(MRes_tut2, aes(x = RA, y = corr_spell)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_minimal() + theme(text = element_text(size = 13)) ggplot(MRes_tut2, aes(x = std_RA, y = corr_spell)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_minimal() + theme(text = element_text(size = 13)) ggplot(MRes_tut2, aes(x = std_SPELL, y = corr_spell)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_minimal() + theme(text = element_text(size = 13)) 7.2.3 Model the Data We are going to do hierarchical regression first - we’ll build one model (which we’ll call model0) that is the mean of our outcome variable, and another model (model1) which contains all our predictors: model0 &lt;- lm(corr_spell ~ 1, data = MRes_tut2) model1 &lt;- lm(corr_spell ~ age + RA + std_RA + std_SPELL, data = MRes_tut2) Let’s compare them to each other: anova(model0, model1) ## Analysis of Variance Table ## ## Model 1: corr_spell ~ 1 ## Model 2: corr_spell ~ age + RA + std_RA + std_SPELL ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 46 26348.4 ## 2 42 3901.1 4 22447 60.417 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We see that the models differ from each other (look a the p-value of the comparison) and that the model with the four predictors has the lower Residuals (RSS) value meaning there is less error between the model and the observed data relative to the simpler intercept-only model (i.e., the mean) and the observed data. Checking our Assumptions OK, so they differ - now let’s plot information about our model assumptions - remember, we are particularly interested in Cook’s distance values for our case… check_model(model1, check = c(&quot;homogeneity&quot;,&quot;outliers&quot;,&quot;qq&quot;,&quot;vif&quot;)) The errors looks fairly equally distributed along our fitted values (homogeneity of variance) - although a little worse for high fitted values - and from the Q-Q plot we can tell they look fairly normal (they should follow the diagonal). How about influential cases? So, Case 10 looks a bit dodgy - it has a high Cook’s Distance value (indicated by it falling outside of the dashed lines) - which suggests it is having a disproportionate effect on our model. Let’s exclude it using the filter() function - the symbol != means ‘does not equal’ so we are selecting values other than Case 10. Dropping an Influential Case MRes_tut2_drop10 &lt;- filter(MRes_tut2, case != &quot;10&quot;) Re(model) the Data We now create another model (model2) which doesn’t include Case 10. model2 &lt;- lm(corr_spell ~ age + RA + std_RA + std_SPELL, data = MRes_tut2_drop10) Let’s check the model assumptions again using check_model(). Checking our Assumptions check_model(model2, check = c(&quot;homogeneity&quot;,&quot;outliers&quot;,&quot;qq&quot;,&quot;vif&quot;)) Now, let’s look at the multicollinearity values measured by VIF: vif(model2) ## age RA std_RA std_SPELL ## 3.843462 14.763168 14.672084 3.140457 It looks like RA and std_RA are problematic. We can look at the correlation between them using the rcorr() function: rcorr(MRes_tut2_drop10$RA, MRes_tut2_drop10$std_RA) ## x y ## x 1.00 0.88 ## y 0.88 1.00 ## ## n= 46 ## ## ## P ## x y ## x 0 ## y 0 Re(model) the Data The correlation is pretty high (0.88), so let’s exclude the predictor with the highest VIF value (which is RA) and build a new model: model3 &lt;- lm(corr_spell ~ age + std_RA + std_SPELL, data = MRes_tut2_drop10) vif(model3) ## age std_RA std_SPELL ## 1.190827 2.636186 2.821235 Checking our Assumptions These values look ok now. Let’s check the model assumptions again. check_model(model3, check = c(&quot;homogeneity&quot;,&quot;outliers&quot;,&quot;qq&quot;,&quot;vif&quot;)) Summary of our Model Now let’s generate the coefficients as this looks like a sensible model. summary(model3) ## ## Call: ## lm(formula = corr_spell ~ age + std_RA + std_SPELL, data = MRes_tut2_drop10) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.801 -6.907 1.327 5.155 24.669 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -209.4400 26.7210 -7.838 9.43e-10 *** ## age 1.1033 0.2133 5.172 6.09e-06 *** ## std_RA 0.3804 0.1385 2.747 0.00883 ** ## std_SPELL 1.2107 0.1650 7.339 4.78e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.824 on 42 degrees of freedom ## Multiple R-squared: 0.8388, Adjusted R-squared: 0.8273 ## F-statistic: 72.87 on 3 and 42 DF, p-value: &lt; 2.2e-16 model0 &lt;- lm(corr_spell ~ 1, data = MRes_tut2_drop10) anova(model3, model0) ## Analysis of Variance Table ## ## Model 1: corr_spell ~ age + std_RA + std_SPELL ## Model 2: corr_spell ~ 1 ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 42 4053.5 ## 2 45 25151.0 -3 -21098 72.866 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We’d write our equation as something like: Spelled correct = -209.44 + 1.10(age) + 0.38(std_RA) + 1.21(std_SPELL) + residual 7.3 Stepwise regression We can also do stepwise regression - forwards is when you start with the null model and predictors are added until they don’t explain any more variance, backwards is when you start with the full model and remove predictors until removal starts affecting your model’s predictive ability. Let’s keep case 10 dropped and also drop the high VIF predictor (RA). This is handy for models with lots of predictors where the order in sequential regression is not obvious. 7.3.1 Model the Data model0 &lt;- lm(corr_spell ~ 1, data = MRes_tut2_drop10) model1 &lt;- lm(corr_spell ~ age + std_RA + std_SPELL, data = MRes_tut2_drop10) Let’s do stepwise forwards: steplimitsf &lt;- step(model0, scope = list (lower = model0, upper = model1), direction = &quot;forward&quot;) ## Start: AIC=291.98 ## corr_spell ~ 1 ## ## Df Sum of Sq RSS AIC ## + std_SPELL 1 17802.2 7348.8 237.39 ## + std_RA 1 14780.1 10370.9 253.23 ## &lt;none&gt; 25151.0 291.98 ## + age 1 48.7 25102.3 293.89 ## ## Step: AIC=237.39 ## corr_spell ~ std_SPELL ## ## Df Sum of Sq RSS AIC ## + age 1 2567.20 4781.6 219.62 ## + std_RA 1 714.14 6634.7 234.69 ## &lt;none&gt; 7348.8 237.39 ## ## Step: AIC=219.62 ## corr_spell ~ std_SPELL + age ## ## Df Sum of Sq RSS AIC ## + std_RA 1 728.1 4053.5 214.02 ## &lt;none&gt; 4781.6 219.62 ## ## Step: AIC=214.02 ## corr_spell ~ std_SPELL + age + std_RA summary(steplimitsf) ## ## Call: ## lm(formula = corr_spell ~ std_SPELL + age + std_RA, data = MRes_tut2_drop10) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.801 -6.907 1.327 5.155 24.669 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -209.4400 26.7210 -7.838 9.43e-10 *** ## std_SPELL 1.2107 0.1650 7.339 4.78e-09 *** ## age 1.1033 0.2133 5.172 6.09e-06 *** ## std_RA 0.3804 0.1385 2.747 0.00883 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.824 on 42 degrees of freedom ## Multiple R-squared: 0.8388, Adjusted R-squared: 0.8273 ## F-statistic: 72.87 on 3 and 42 DF, p-value: &lt; 2.2e-16 Stepwise backwards: steplimitsb &lt;- step(model1, direction = &quot;back&quot;) ## Start: AIC=214.02 ## corr_spell ~ age + std_RA + std_SPELL ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 4053.5 214.02 ## - std_RA 1 728.1 4781.6 219.62 ## - age 1 2581.2 6634.7 234.69 ## - std_SPELL 1 5198.3 9251.8 249.98 summary(steplimitsb) ## ## Call: ## lm(formula = corr_spell ~ age + std_RA + std_SPELL, data = MRes_tut2_drop10) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.801 -6.907 1.327 5.155 24.669 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -209.4400 26.7210 -7.838 9.43e-10 *** ## age 1.1033 0.2133 5.172 6.09e-06 *** ## std_RA 0.3804 0.1385 2.747 0.00883 ** ## std_SPELL 1.2107 0.1650 7.339 4.78e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.824 on 42 degrees of freedom ## Multiple R-squared: 0.8388, Adjusted R-squared: 0.8273 ## F-statistic: 72.87 on 3 and 42 DF, p-value: &lt; 2.2e-16 And stepwise using both forwards and backwards procedures: steplimitsboth &lt;- step(model0, scope = list (upper = model1), direction = &quot;both&quot;) ## Start: AIC=291.98 ## corr_spell ~ 1 ## ## Df Sum of Sq RSS AIC ## + std_SPELL 1 17802.2 7348.8 237.39 ## + std_RA 1 14780.1 10370.9 253.23 ## &lt;none&gt; 25151.0 291.98 ## + age 1 48.7 25102.3 293.89 ## ## Step: AIC=237.39 ## corr_spell ~ std_SPELL ## ## Df Sum of Sq RSS AIC ## + age 1 2567.2 4781.6 219.62 ## + std_RA 1 714.1 6634.7 234.69 ## &lt;none&gt; 7348.8 237.39 ## - std_SPELL 1 17802.2 25151.0 291.98 ## ## Step: AIC=219.62 ## corr_spell ~ std_SPELL + age ## ## Df Sum of Sq RSS AIC ## + std_RA 1 728.1 4053.5 214.02 ## &lt;none&gt; 4781.6 219.62 ## - age 1 2567.2 7348.8 237.39 ## - std_SPELL 1 20320.7 25102.3 293.89 ## ## Step: AIC=214.02 ## corr_spell ~ std_SPELL + age + std_RA ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 4053.5 214.02 ## - std_RA 1 728.1 4781.6 219.62 ## - age 1 2581.2 6634.7 234.69 ## - std_SPELL 1 5198.3 9251.8 249.98 Checking our Assumptions check_model(steplimitsboth, check = c(&quot;homogeneity&quot;,&quot;outliers&quot;,&quot;qq&quot;,&quot;vif&quot;)) These look ok. summary(steplimitsboth) ## ## Call: ## lm(formula = corr_spell ~ std_SPELL + age + std_RA, data = MRes_tut2_drop10) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.801 -6.907 1.327 5.155 24.669 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -209.4400 26.7210 -7.838 9.43e-10 *** ## std_SPELL 1.2107 0.1650 7.339 4.78e-09 *** ## age 1.1033 0.2133 5.172 6.09e-06 *** ## std_RA 0.3804 0.1385 2.747 0.00883 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.824 on 42 degrees of freedom ## Multiple R-squared: 0.8388, Adjusted R-squared: 0.8273 ## F-statistic: 72.87 on 3 and 42 DF, p-value: &lt; 2.2e-16 You’ll see that the same final model is arrived it in each case. We have three significant predictors. Entering predictors based on their p-values We can also use the ols_step_forward_p() function from the package olsrr - this works by finding the best model by entering the predictors based on their p values. There are other methods within the olsrr package. To see them, type olsrr:: into the console window. pmodel &lt;- ols_step_forward_p(model1) pmodel ## ## ## Stepwise Summary ## ------------------------------------------------------------------------- ## Step Variable AIC SBC SBIC R2 Adj. R2 ## ------------------------------------------------------------------------- ## 0 Base Model 424.527 428.184 290.981 0.00000 0.00000 ## 1 std_SPELL 369.930 375.416 237.491 0.70781 0.70117 ## 2 age 352.161 359.476 221.180 0.80988 0.80104 ## 3 std_RA 346.562 355.706 216.764 0.83883 0.82732 ## ------------------------------------------------------------------------- ## ## Final Model Output ## ------------------ ## ## Model Summary ## --------------------------------------------------------------- ## R 0.916 RMSE 9.387 ## R-Squared 0.839 MSE 88.120 ## Adj. R-Squared 0.827 Coef. Var 16.645 ## Pred R-Squared 0.807 AIC 346.562 ## MAE 7.512 SBC 355.706 ## --------------------------------------------------------------- ## RMSE: Root Mean Square Error ## MSE: Mean Square Error ## MAE: Mean Absolute Error ## AIC: Akaike Information Criteria ## SBC: Schwarz Bayesian Criteria ## ## ANOVA ## --------------------------------------------------------------------- ## Sum of ## Squares DF Mean Square F Sig. ## --------------------------------------------------------------------- ## Regression 21097.470 3 7032.490 72.866 0.0000 ## Residual 4053.508 42 96.512 ## Total 25150.978 45 ## --------------------------------------------------------------------- ## ## Parameter Estimates ## ---------------------------------------------------------------------------------------------- ## model Beta Std. Error Std. Beta t Sig lower upper ## ---------------------------------------------------------------------------------------------- ## (Intercept) -209.440 26.721 -7.838 0.000 -263.365 -155.515 ## std_SPELL 1.211 0.165 0.764 7.339 0.000 0.878 1.544 ## age 1.103 0.213 0.350 5.172 0.000 0.673 1.534 ## std_RA 0.380 0.138 0.276 2.747 0.009 0.101 0.660 ## ---------------------------------------------------------------------------------------------- In this case, the model with the lowest AIC value is also the one arrived at statistically via the sequential procedure based on p-values - but this may not always be the case. End of workshop 7 materials "],["anova-part-1.html", "Workshop 8 ANOVA Part 1 8.1 Overview 8.2 Between Participants ANOVA 8.3 Repeated Measures ANOVA 8.4 Factorial ANOVA 8.5 Your Challenge (do this during the in-person session) 8.6 Answers", " Workshop 8 ANOVA Part 1 In this workshop we will explore Analysis of Variance (ANOVA) in the context of model building in R for between participants designs, repeated measures designs, and factorial designs. You will learn how to use the {afex} package for building models with Type III Sums of Squares, and the {emmeans} package to conduct follow up tests to explore main effects and interactions. 8.1 Overview We will begin by exploring why we tend to use ANOVA (rather than multiple t-tests), before moving on to some examples of ANOVA for between participants and repeated measures designs.       8.2 Between Participants ANOVA You will now build your first ANOVA in R for a between participants design. 8.2.1 Loading our Packages First of all, we need to load the three packages we will be using - they are {tidyverse}, {afex}, and {emmeans}. The {afex} package is the one we use for conducting ANOVA. We use the {emmeans} package for running follow-up tests on the ANOVA model that we will be building. library(tidyverse) library(afex) library(emmeans) 8.2.2 Reading in our Data We have 45 participants in a between participants design where we are interested in the effect of beverage consumed on ability on a motor task. Our experimental factor (beverage type) has three levels. These are Water vs. Single Espresso vs. Double Espresso, and Ability is our DV measured on a continuous scale. Let’s read in our data. my_data &lt;- read_csv(&quot;https://raw.githubusercontent.com/george-farmer/PCHN63101/refs/heads/main/cond.csv&quot;) head(my_data) ## # A tibble: 6 × 3 ## Participant Condition Ability ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Water 4.82 ## 2 2 Water 5.41 ## 3 3 Water 5.73 ## 4 4 Water 4.36 ## 5 5 Water 5.47 ## 6 6 Water 5.50 We see that we have three variables, but our experimental variable Condition is not coded as a factor. Let’s fix that… my_data_tidied &lt;- my_data %&gt;% mutate(Condition = factor(Condition)) head(my_data_tidied) ## # A tibble: 6 × 3 ## Participant Condition Ability ## &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 1 Water 4.82 ## 2 2 Water 5.41 ## 3 3 Water 5.73 ## 4 4 Water 4.36 ## 5 5 Water 5.47 ## 6 6 Water 5.50 8.2.3 Summarising our Data Let’s work our some summary statistics and build a data visualisation next. my_data_tidied %&gt;% group_by(Condition) %&gt;% summarise(mean = mean(Ability), sd = sd(Ability)) ## # A tibble: 3 × 3 ## Condition mean sd ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Double Espresso 8.89 0.467 ## 2 Single Espresso 6.99 0.419 ## 3 Water 5.17 0.362 8.2.4 Visualising our Data set.seed(1234) my_data_tidied %&gt;% ggplot(aes(x = Condition, y = Ability, colour = Condition)) + geom_violin() + geom_jitter(width = .1) + guides(colour = &#39;none&#39;) + stat_summary(fun.data = &quot;mean_cl_boot&quot;, colour = &quot;black&quot;) + theme_minimal() + theme(text = element_text(size = 13)) We have built a visualisation where we have plotted the raw data points using the geom_jitter() function, and the shape of the distribution for each condition using the geom_violin() function. We have also added some summary data in the form of the Mean and Confidence Intervals around the Mean using the stat_summary() function. 8.2.5 Building our ANOVA Model Let’s now build our model using the aov_ez() function in the {afex} package. We use id to specify which variable in our dataframe identifies participants. dv stands for dependent variable, and we use the between argument to identify the variable containing our between subjects conditions. Lastly we specify the dataframe that aov_ez() should use. We are going to map the output of the aov_ez() function onto a variable I’m calling model. This means that the ANOVA results will be stored in this variable and will allow us to access them later. model &lt;- aov_ez(id = &#39;Participant&#39;, dv = &#39;Ability&#39;, between = &#39;Condition&#39;, data = my_data_tidied) ## Contrasts set to contr.sum for the following variables: Condition To get the output of the ANOVA, we can use the summary() function with our newly created model. 8.2.6 Interpreting the Model Output summary(model) ## Anova Table (Type 3 tests) ## ## Response: Ability ## num Df den Df MSE F ges Pr(&gt;F) ## Condition 2 42 0.17484 297.05 0.93397 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The effect size (ges) is generalised eta squared and for designs with more than one factor it can be a useful indicator of how much variance in the dependent variable can be explained by each factor (plus any interactions between factors). So, we know there is an effect in our model - the F-value is pretty big and the p-value pretty small) but we can’t know what’s driving the difference yet. We need to run some pairwise comparisons using the emmeans() function to tell us what mean(s) differ(s) from what other mean(s). emmeans(model, pairwise ~ Condition) ## $emmeans ## Condition emmean SE df lower.CL upper.CL ## Double Espresso 8.89 0.108 42 8.67 9.10 ## Single Espresso 6.99 0.108 42 6.77 7.20 ## Water 5.17 0.108 42 4.95 5.38 ## ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## Double Espresso - Single Espresso 1.90 0.153 42 12.453 &lt;.0001 ## Double Espresso - Water 3.72 0.153 42 24.372 &lt;.0001 ## Single Espresso - Water 1.82 0.153 42 11.920 &lt;.0001 ## ## P value adjustment: tukey method for comparing a family of 3 estimates Note that the default adjustment for multiple comparisons is Tukey’s. We can change that by adding an extra parameter to our model such as adjust = \"bonferonni\"). In this case, it doesn’t make any difference to our comparisons. emmeans(model, pairwise ~ Condition, adjust = &quot;bonferroni&quot;) ## $emmeans ## Condition emmean SE df lower.CL upper.CL ## Double Espresso 8.89 0.108 42 8.67 9.10 ## Single Espresso 6.99 0.108 42 6.77 7.20 ## Water 5.17 0.108 42 4.95 5.38 ## ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## Double Espresso - Single Espresso 1.90 0.153 42 12.453 &lt;.0001 ## Double Espresso - Water 3.72 0.153 42 24.372 &lt;.0001 ## Single Espresso - Water 1.82 0.153 42 11.920 &lt;.0001 ## ## P value adjustment: bonferroni method for 3 tests We found a significant effect of Beverage type (F (2,42) = 297.05, p &lt; .001, generalised η2 = .93). Tukey comparisons revealed that the Water group performed significantly worse than the Single Espresso Group (p &lt; .001), that the Water group performed significantly worse than the Double Espresso Group (p &lt; .001), and that the Single Espresso Group performed significantly worse than the Double Espresso Group (p &lt; .001). In other words, drinking some coffee improves motor performance relative to drinking water, and drinking a lot of coffee improves motor performance even more.    8.3 Repeated Measures ANOVA Let’s imagine we have an experiment where we asked 32 participants to learn how to pronounce words of differing levels of complexity - Very Easy, Easy, Hard, and Very Hard. They were presented with these words in an initial exposure phase. After a 30 minute break we tested participants by asking them to say the words out loud when they appeared on a computer screen. We recorded their times in seconds. We want to know whether there is a difference in their response times for each level of word complexity. 8.3.1 Reading in our Data First we read in the data. rm_data &lt;- read_csv(&quot;https://raw.githubusercontent.com/george-farmer/PCHN63101/refs/heads/main/rm_data.csv&quot;) head(rm_data) ## # A tibble: 6 × 3 ## Participant Condition RT ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Very Easy 1.25 ## 2 2 Very Easy 1.16 ## 3 3 Very Easy 1.12 ## 4 4 Very Easy 1.33 ## 5 5 Very Easy 1.16 ## 6 6 Very Easy 1.15 We can see from the head() function that Condition isn’t yet coded as a factor. Let’s fix that. rm_data_tidied &lt;- rm_data %&gt;% mutate(Condition = factor(Condition)) head(rm_data_tidied) ## # A tibble: 6 × 3 ## Participant Condition RT ## &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 1 Very Easy 1.25 ## 2 2 Very Easy 1.16 ## 3 3 Very Easy 1.12 ## 4 4 Very Easy 1.33 ## 5 5 Very Easy 1.16 ## 6 6 Very Easy 1.15 8.3.2 Summarising our Data Let’s generate the Mean and Standard Deviation for each of our four conditions. rm_data_tidied %&gt;% group_by(Condition) %&gt;% summarise(mean = mean(RT), sd = sd (RT)) ## # A tibble: 4 × 3 ## Condition mean sd ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Easy 1.23 0.0610 ## 2 Hard 1.39 0.118 ## 3 Very Easy 1.20 0.0511 ## 4 Very Hard 1.87 0.187 8.3.3 Visualising our Data And visualise the data - note here that I am using the fct_reorder() function to reorder the levels of our factor based on the RT. This can be useful to make our viusalisations more easily interpretable. rm_data_tidied %&gt;% ggplot(aes(x = fct_reorder(Condition, RT), y = RT, colour = Condition)) + geom_violin() + geom_jitter(width = .1) + guides(colour = &#39;none&#39;) + stat_summary(fun.data = &quot;mean_cl_boot&quot;, colour = &quot;black&quot;) + theme_minimal() + theme(text = element_text(size = 13)) + labs(x = &quot;Condition&quot;, y = &quot;RT (s)&quot;) 8.3.4 Building our ANOVA Model We build our ANOVA model in a similar way to before. Except in this case we use the within argument rather than the between argument in order to capture the fact that Condition is a repeated measures factor. rm_model &lt;- aov_ez(id = &#39;Participant&#39;, dv = &#39;RT&#39;, within = &#39;Condition&#39;, data = rm_data_tidied) 8.3.5 Interpreting the Model Output We extract the summary of our model in the same way we did for the between participants ANOVA. summary(rm_model) ## ## Univariate Type III Repeated-Measures ANOVA Assuming Sphericity ## ## Sum Sq num Df Error SS den Df F value Pr(&gt;F) ## (Intercept) 259.07 1 0.50313 31 15962.33 &lt; 2.2e-16 *** ## Condition 9.27 3 1.20624 93 238.23 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Mauchly Tests for Sphericity ## ## Test statistic p-value ## Condition 0.38404 3.0211e-05 ## ## ## Greenhouse-Geisser and Huynh-Feldt Corrections ## for Departure from Sphericity ## ## GG eps Pr(&gt;F[GG]) ## Condition 0.65596 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## HF eps Pr(&gt;F[HF]) ## Condition 0.7000534 3.359493e-31 With this option, we didn’t get the effect size measure in our measure. We can generate that though by asking for our model to be presented in anova format using the anova() function. anova(rm_model) ## Anova Table (Type 3 tests) ## ## Response: RT ## num Df den Df MSE F ges Pr(&gt;F) ## Condition 1.9679 61.004 0.019773 238.23 0.84431 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The effect size is measured by ges and is the recommended effect size measure for repeated measures designs (Bakeman, 2005). Note the dfs in this output are always corrected as if there is a violation of sphericity (violated when the variances of the differences between all possible pairs of within-subject conditions (i.e., levels of the independent variable) are not equal) - to be conservative (and to avoid Type I errors) we might be better off to always choose these corrected dfs. From this, we can see we have effect of Condition. But we don’t know where the differences lie between the different levels of our factor. So we use the emmeans() function to find that out. Here we will be using the Bonferroni correction for multiple comparisons. emmeans(rm_model, pairwise ~ Condition, adjust = &quot;Bonferroni&quot;) ## $emmeans ## Condition emmean SE df lower.CL upper.CL ## Easy 1.23 0.01079 31 1.21 1.25 ## Hard 1.39 0.02085 31 1.35 1.43 ## Very.Easy 1.20 0.00904 31 1.18 1.22 ## Very.Hard 1.87 0.03302 31 1.80 1.94 ## ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## Easy - Hard -0.1633 0.0226 31 -7.225 &lt;.0001 ## Easy - Very.Easy 0.0285 0.0143 31 1.986 0.3353 ## Easy - Very.Hard -0.6430 0.0338 31 -19.014 &lt;.0001 ## Hard - Very.Easy 0.1917 0.0230 31 8.354 &lt;.0001 ## Hard - Very.Hard -0.4797 0.0363 31 -13.220 &lt;.0001 ## Very.Easy - Very.Hard -0.6715 0.0341 31 -19.710 &lt;.0001 ## ## P value adjustment: bonferroni method for 6 tests From the above we can see that all conditions differ from all other conditions, apart from the Easy vs. Very Easy comparison which is not significant. 8.4 Factorial ANOVA       Imagine the case where we’re interested in the effect of positive vs negative contexts on how quickly (in milliseconds) people respond to positive vs negative sentences. We think there might be a priming effect (i.e., people are quicker to respond to positive sentences after positive contexts than they are after negative contexts, and vice versa). So we have two factors (context, sentence), each with two levels (positive, negative). This is what’s know as a full factorial design where every participant takes part in every condition. 8.4.1 Reading in our Data factorial_data &lt;- read_csv(&quot;https://raw.githubusercontent.com/george-farmer/PCHN63101/refs/heads/main/factorial_data.csv&quot;) head(factorial_data) ## # A tibble: 6 × 5 ## Subject Item RT Sentence Context ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 3 1270 Positive Negative ## 2 1 7 739 Positive Negative ## 3 1 11 982 Positive Negative ## 4 1 15 1291 Positive Negative ## 5 1 19 1734 Positive Negative ## 6 1 23 1757 Positive Negative Again we see that our two experimental factors are not coded as factors so let’s fix that. factorial_data_tidied &lt;- factorial_data %&gt;% mutate(Sentence = factor(Sentence), Context = factor(Context)) head(factorial_data_tidied) ## # A tibble: 6 × 5 ## Subject Item RT Sentence Context ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; ## 1 1 3 1270 Positive Negative ## 2 1 7 739 Positive Negative ## 3 1 11 982 Positive Negative ## 4 1 15 1291 Positive Negative ## 5 1 19 1734 Positive Negative ## 6 1 23 1757 Positive Negative 8.4.2 Summarising our Data Let’s generate some summary statistics - note, we specify our two grouping variables in the group_by() function call. factorial_data_tidied %&gt;% group_by(Context, Sentence) %&gt;% summarise(mean_rt = mean(RT), sd_rt = sd(RT)) ## `summarise()` has grouped output by &#39;Context&#39;. You can override using the ## `.groups` argument. ## # A tibble: 4 × 4 ## # Groups: Context [2] ## Context Sentence mean_rt sd_rt ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Negative Negative 1474. 729. ## 2 Negative Positive NA NA ## 3 Positive Negative NA NA ## 4 Positive Positive 1579. 841. We have NA for two conditions suggesting we have missing data somewhere in our dataset. We’re going to use a new package now, called {visdat}. It allows us to visualise our dataset using the vis_dat() function and to visualise missing data using the vis_miss() function. library(visdat) vis_miss(factorial_data_tidied) We can see in the above visualisation that we do indeed have some missing data. We need to tell R what we want it to do with that. We can use the na.rm = TRUE parameter to tell it we want missing data to be ignored. factorial_data_tidied %&gt;% group_by(Context, Sentence) %&gt;% summarise(mean_rt = mean(RT, na.rm = TRUE), sd_rt = sd(RT, na.rm = TRUE)) ## `summarise()` has grouped output by &#39;Context&#39;. You can override using the ## `.groups` argument. ## # A tibble: 4 × 4 ## # Groups: Context [2] ## Context Sentence mean_rt sd_rt ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Negative Negative 1474. 729. ## 2 Negative Positive 1595. 887. ## 3 Positive Negative 1633. 877. ## 4 Positive Positive 1579. 841. Now we have the summary statistics that we expect. 8.4.3 Visualising our Data We can use a modification of the ggplot() code we’ve used above to generate our visualisation. Note, I am filtering our the missing data using the filter() function before we start our plot. I am also specifying that we’re wanting to plot a combination of our two factors in the aes() definition using Context:Sentence. There are further things we could modify to improve the look of this graph. Can you figure out ways in which the labelling of the two factors could be clearer? factorial_data_tidied %&gt;% filter(!is.na(RT)) %&gt;% ggplot(aes(x = Context:Sentence, y = RT, colour = Context:Sentence)) + geom_violin() + geom_jitter(width = .1, alpha = .25) + guides(colour = &#39;none&#39;) + stat_summary(fun.data = &quot;mean_cl_boot&quot;, colour = &quot;black&quot;) + theme_minimal() + theme(text = element_text(size = 13)) + labs(x = &quot;Context X Sentence&quot;, y = &quot;RT (ms)&quot;) 8.4.4 Building our ANOVA Model We have our data in long format where every row is one observation. We haven’t done any data aggregation. The aov_ez() function will do this for us as ANOVA models need to be built over means (not raw data). The syntax is very similar to what we ran previously, although this time you’ll see we are providing two terms to the within argument. We do this by combining them like this c('Context','Sentence'). This term corresponds to two main effects, aov_ez will also automatically calculate the interaction between them. By setting na.rm to be TRUE, we are telling the analysis to ignore individual trials where there might be missing data - effectively this calculates the condition means over the data that is present (and ignores trials where it is missing). The fun_aggregate = mean argument tells aov_ez() to use means when aggregating, this is the default behaviour anyway, but specifying it here avoids us getting a warning message about it. model_subjects &lt;- aov_ez(id=&#39;Subject&#39;, dv=&#39;RT&#39;, within = c(&#39;Context&#39;,&#39;Sentence&#39;), na.rm = TRUE, fun_aggregate = mean, data = factorial_data_tidied) We can generate the output using the anova() function as we did earlier. anova(model_subjects) ## Anova Table (Type 3 tests) ## ## Response: RT ## num Df den Df MSE F ges Pr(&gt;F) ## Context 1 59 90195 3.1767 0.0060231 0.07984 . ## Sentence 1 59 124547 0.6283 0.0016524 0.43114 ## Context:Sentence 1 59 93889 4.5967 0.0090449 0.03616 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Let’s now interpret our ANOVA and we will set the error correction adjustment to equal none as only some of the comparisons actually make theoretical sense - these are the ones where we’re comparing like with like - Sentences of the same type (Positive or Negative) preceded by one version of our Context factor vs. the other. emmeans(model_subjects, pairwise ~ Context * Sentence, adjust = &quot;none&quot;) ## $emmeans ## Context Sentence emmean SE df lower.CL upper.CL ## Negative Negative 1474 51.1 59 1372 1576 ## Positive Negative 1628 58.9 59 1510 1746 ## Negative Positive 1595 56.5 59 1482 1708 ## Positive Positive 1579 63.9 59 1451 1707 ## ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## Negative Negative - Positive Negative -153.9 50.0 59 -3.075 0.0032 ## Negative Negative - Negative Positive -120.9 63.4 59 -1.908 0.0612 ## Negative Negative - Positive Positive -105.2 53.5 59 -1.966 0.0540 ## Positive Negative - Negative Positive 33.0 65.5 59 0.503 0.6165 ## Positive Negative - Positive Positive 48.7 57.1 59 0.852 0.3976 ## Negative Positive - Positive Positive 15.7 60.3 59 0.261 0.7953 The key comparisons are the Negative Negative - Positive Negative and the Negative Positive - Positive Positive ones. In the first case, we are comparing reaction times to Negative Sentences preceded by Negative vs. Positive Contexts, while in the second we are comparing reaction times to Positive Sentences preceded by Negative vs. Positive Contexts. We can manually correct for multiple comparisons (which in this case is 2) by multiplying the corresponding p-values by 2 (and putting a limit of 1 on the maximum p-value possible). In this case, the first key comparison is significant (p = .0064) while the second is not. We might write up the results like: We conducted a 2 (Context: Positive vs. Negative) x 2 (Sentence: Positive vs. Negative) repeated measures ANOVA to investigate the influence of Context valence on reaction times to Sentences of Positive or Negative valence. The ANOVA revealed no effect of Sentence (F &lt; 1), no effect of Context (F(1, 59) = 3.18, p = .080, ηG2 = .006), but an interaction between Sentence and Context (F(1, 59) = 4.60, p = .036, ηG2 = .009). The interaction was interpreted by conducting Bonferroni-corrected pairwise companions. These comparisons revealed that the interaction was driven by Negative Sentences being processed faster in Negative vs. Positive Contexts (1,474 ms. vs. 1,628 ms., t(118) = 3.08, p = .0064) while Positive Sentences were read at similar speeds in Negative vs. Positive Contexts (1,595 ms. vs. 1,579 ms., t(118) = .261, p = 1). 8.5 Your Challenge (do this during the in-person session) I would now like you to work on the following questions on your own. 8.5.1 Question 1 Our first data file is called ANOVA_data1.csv and can be found here: https://raw.githubusercontent.com/george-farmer/PCHN63101/refs/heads/main/ANOVA_data1.csv 24 participants responded to a word that was either common (i.e., high lexical frequency) or rare (i.e., low lexical frequency). This is our IV and is coded as ‘high’ vs. low’. Our DV is reaction time and is coded as ‘RT’. Subject number is coded as ‘Subject’. We want to know whether there is a difference between conditions (and if so, where that difference lies). Visualise the data, generate descriptives, and run the appropriate ANOVA to determine whether our independent variable (Condition) has an influence on our dependent variable (RT). 8.5.2 Question 2 Our second data file is called ANOVA_data2.csv and can be found here: https://raw.githubusercontent.com/george-farmer/PCHN63101/refs/heads/main/ANOVA_data2.csv These data are also from a reaction time experiment but with a slightly more complex design.48 participants responded to a word that differed in how frequent it was. This factor is between participants and we have four levels coded as ‘very low’, ‘low’, ‘high’, and ‘very high’. Our DV is reaction time and is coded as ‘RT’. Subject number is coded as ‘Subject’. We want to know if there is a difference between our conditions (and if so, where that difference lies). 8.5.3 Question 3 Our third data file is called ANOVA_data3.csv and can be found here: https://raw.githubusercontent.com/george-farmer/PCHN63101/refs/heads/main/ANOVA_data3.csv These data are from a 2 x 2 repeated measures reaction time experiment. We were interested in how quickly participants could respond to images that were Large vs. Small and in Colour vs. Black &amp; White. We expect that Large Colour images will be responded to more quickly than Small B &amp; W images. We’re not sure about Small Colour images and Large B &amp; W images. We measured the response times of 24 participants responding to an image in each of these four conditions. We want to determine if there is a difference between our conditions (and if so, where that difference lies). 8.5.4 Question 4 Our fourth data file is called ANOVA_data4.csv and can be found here: https://raw.githubusercontent.com/george-farmer/PCHN63101/refs/heads/main/ANOVA_data4.csv These data are from a 2 x 2 x 3 mixed design experiment where we measured people’s response times to maths questions that were either Easy or Hard, and to which they had to respond either under Time Pressure or under No Time Pressure. These are our first two factors and are repeated measures (i.e., everyone saw all 4 conditions). Our third factor is between subjects and corresponds to whether our participants were in one of three groups. The groups were Psychology Students, Maths Students, and Arts Students. We want to know where a participant’s performance on the maths questions under time pressure vs. not under time pressure is influenced by which one of these three groups they belong to. Conduct the appropriate ANOVA to answer this question. Remember to start off with some data visualisation(s). 8.6 Answers 8.6.1 Question 1 The ANOVA should reveal an effect of Condition with F(1, 22) = 91.217. As there are just two levels to our factor, we don’t need to run any follow up tests to know what’s driving the effect. By looking at the descriptive statistics, we see that RT is 865 for our high condition and 1178 for our low condition. 8.6.2 Question 2 The ANOVA should reveal an effect of Condition with F(3, 44) = 203.21. To interpret this further we need to run follow up comparisons. Using the Bonferroni correction these should indicate that every level of Condition differs from every other level. 8.6.3 Question 3 The ANOVA should reveal a main effect of Size (F(1, 23) = 198.97), a main effect of Colour (F(1, 23) = 524.27), and an interaction between these two factors (F(1, 23) = 11.08). To interpret this interaction further we need to run follow up comparisons. Using the Bonferroni correction these should indicate that every level of Condition differs from every other level. 8.6.4 Question 4 This question is slightly trickier than the ones we’ve looked at so far. After you’ve built your ANOVA, you’ll discover a significant 3-way interaction (F(2, 69) = 4.63). You’ll need to break this down further - one possible approach would be to look at the interaction between Difficulty and Time Pressure separately for each level of our between group factor. In other words, you’d build one 2 x 2 ANOVA model for each of your Student Groups. If you do this, you’ll see that the 2 x 2 interaction is not significant for the Arts group, nor the Maths group, but the interaction is significant for the Psychology group (F(1, 23) = 11.08)) - as too are the two main effects of Difficulty and Time Pressure. But as these two factors interact, the meaning in our data is in the interaction term (not just these two main effects). So, the 3-way interaction was telling us that the 2-way interaction differed for at least one of our Groups (relative to the other Groups). We need to examine this 2-way interaction further for our Psychology group by conducting pairwise comparisons, exactly as you’ve done above. This will reveal that for our Psychology group, each condition differs significantly from the other conditions. It means that for Psychology students, the Hard problems take long under Time Pressure vs. under No Time Pressure, and the Easy problems take longer under Time Pressure vs. under No Time Pressure (but neither of these are as long as for Hard problems). End of workshop 8 materials "],["anova-part-2.html", "Workshop 9 ANOVA Part 2 9.1 Overview 9.2 Building our ANCOVA 9.3 AN(C)OVA as a Special Case of Regression", " Workshop 9 ANOVA Part 2 9.1 Overview In this workshop we will continue our examination of ANOVA. Specifically, we will focus on ANCOVA (Analysis of Covariance) before exploring how AN(C)OVA is a special case of regression and how both can be understood in the context of the General Linear Model.       9.2 Building our ANCOVA 9.2.1 Loading our Packages Let’s run through the ANCOVA example that I cover in the video above. First we need to load the packages we need. We’ll be using the tidyverse for general data wrangling and data visualisation, and then the afex package for building our ANCOVA model, and the emmeans package for running pairwise comparisons and displaying our adjusted means. library(tidyverse) # Load the tidyverse packages library(afex) # ANOVA functions library(emmeans) # Needed for pairwise comparisons 9.2.2 Reading in our Data Now we’re going to read in our data. my_data &lt;- read_csv(&quot;https://raw.githubusercontent.com/ajstewartlang/12_glm_anova_pt2/master/data/ancova_data.csv&quot;) head(my_data) ## # A tibble: 6 × 4 ## Participant Condition Ability Gaming ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Water 3.49 9.37 ## 2 2 Water 5.61 10.7 ## 3 3 Water 5.29 9.35 ## 4 4 Water 4.75 10.2 ## 5 5 Water 4.44 9.57 ## 6 6 Water 2.53 7.37 We see Condition isn’t properly coded as a factor, so let’s fix that. my_data &lt;- my_data %&gt;% mutate(Condition = factor(Condition)) head(my_data) ## # A tibble: 6 × 4 ## Participant Condition Ability Gaming ## &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Water 3.49 9.37 ## 2 2 Water 5.61 10.7 ## 3 3 Water 5.29 9.35 ## 4 4 Water 4.75 10.2 ## 5 5 Water 4.44 9.57 ## 6 6 Water 2.53 7.37 9.2.3 Summarising our Data Let’s work our some summary statistics and build a data visualisation next. my_data %&gt;% group_by(Condition) %&gt;% summarise(mean_ability = mean(Ability)) ## # A tibble: 3 × 2 ## Condition mean_ability ## &lt;fct&gt; &lt;dbl&gt; ## 1 Double Espresso 9.02 ## 2 Single Espresso 6.69 ## 3 Water 4.82 9.2.4 Visualising our Data ggplot(my_data, aes(x = Gaming, y = Ability, colour = Condition)) + geom_point(size = 3, alpha = .9) + labs(x = &quot;Gaming Frequency (hours per week)&quot;, y = &quot;Motor Ability&quot;) + theme_minimal() + theme(text = element_text(size = 11)) We have built a visualisation where we have plotted the raw data points using the geom_point() function. 9.2.5 Building our ANOVA model Let’s first build an ANOVA model, and ignore the presence of the covariate in our dataset. anova_model &lt;-aov_ez(id = &#39;Participant&#39;, dv = &#39;Ability&#39;, between = &#39;Condition&#39;, data = my_data) ## Contrasts set to contr.sum for the following variables: Condition anova(anova_model) ## Anova Table (Type 3 tests) ## ## Response: Ability ## num Df den Df MSE F ges Pr(&gt;F) ## Condition 2 42 1.2422 53.432 0.71786 2.882e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 On the basis of this output, it appears we have an effect of Condition. To explore this further, we would use the emmeans() function to run the pairwise comparisons. emmeans(anova_model, pairwise ~ Condition) ## $emmeans ## Condition emmean SE df lower.CL upper.CL ## Double Espresso 9.02 0.288 42 8.43 9.60 ## Single Espresso 6.69 0.288 42 6.11 7.27 ## Water 4.82 0.288 42 4.24 5.40 ## ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## Double Espresso - Single Espresso 2.33 0.407 42 5.720 &lt;.0001 ## Double Espresso - Water 4.20 0.407 42 10.317 &lt;.0001 ## Single Espresso - Water 1.87 0.407 42 4.597 0.0001 ## ## P value adjustment: tukey method for comparing a family of 3 estimates On the basis of this, we might conclude that we have an effect of Condition, and that each of our three groups differs significantly from the others. But would this be right? No, because we haven’t taken account of our covariate. Let’s build our ANCOVA model, adding our covariate. We set the factorize parameter to be FALSE so that it is treated as a continuous predictor, rather than an experimental factor in our model. model_ancova &lt;- aov_ez(id=&#39;Participant&#39;, dv = &#39;Ability&#39;, between = &#39;Condition&#39;, covariate = &#39;Gaming&#39;, data = my_data, factorize = FALSE) ## Warning: Numerical variables NOT centered on 0 (matters if variable in interaction): ## Gaming ## Contrasts set to contr.sum for the following variables: Condition anova(model_ancova) ## Anova Table (Type 3 tests) ## ## Response: Ability ## num Df den Df MSE F ges Pr(&gt;F) ## Condition 2 41 0.55171 0.8771 0.04103 0.4236 ## Gaming 1 41 0.55171 53.5636 0.56643 5.87e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 On this basis of this output, we see that we no longer have an effect of Condition, but we do have an effect of our covariate. We can use the emmeans() function to produce the adjusted means (i.e., the means for each of our three groups taking into account the influence of our covariate). emmeans(model_ancova, pairwise ~ Condition) ## $emmeans ## Condition emmean SE df lower.CL upper.CL ## Double Espresso 6.32 0.415 41 5.48 7.16 ## Single Espresso 6.87 0.193 41 6.48 7.26 ## Water 7.33 0.393 41 6.53 8.12 ## ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## Double Espresso - Single Espresso -0.552 0.478 41 -1.155 0.4863 ## Double Espresso - Water -1.008 0.761 41 -1.324 0.3900 ## Single Espresso - Water -0.456 0.418 41 -1.092 0.5244 ## ## P value adjustment: tukey method for comparing a family of 3 estimates 9.3 AN(C)OVA as a Special Case of Regression We are now going to look at ANOVA (and then ANCOVA) as a special case of regression. 9.3.1 Visualising our Data Let’s visualise the data with Condition on the x-axis. my_data %&gt;% ggplot(aes(x = Condition, y = Ability, colour = Condition)) + geom_violin() + geom_jitter(width = .05, alpha = .8) + labs(x = &quot;Condition&quot;, y = &quot;Motor Ability&quot;) + stat_summary(fun.data = mean_cl_boot, colour = &quot;black&quot;) + guides(colour = FALSE) + theme_minimal() + theme(text = element_text(size = 12)) ## Warning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use &quot;none&quot; instead as of ## ggplot2 3.3.4. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. Let’s check how our Condition factor is currently coded in terms of its contrasts. Note, the expression my_data$Condition is the Base R way of referring to the variable called Condition in the dataset my_data. 9.3.2 Setting up our Contrasts contrasts(my_data$Condition) ## Single Espresso Water ## Double Espresso 0 0 ## Single Espresso 1 0 ## Water 0 1 We want our Water group to be the reference level (thus corresponding to the intercept of our linear model), and dummy coded as (0, 0) but it’s not currently coded as such. Let’s fix that. my_data &lt;- my_data %&gt;% mutate(Condition = fct_relevel(Condition, c(&quot;Water&quot;, &quot;Double Espresso&quot;, &quot;Single Espresso&quot;))) contrasts(my_data$Condition) ## Double Espresso Single Espresso ## Water 0 0 ## Double Espresso 1 0 ## Single Espresso 0 1 9.3.3 ANOVA as a Linear Model OK, this is now what we want. Let’s model our linear model using the lm() function and examine the result. model_lm &lt;- lm(Ability ~ Condition, data = my_data) model_lm ## ## Call: ## lm(formula = Ability ~ Condition, data = my_data) ## ## Coefficients: ## (Intercept) ConditionDouble Espresso ConditionSingle Espresso ## 4.817 4.199 1.871 We can see that the Intercept corresponds to the mean of our Water Condition. To work out the mean Ability of our Double Espresso Group, we use the coding for the Double Espresso group (1, 0) with our equation: Ability = Intercept + β1(Double Espresso) + β2(Single Espresso) Ability = 4.817 + 4.199(1) + 1.871(0) Ability = 4.817 + 4.199 Ability = 9.016 To work out the mean Ability of our Single Espresso Group, we use the coding for the Single Espresso group (0, 1) with our equation: Ability = 4.817 + 4.199(0) + 1.871(1) Ability = 4.817 + 1.871 Ability = 6.688 9.3.4 ANCOVA as a Linear Model OK, now to build our ANCOVA using the lm() function, we simply add the covariate (Gaming) to our model specification. model_ancova &lt;- lm(Ability ~ Gaming + Condition, data = my_data) model_ancova ## ## Call: ## lm(formula = Ability ~ Gaming + Condition, data = my_data) ## ## Coefficients: ## (Intercept) Gaming ConditionDouble Espresso ## -3.4498 0.8538 -1.0085 ## ConditionSingle Espresso ## -0.4563 We can work out the mean of our reference group (Water) by plugging in the values to our equation - note that Gaming is not a factor and we need to enter the mean of this variable. We can work it out with the following. mean(my_data$Gaming) ## [1] 12.62296 We add this mean (12.62296) to our equation alongside the co-efficients for each of our predictors. With our dummy coding scheme, we can work out the adjusted mean of our Water group. Ability = Intercept + β1(Gaming) + β2(Double Espresso) + β3(Single Espresso) Ability = -3.4498 + 0.8538(12.62296) + (- 1.0085)(0) + (-0.4563)(0) Ability = -3.4498 + 10.777 Ability = 7.33 7.33 is the adjusted mean for the Water group, which is what we had from calling the emmeans() function following the ANCOVA. Have a go yourselves at working out the adjusted means of the other two Conditions using our dummy coding. 9.3.5 Centering our Covariate In the video, I mention that we could also have scaled and centred our covariate. This standardises the variable (with the mean centred on zero) and removes the need to multiply the linear model coefficient for the covariate by the covariate’s mean. Generally, it makes interpretation of the coefficients in our linear model easier. We can use the scale() function to create a new (scaled and centred) version of our covariate in our data frame. my_scaled_data &lt;- my_data %&gt;% mutate(centred_gaming = scale(Gaming)) We can look at both the uncentred and the centred covariate to see that nothing has changed in the data, other than the variable mean is now centred on zero and the distribution has been scaled. plot(density(my_scaled_data$Gaming)) plot(density(my_scaled_data$centred_gaming)) So let’s build our linear model with the scaled and centred covariate. model_ancova_centred &lt;- lm(Ability ~ centred_gaming + Condition, data = my_scaled_data) model_ancova_centred ## ## Call: ## lm(formula = Ability ~ centred_gaming + Condition, data = my_scaled_data) ## ## Coefficients: ## (Intercept) centred_gaming ConditionDouble Espresso ## 7.3280 2.3046 -1.0085 ## ConditionSingle Espresso ## -0.4563 We see that the Intercept now corresponds to the adjusted mean for the Water group. We can calculate the adjusted mean for the Double Espresso group by subtracting 1.0085 from 7.3280, and we can calculate the adjusted mean for the Single Espresso group by subtracting 0.4563 from 7.3280. Hopefully you see that scaling and centering the covariate makes it’s a lot easier to then interpret the coefficients of our linear model.    End of workshop 9 materials End of unit materials! "],["technical-details-and-credits.html", "Technical Details and Credits", " Technical Details and Credits This course was originally developed and made open access by Andrew Stewart. Updates in 2023 and 2024 by George Farmer. The workshops and this website were all written using R Markdown and Bookdown. The website is hosted on GitHub. The lecture content is licensed under CC-BY. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
