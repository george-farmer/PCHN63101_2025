[["index.html", "Advanced Data Skills, Open Science and Reproducibility 2025/26 Welcome", " Advanced Data Skills, Open Science and Reproducibility 2025/26 Welcome Unit Lead: George Farmer Welcome to the Advanced Data Skills, Open Science and Reproducibility M.Res. unit PCHN63101. This unit will be taught via a flipped classroom model. Make sure you go through the content on this site in advance of the in-person sessions. These are held on Mondays from 1 to 3pm in the Coupland 3 Building in room room LG.118. For questions about the course you can contact me via email (george.farmer@manchester.ac.uk). If you have any questions or technical problems relating to each week’s content, please post these on the Canvas discussion board. There are two assignments associated with this unit. The full details of these (plus hand in dates) can be found on the Canvas page for this unit. "],["open-science.html", "Workshop 1 Open Science 1.1 Open Research and Reproducibility 1.2 Experimental Power 1.3 Open Source Software", " Workshop 1 Open Science In this workshop I will first introduce you to the key concepts in open research, and talk about the so-called “replication crisis” in the Psychological, Biomedical, and Life Sciences that has resulted in the Open Research movement. I will also discuss the importance of adopting reproducible research practices in your own research, and provide an introduction to various tools and processes you can incorporate into your own research workflows that will allow you to conduct reproducible research. Like the other workshops in this series, this one involves a mix of recorded videos, narrative, and links to various resources for you to explore and to read. 1.1 Open Research and Reproducibility First I’d like you to watch the following video on the history of the replication crisis and the issues which have motivated a move towards the adoption of open and reproducible research practices. The video covers the so-called replication crisis in the biomedical sciences, issues around open research, and summarise some of the initiatives (including the UK Reproducibility Network) that have been established to address the fundamental problems around open research, transparency, and reproducibility in science.       Before watching the next video, please have a read through the following paper by Ioannidis (2005) which arguably started the conversation around reproducibility that has had such an impact on research in Psychology and across the Biomedical sciences for the last few years. Clicking on the image below will take you to the paper. Ioannidis (2005)       Bishop (2019) This post by Dorothy Bishop in 2019 nicely captures the situation a number of years later. Clicking on the image below will take you to the paper.    1.1.1 How to do Reproducible Research One of the biggest challenges facing researchers who are used to the old way of conducting research is that they feel that they don’t have the knowledge or technical skills to adopt open and reproducible research practices. But it’s not that hard! Before you run your experiment, you can pre-register your hypotheses so that when you come to analyse and write-up your results, you can demonstrate that your predictions really were made in advance of data collection. You can also make your research data open (and FAIR) alongside your code so that others can recreate your analyses. And by posting your research article on a pre-print server (such as PsyArXiv or bioRxiv) before submission to a journal, you make your research findings available to all. The adoption of open source software such as R, also means that any research findings you produce can be re-produced by others who can access your data and code. This principle of using open tools to allow us to produce open (and reusable) data and code is the fundamental philosophy behind all of the workshops in this unit. Take the time to read the following paper, it’s a great guide to the various things you can do to make your own research more open. Just click on the image to open the paper.    Haeffel (2022) This thought provoking paper by Gerald Haeffel suggests that psychology needs to focus more on theory development (and encourage the publication of results that refute theories). 1.2 Experimental Power We are now going to look at the issues around experimental power (and why it is important). One of the insights revealed by the “replication crisis” is that very often research is underpowered for the effect size of interest (i.e., even if the effect is there, your experiment is unlikely to find it). Even when underpowered studies do reveal the effect of interest, the effect size itself will be over-estimated (thus causing problems for future work that might base their power estimates on this incorrect effect size estimate). One solution to the challenge is to conduct data simulation as part of the experimental design process. There are many ways to do this using R, and there are several packages on CRAN (the Comprehensive R Archive Network) that provide functions to simulate data for different kinds of designs.       If you’re interested in reading more about power, you might like to take a look at this classic “Power Primer” paper by Jacob Cohen.    1.3 Open Source Software If you want to produce open and reproducible research, you should be using open source software in your workflow. Research produced using proprietary software cannot be easily reproduced by others. Open source software is software that is licensed to be free to modify, remix, and improve. It is usually free to use, and is centred on the principles of open exchange, collaborative participation, rapid prototyping, transparency, meritocracy, and community-oriented development. The move towards open source began in the early 1980s partly because of a printer and developed further that decade in the form of the Free Software Foundation established by Richard Stallman. In the late 1990s, the Open Source Initiative was launched to raise awareness and adoption of open source software, and build bridges between open source communities of practice. Open source software is made by many people and distributed under an OSD-compliant license which grants all the rights to use, study, change, and share the software in modified and unmodified form. Software freedom is essential to enabling community development of open source software. There is a huge amount of open source software available - some of which you will find useful both in the context of this unit, but also in the context of how you study, and in how you conduct your research. Below is an interesting CNBC video discussing the rise of Open Source Software - it ends with mention of the need to collaborate in an open manner on global challenges such as the environment, cancer, and Alzheimer’s disease.       1.3.1 Statistical and Scientific Computing R and RStudio Desktop It goes without saying that R and RStudio Desktop are the two most obvious examples of open source software that are relevant to this course. In terms of other open source languages used for data analysis and statistical modelling, you might also be interested in Python and Julia.              Python While R tends to be the go-to language for people who are interested in data wrangling, data visualisation, and statistical modelling, Python is arguably the ‘better’ language in the sense it is more general purpose. Python is used widely by the machine learning community (to name just one example). Octave You may have heard of - or even used - MATLAB for numerical computing. There is an open source equivalent, called GNU Octave, that you may be interested in checking out. 1.3.2 Document Creation Up until this time you’ve probably mainly used Microsoft Word for writing documents. LibreOffice is a great open source equivalent to the Microsoft Office suite and offers a huge range of applications for document writing, working with spreadsheets, and the creation of presentations. If you’re interested in writing using Markdown (which is really easy to get to grips with), you might be interested in using HackMD. HackMD is a Markdown editor that critically allows you to write collaborative documents and presentations with others. 1.3.3 Building Experiments PsychoPy offers a great open source solution to build experiments to collect human data, and via the companion hosting site Pavlovia provides an easy to use method for running the PsychoPy experiments online. PsychoPy has been around for a number of years and has lots of pre-built experiment templates that you can adapt as needs be. There is a great reference that describes the PsychoPy environment here.              1.3.4 Linux One of the most widely used pieces of open source software is the Linux operating system. Rather than running your computer on Windows, or Mac OS, you could choose to run it using Linux. Linux runs the majority of the internet servers in world is becoming increasingly popular in academic settings. Linux refers to a bunch of open source Unix-like operating systems. It was developed and released by Linus Torvalds in 1991. Some of the most popular distributions of Linux are Ubuntu, Fedora, and Debian. If you really want to get into the computational side of research, it’s important to discover the world of Linux. 1.3.5 More Reading If you’re interested in other examples of open source software, you might be interested in having a look at this list of open source alternatives on the Tech Radar site. End of workshop 1 materials "],["r-and-rstudio.html", "Workshop 2 R and RStudio 2.1 Why R? 2.2 Getting Started 2.3 Keeping Things Organised 2.4 Good Coding Style 2.5 Your First R Script", " Workshop 2 R and RStudio 2.1 Why R? In this workshop we will cover the language, R, and RStudio Desktop, the integrated development environment (IDE) that you will use to write reproducible code involving the wrangling, visualization, summary, and statistical modelling of your data. Both R and RStudio Desktop are examples of Open Source Software. The video below introduces you to R and talks about the importance of adopting such tools in our research analysis workflows.       Below is a video of a great talk by J.J. Allaire, entrepreneur and founder of RStudio (and other organizations). This video is from rstudio::conf 2020 and in it J.J. talks about his journey from being a Political Scientist, how he got involved in R, and the importance of Open Source in the context of Reproducible Data Science. If you click on the image, you’ll be taken to the RStudio website where you can watch the recording. If you’re interested, you might like to look at some of the other videos on the RStudio site.       2.2 Getting Started In this next video you’ll see how to install R (the language) and RStudio Desktop (the IDE for working with the language). You can download R from here for a variety of platforms including Mac OS, Windows, and Ubuntu. To download free RStudio Desktop just go to here. If you are using a Chromebook, or have a tablet, or are having difficulties installing R and RStudio Desktop on your computer, you can use Posit cloud to run the RStudio environment in your browser. You’ll need to sign up - there is a free plan available.       2.3 Keeping Things Organised When you are doing data analysis using R, it’s important to use a sensible structure for your folders and files. As you saw in the video above, creating a new project with a .Rproj file is the easiest way to this is using RStudio Desktop. Good file management is as important as good coding style (which we’ll come to next). There’s a great paper on project management that you can read by clicking the image below.          2.4 Good Coding Style In the following video you will learn a little about good coding style. It’s important when you’re writing analysis scripts that your code is understandable by others, and by future you. If you get into the habit of good coding style early on, it will make things a lot easier in the long run - and you’ll find it easier to work collaboratively as others will find it easier to work with you.       You can have a look at the helpful Tidyverse Style Guide here. If you want to make your code and data open (and you really should unless there’s a good reason not to do so), it’s important to license it properly to allow others to (re)use and remix it. It’s often good to use the most permissive license that you can. Some good licenses are the MIT License, and the Creative Commons License CC-BY 4.0. You can use this handy guide if you need help choosing the right license for your own work.    2.5 Your First R Script You’re now going to run your first R script. We will create three visualisations of UFO sightings in the US using a database of more than 80,000 UFO sightings over the years. Before you run the code, you will need to install two packages onto your computer - they are tidyverse and patchwork.       Once you have installed the packages, paste the following code into a new R script. Run the code in the same way shown in the video. Does your visualisation look like the one in the video? library(tidyverse) # load the tidyverse library(patchwork) # needed to combine our 4 plots at the end # read in data ufo_sightings &lt;- read_csv(&quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-06-25/ufo_sightings.csv&quot;) # plot of top 10 US states with number of sightings in each state plot1 &lt;- ufo_sightings %&gt;% filter(!is.na(state)) %&gt;% mutate(state = str_to_upper(state)) %&gt;% group_by(state) %&gt;% tally() %&gt;% top_n(10) %&gt;% ggplot(aes(x = reorder(state, n), y = n, fill = state)) + geom_col() + coord_flip() + guides(fill = &quot;none&quot;) + labs(title = &quot;Top 10 States for UFO Sightings&quot;, x = NULL, y = NULL) + ylim(0, 11000) + theme_minimal() + theme(text = element_text(size = 15)) # work out states within lat and long limits (i.e., exclude Alaska) tidied_ufo &lt;- ufo_sightings %&gt;% filter(country == &quot;us&quot;) %&gt;% filter(latitude &gt; 24 &amp; latitude &lt; 50) # plot all sightings on a map of the US plot2 &lt;- tidied_ufo %&gt;% ggplot(aes(x = longitude, y = latitude)) + geom_point(size = .5, alpha = .25) + theme_void() + coord_cartesian() + labs(title = &quot;Sites of UFO Sightings in the US&quot;) + theme(text = element_text(size = 15)) # plot of top 10 UFO shapes spotted in California plot3 &lt;- tidied_ufo %&gt;% filter(state == &quot;ca&quot;) %&gt;% filter(ufo_shape != &quot;other&quot;) %&gt;% filter(ufo_shape != &quot;unknown&quot;) %&gt;% group_by(ufo_shape) %&gt;% tally() %&gt;% top_n(10) %&gt;% mutate(ufo_shape = str_to_title(ufo_shape)) %&gt;% ggplot(aes(x = reorder(ufo_shape, n), y = n, fill = ufo_shape)) + geom_col() + coord_flip() + guides(fill = &quot;none&quot;) + labs(title = &quot;Top 10 UFO Shapes spotted in California&quot;, x = NULL, y = NULL) + theme_minimal() + theme(text = element_text(size = 15)) # Put plots together my_plot &lt;- (plot1 + plot3) / (plot2) ggsave(&quot;ufo_plot.jpg&quot;, plot = my_plot, width = 12, height = 10) End of workshop 2 materials "],["data-wrangling-and-data-summarising.html", "Workshop 3 Data Wrangling and Data Summarising 3.1 Wrangling your data 3.2 Summarising your data", " Workshop 3 Data Wrangling and Data Summarising In this workshop we shall take our first look at some key tools in the Tidyverse that will allow us to wrangle and tidy our data so that it’s in the format that we need in order to visualize and model it. By making our data wrangling reproducible (i.e., by coding it in R), we can easily re-run this stage of our analysis pipeline as new data gets added. Reproducibility of the data wrangling stage is a key part of the analysis process and often gets overlooked in terms of needing to ensure it is reproducible. The Tidyverse is a collection of packages that all ‘play nicely’ with each other. They are based on a common philosophy where data are represented in rectangular format (i.e., with rows and columns). These rectangular structures are known in the Tidyverse as tibbles. If you’re interested, you can read more about tibbles in the R4DS book here. 3.1 Wrangling your data Have a look at the following video where I walk you through this worksheet. Then I want you to work through the content by writing (and running) the script on your own machine.       3.1.1 Loading the Tidyverse Let’s take our first look at data wrangling. We are going to start with a dataset that comes with the Tidyverse. The dataset is called mpg and comprises fuel economy data from 1999 to 2008 for 38 popular models of cars in the US. First, we need to load the tidyverse library with the following: library(tidyverse) If you run this line without having first installed the Tidyverse on your computer, you will encounter an error. R packages only need to be installed once, so if you want to load one into your library for the first time, you need to install it with install.packages(*packagename*). For the tidyverse we need to install it with: install.packages(&quot;tidyverse&quot;) Once you have installed the tidyverse, you can then load it into your llbrary with the library() function. You only ever need to install a package once on your machine (unless you have updated R or you want to install the most up-to-date version of a particular package). When you are writing your R scripts, you never want to have the install.packages() function in the body of the script as if someone else were to run your script, this would update packages on their computer (which they might not want). The mpg dataset The mpg dataset is loaded as part of the Tidyverse. In the help file, which you can access by typing help(mpg) or ?mpg we see the following: Description This dataset contains a subset of the fuel economy data that the EPA makes available on http://fueleconomy.gov. It contains only models which had a new release every year between 1999 and 2008 - this was used as a proxy for the popularity of the car. A data frame with 234 rows and 11 variables. manufacturer - manufacturer model - model name displ - engine displacement, in litres year - year of manufacture cyl - number of cylinders trans -type of transmission drv -f = front-wheel drive, r = rear wheel drive, 4 = 4wd cty - city miles per gallon hwy - highway miles per gallon fl - fuel type class - “type” of car Note you can also use help to inspect functions, for example: Typing ?sd will show you the documentation for R’s Standard Deviation function. 3.1.2 Using head() and str() We can explore the mpg dataset that is loaded with the Tidyverse in a number of ways. If we want to look at the first 6 lines of the dataset, we can use the head() function. head(mpg) ## # A tibble: 6 × 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 audi a4 1.8 1999 4 auto(l5) f 18 29 p compact ## 2 audi a4 1.8 1999 4 manual(m5) f 21 29 p compact ## 3 audi a4 2 2008 4 manual(m6) f 20 31 p compact ## 4 audi a4 2 2008 4 auto(av) f 21 30 p compact ## 5 audi a4 2.8 1999 6 auto(l5) f 16 26 p compact ## 6 audi a4 2.8 1999 6 manual(m5) f 18 26 p compact We see that it is a tibble - or a rectangular data frame - made up of rows and columns. This is tidy format where each observation corresponds to a row. Most of the analyses we will run in R involve tidy data. Within the Tidyverse, the tibble is the standard way to represent data. You’ll spend a lot of time tidying and wrangling your data to get it into this format! By doing this in R using a script that you write, you are making this key stage reproducible. You can run the script again on an updated or different dataset - thus likely saving you lots of time! We can also ask for information about the structure of our dataset with str(). This will tell us about the columns, what type of variable each is, the number of rows etc. str(mpg) ## tibble [234 × 11] (S3: tbl_df/tbl/data.frame) ## $ manufacturer: chr [1:234] &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; ... ## $ model : chr [1:234] &quot;a4&quot; &quot;a4&quot; &quot;a4&quot; &quot;a4&quot; ... ## $ displ : num [1:234] 1.8 1.8 2 2 2.8 2.8 3.1 1.8 1.8 2 ... ## $ year : int [1:234] 1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 ... ## $ cyl : int [1:234] 4 4 4 4 6 6 6 4 4 4 ... ## $ trans : chr [1:234] &quot;auto(l5)&quot; &quot;manual(m5)&quot; &quot;manual(m6)&quot; &quot;auto(av)&quot; ... ## $ drv : chr [1:234] &quot;f&quot; &quot;f&quot; &quot;f&quot; &quot;f&quot; ... ## $ cty : int [1:234] 18 21 20 21 16 18 18 18 16 20 ... ## $ hwy : int [1:234] 29 29 31 30 26 26 27 26 25 28 ... ## $ fl : chr [1:234] &quot;p&quot; &quot;p&quot; &quot;p&quot; &quot;p&quot; ... ## $ class : chr [1:234] &quot;compact&quot; &quot;compact&quot; &quot;compact&quot; &quot;compact&quot; ... 3.1.3 Use select() to select columns If we want to, we could just select one of the columns using the select() function. Below we are just selecing the column entitled manufacturer. mpg %&gt;% select(manufacturer) ## # A tibble: 234 × 1 ## manufacturer ## &lt;chr&gt; ## 1 audi ## 2 audi ## 3 audi ## 4 audi ## 5 audi ## 6 audi ## 7 audi ## 8 audi ## 9 audi ## 10 audi ## # ℹ 224 more rows Related to the select() function is rename(). It does exactly what you think it might; it renames a column. We can also look at the different car manufacturers in the dataset by using the distinct() function. This gives us the unique manufacturer names. This function can be quite handy if you want to check a dataset for duplicates of (e.g.) participant IDs. mpg %&gt;% distinct(manufacturer) ## # A tibble: 15 × 1 ## manufacturer ## &lt;chr&gt; ## 1 audi ## 2 chevrolet ## 3 dodge ## 4 ford ## 5 honda ## 6 hyundai ## 7 jeep ## 8 land rover ## 9 lincoln ## 10 mercury ## 11 nissan ## 12 pontiac ## 13 subaru ## 14 toyota ## 15 volkswagen 3.1.4 Use filter() to select rows Sometimes we might want to select only a subset of rows in our dataset. We can do that using the filter() function. For example, here we filter our dataset to include only cars made by ‘honda’. mpg %&gt;% filter(manufacturer == &quot;honda&quot;) ## # A tibble: 9 × 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 honda civic 1.6 1999 4 manual(m5) f 28 33 r subcompact ## 2 honda civic 1.6 1999 4 auto(l4) f 24 32 r subcompact ## 3 honda civic 1.6 1999 4 manual(m5) f 25 32 r subcompact ## 4 honda civic 1.6 1999 4 manual(m5) f 23 29 p subcompact ## 5 honda civic 1.6 1999 4 auto(l4) f 24 32 r subcompact ## 6 honda civic 1.8 2008 4 manual(m5) f 26 34 r subcompact ## 7 honda civic 1.8 2008 4 auto(l5) f 25 36 r subcompact ## 8 honda civic 1.8 2008 4 auto(l5) f 24 36 c subcompact ## 9 honda civic 2 2008 4 manual(m6) f 21 29 p subcompact Note, we use the operator == which means ‘is equal to’. This is a logical operator - other logical operators include less than &lt;, greater than &gt;, less than or equal to &lt;=, greater then or equal to &gt;=, and is not equal to !=. We can also filter using a combination of possibilities via logical OR | or logical AND &amp;. The first code chunk below filters the dataset for cases where the manufacturer is ‘honda’ OR ‘toyota’. mpg %&gt;% filter(manufacturer == &quot;honda&quot; | manufacturer == &quot;toyota&quot;) ## # A tibble: 43 × 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 honda civic 1.6 1999 4 manual(m5) f 28 33 r subcompact ## 2 honda civic 1.6 1999 4 auto(l4) f 24 32 r subcompact ## 3 honda civic 1.6 1999 4 manual(m5) f 25 32 r subcompact ## 4 honda civic 1.6 1999 4 manual(m5) f 23 29 p subcompact ## 5 honda civic 1.6 1999 4 auto(l4) f 24 32 r subcompact ## 6 honda civic 1.8 2008 4 manual(m5) f 26 34 r subcompact ## 7 honda civic 1.8 2008 4 auto(l5) f 25 36 r subcompact ## 8 honda civic 1.8 2008 4 auto(l5) f 24 36 c subcompact ## 9 honda civic 2 2008 4 manual(m6) f 21 29 p subcompact ## 10 toyota 4runner 4wd 2.7 1999 4 manual(m5) 4 15 20 r suv ## # ℹ 33 more rows While below we filter for cases where the manufacturer is ‘honda’ and the year of manufacture is ‘1999’. mpg %&gt;% filter(manufacturer == &quot;honda&quot; &amp; year == &quot;1999&quot;) ## # A tibble: 5 × 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 honda civic 1.6 1999 4 manual(m5) f 28 33 r subcompact ## 2 honda civic 1.6 1999 4 auto(l4) f 24 32 r subcompact ## 3 honda civic 1.6 1999 4 manual(m5) f 25 32 r subcompact ## 4 honda civic 1.6 1999 4 manual(m5) f 23 29 p subcompact ## 5 honda civic 1.6 1999 4 auto(l4) f 24 32 r subcompact Combining functions We can combine the use of filter() with select() to filter for case where the manufacturer is ‘honda’, the year of manufacture is ‘1999’ and we only want to display these two columns plus those telling us about fuel economy - cty and hwy. mpg %&gt;% filter(manufacturer == &quot;honda&quot; &amp; year == &quot;1999&quot;) %&gt;% select(manufacturer, year, cty, hwy) ## # A tibble: 5 × 4 ## manufacturer year cty hwy ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 honda 1999 28 33 ## 2 honda 1999 24 32 ## 3 honda 1999 25 32 ## 4 honda 1999 23 29 ## 5 honda 1999 24 32 By combining just a few functions, you can imagine that we can build some quite complex data wrangling rules quite straightforwardly. 3.1.5 The pipe %&gt;% Note that in these examples above we used the %&gt;% operator - this is called the pipe and allows us to pass information from one side of the pipe to the other. You can read it out load as ‘and then’. All of the functions (such as select(), filter() etc.) in the Tidyverse are known as verbs, and they describe what they do. The pipe is one of the most commonly used operators in the Tidyverse and allows us to chain together different lines of code - with the output of each line being passed on as input into the next. In this example, the dataset mpg is passed along to the distinct() function where we ask for a list of the distinct (i.e., unique) manufacturers. This output itself is a vector. Vectors are a basic data structure and contain elements of the same type - for example, a bunch of numbers. We can add another line to our piped chain to tell us how many elements are in this vector. We could read this out loud as ‘take the dataset mpg, and then work out the distinct manufacturer names, and then count them’. mpg %&gt;% distinct(manufacturer) %&gt;% count() ## # A tibble: 1 × 1 ## n ## &lt;int&gt; ## 1 15 3.1.6 Tidying up a dataset Tidying variable names At the moment, the car manufacturer names are all in lower case. It would look a lot nicer if they were in title case (i.e., with capitalisation on the first letter of each word). We can use the mutate() function to create a new column - this time, the name of the new column is also the name of the old column that we’re wanting to modify using the function str_to_title(). What this will do is overwrite the column manufacturer and replace it with the new version with the car manufacturer names in title case. mpg %&gt;% mutate(manufacturer = str_to_title(manufacturer)) ## # A tibble: 234 × 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Audi a4 1.8 1999 4 auto(l5) f 18 29 p compact ## 2 Audi a4 1.8 1999 4 manual(m5) f 21 29 p compact ## 3 Audi a4 2 2008 4 manual(m6) f 20 31 p compact ## 4 Audi a4 2 2008 4 auto(av) f 21 30 p compact ## 5 Audi a4 2.8 1999 6 auto(l5) f 16 26 p compact ## 6 Audi a4 2.8 1999 6 manual(m5) f 18 26 p compact ## 7 Audi a4 3.1 2008 6 auto(av) f 18 27 p compact ## 8 Audi a4 quattro 1.8 1999 4 manual(m5) 4 18 26 p compact ## 9 Audi a4 quattro 1.8 1999 4 auto(l5) 4 16 25 p compact ## 10 Audi a4 quattro 2 2008 4 manual(m6) 4 20 28 p compact ## # ℹ 224 more rows The column model is also lowercase. Let’s make that title case too. We can use the mutate() function to work over more than one column at the same time like this: mpg %&gt;% mutate(manufacturer = str_to_title(manufacturer), model = str_to_title(model)) ## # A tibble: 234 × 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Audi A4 1.8 1999 4 auto(l5) f 18 29 p compact ## 2 Audi A4 1.8 1999 4 manual(m5) f 21 29 p compact ## 3 Audi A4 2 2008 4 manual(m6) f 20 31 p compact ## 4 Audi A4 2 2008 4 auto(av) f 21 30 p compact ## 5 Audi A4 2.8 1999 6 auto(l5) f 16 26 p compact ## 6 Audi A4 2.8 1999 6 manual(m5) f 18 26 p compact ## 7 Audi A4 3.1 2008 6 auto(av) f 18 27 p compact ## 8 Audi A4 Quattro 1.8 1999 4 manual(m5) 4 18 26 p compact ## 9 Audi A4 Quattro 1.8 1999 4 auto(l5) 4 16 25 p compact ## 10 Audi A4 Quattro 2 2008 4 manual(m6) 4 20 28 p compact ## # ℹ 224 more rows There are quite a few columns there, so how about we select just the manufacturer, model, year, transmission, and hwy columns: mpg %&gt;% mutate(manufacturer = str_to_title(manufacturer), model = str_to_title(model)) %&gt;% select(manufacturer, model, year, trans, hwy) ## # A tibble: 234 × 5 ## manufacturer model year trans hwy ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 Audi A4 1999 auto(l5) 29 ## 2 Audi A4 1999 manual(m5) 29 ## 3 Audi A4 2008 manual(m6) 31 ## 4 Audi A4 2008 auto(av) 30 ## 5 Audi A4 1999 auto(l5) 26 ## 6 Audi A4 1999 manual(m5) 26 ## 7 Audi A4 2008 auto(av) 27 ## 8 Audi A4 Quattro 1999 manual(m5) 26 ## 9 Audi A4 Quattro 1999 auto(l5) 25 ## 10 Audi A4 Quattro 2008 manual(m6) 28 ## # ℹ 224 more rows Recoding variables In the real world, data frames do not always arrive on our computer in tidy format. Very often you need to engage in some data tidying before you can do anything useful with them. We’re going to look at an example of how we go from messy data to tidy data. my_messy_data &lt;- read_csv(&quot;https://raw.githubusercontent.com/ajstewartlang/03_data_wrangling/master/data/my_data.csv&quot;) We ran a reaction time experiment with 24 participants and 4 conditions - they are numbered 1-4 in our data file. head(my_messy_data) ## # A tibble: 6 × 3 ## participant condition rt ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 879 ## 2 1 2 1027 ## 3 1 3 1108 ## 4 1 4 765 ## 5 2 1 1042 ## 6 2 2 1050 This is a repeated measures design where we had one factor (Prime Type) with two levels (A vs. B) and a second factor (Target Type) with two levels (A vs. B). We want to recode our data frame so it better matches our experimental design. First we need to recode our 4 conditions like this: Recode condition columns follows: Condition 1 = Prime A, Target A Condition 2 = Prime A, Target B Condition 3 = Prime B, Target A Condition 4 = Prime B, Target B my_messy_data %&gt;% mutate(condition = recode(condition, &quot;1&quot; = &quot;PrimeA_TargetA&quot;, &quot;2&quot; = &quot;PrimeA_TargetB&quot;, &quot;3&quot; = &quot;PrimeB_TargetA&quot;, &quot;4&quot; = &quot;PrimeB_TargetB&quot;)) %&gt;% head() ## # A tibble: 6 × 3 ## participant condition rt ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 PrimeA_TargetA 879 ## 2 1 PrimeA_TargetB 1027 ## 3 1 PrimeB_TargetA 1108 ## 4 1 PrimeB_TargetB 765 ## 5 2 PrimeA_TargetA 1042 ## 6 2 PrimeA_TargetB 1050 We now need to separate out our Condition column into two - one for our first factor (Prime), and one for our second factor (Target). The separate() function does just this - when used in conjunction with a piped tibble, it needs to know which column we want to separate, what new columns to create by separating that original column, and on what basis we want to do the separation. In the example below we tell separate() that we want to separate the column labeled condition into two new columns called Prime and Target and we want to do this at any points where a _ is present in the column to be separated. my_messy_data %&gt;% mutate(condition = recode(condition, &quot;1&quot; = &quot;PrimeA_TargetA&quot;, &quot;2&quot; = &quot;PrimeA_TargetB&quot;, &quot;3&quot; = &quot;PrimeB_TargetA&quot;, &quot;4&quot; = &quot;PrimeB_TargetB&quot;)) %&gt;% separate(col = &quot;condition&quot;, into = c(&quot;Prime&quot;, &quot;Target&quot;), sep = &quot;_&quot;) ## # A tibble: 96 × 4 ## participant Prime Target rt ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 PrimeA TargetA 879 ## 2 1 PrimeA TargetB 1027 ## 3 1 PrimeB TargetA 1108 ## 4 1 PrimeB TargetB 765 ## 5 2 PrimeA TargetA 1042 ## 6 2 PrimeA TargetB 1050 ## 7 2 PrimeB TargetA 942 ## 8 2 PrimeB TargetB 945 ## 9 3 PrimeA TargetA 943 ## 10 3 PrimeA TargetB 910 ## # ℹ 86 more rows my_messy_data %&gt;% mutate(condition = recode(condition, &quot;1&quot; = &quot;PrimeA_TargetA&quot;, &quot;2&quot; = &quot;PrimeA_TargetB&quot;, &quot;3&quot; = &quot;PrimeB_TargetA&quot;, &quot;4&quot; = &quot;PrimeB_TargetB&quot;)) %&gt;% separate(col = &quot;condition&quot;, into = c(&quot;Prime&quot;, &quot;Target&quot;), sep = &quot;_&quot;) %&gt;% mutate(Prime = factor(Prime), Target = factor(Target)) ## # A tibble: 96 × 4 ## participant Prime Target rt ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 1 PrimeA TargetA 879 ## 2 1 PrimeA TargetB 1027 ## 3 1 PrimeB TargetA 1108 ## 4 1 PrimeB TargetB 765 ## 5 2 PrimeA TargetA 1042 ## 6 2 PrimeA TargetB 1050 ## 7 2 PrimeB TargetA 942 ## 8 2 PrimeB TargetB 945 ## 9 3 PrimeA TargetA 943 ## 10 3 PrimeA TargetB 910 ## # ℹ 86 more rows The pivot functions Most of the analysis we will conduct in R requires our data to be in tidy, or long, format. In such data sets, one row corresponds to one observation. In the real world, data are rarely in the right format for analysis. In R, the pivot_wider() and pivot_longer() functions are designed to reshape our data files. First, let’s load a datafile that is in wide format (i.e., multiple observations per row). It is from an experiment where we had four conditions (labelled Condition1, Condition2, Condition3, and Condition4). In addition to there being a column for each of the 4 conditions, we also have a column corresponding to participant ID. Each cell in the data set corresponds to a reaction time (measured in milliseconds). my_wide_data &lt;- read_csv(&quot;https://raw.githubusercontent.com/ajstewartlang/03_data_wrangling/master/data/my_wide_data.csv&quot;) The pivot_longer() function head(my_wide_data) ## # A tibble: 6 × 5 ## ID Condition1 Condition2 Condition3 Condition4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 487 492 499 488 ## 2 2 502 494 517 508 ## 3 3 510 483 488 509 ## 4 4 476 488 513 521 ## 5 5 504 478 513 504 ## 6 6 505 486 503 495 So, we can see the data file is in wide format. We want to reshape it to long format. We can do that using the pivot_longer() function. Minimally, we need to specify the data frame that we want to reshape, the columns that we want to ‘pivot’ into longer format, the name of the new column that we are creating, and the name of the column that will hold the values of our reshaped data frame. We are going to map the output to a variable I’m calling my_longer_data. my_longer_data &lt;- my_wide_data %&gt;% pivot_longer(cols = c(Condition1, Condition2, Condition3, Condition4), names_to = &quot;Condition&quot;, values_to = &quot;RT&quot;) Now let’s have a look at what our reshaped data frame looks like. head(my_longer_data) ## # A tibble: 6 × 3 ## ID Condition RT ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Condition1 487 ## 2 1 Condition2 492 ## 3 1 Condition3 499 ## 4 1 Condition4 488 ## 5 2 Condition1 502 ## 6 2 Condition2 494 So you can see our data are now in long - or tidy - format with one observation per row. Note that our Condition column isn’t coded as a factor. It’s important that our data set reflects the structure of our experiment so let’s convert that column to a factor - note that in the following code we are now ‘saving’ the change as we are not mapping the output onto a variable name. my_longer_data %&gt;% mutate(Condition = factor(Condition)) %&gt;% head() ## # A tibble: 6 × 3 ## ID Condition RT ## &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 1 Condition1 487 ## 2 1 Condition2 492 ## 3 1 Condition3 499 ## 4 1 Condition4 488 ## 5 2 Condition1 502 ## 6 2 Condition2 494 The pivot_wider() function We can use the pivot_wider() function to reshape a long data frame so that it goes from long to wide format. It works similarly to pivot_longer(). Let’s take our new, long, data frame and turn it back into wide format. With pivot_wider() we minimally need to specify the data frame that we want to reshape, and a pair or arguments (names_from and values_from) that describe from which column to get the name of the output column, and from which column to get the cell values. my_wider_data &lt;- my_longer_data %&gt;% pivot_wider(names_from = &quot;Condition&quot;, values_from = &quot;RT&quot;) We can check that our data set is back in wide format. head(my_wider_data) ## # A tibble: 6 × 5 ## ID Condition1 Condition2 Condition3 Condition4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 487 492 499 488 ## 2 2 502 494 517 508 ## 3 3 510 483 488 509 ## 4 4 476 488 513 521 ## 5 5 504 478 513 504 ## 6 6 505 486 503 495 3.1.7 Joining Two Datasets Sometimes you might need to combine two datasets. For example, you might have one dataset that contains reading time data (like the one above) and another than contains individual difference measures for the participants in the first dataset. How would we go about combining these two datasets so that we end up with one that includes both the reading time data and the individual difference measures (that perhaps we want to covary out later)? Luckily, the dplyr package contains a number of join functions that allows you to join together different tibbles. First, let’s load the data that contains the individual different measures. individual_diffs &lt;- read_csv(&quot;https://raw.githubusercontent.com/ajstewartlang/03_data_wrangling/master/data/individual_diffs.csv&quot;) Let’s look at the first few rows of the individual differences data. This dataset contains the ID numbers of our participants plus measures of IQ (the iq column) and Working Memory (the wm column). head(individual_diffs) ## # A tibble: 6 × 3 ## ID iq wm ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 100 9 ## 2 2 108 8 ## 3 3 116 9 ## 4 4 95 9 ## 5 5 83 11 ## 6 6 73 10 We want to combine this dataset with our reading time dataset from above my_longer_data which looks like this: head(my_longer_data) ## # A tibble: 6 × 3 ## ID Condition RT ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Condition1 487 ## 2 1 Condition2 492 ## 3 1 Condition3 499 ## 4 1 Condition4 488 ## 5 2 Condition1 502 ## 6 2 Condition2 494 Full Join We can combine using one of the join functions. There are a variety of options including full_join() which includes all of the rows from both tibbles that we want to join. Other options include inner_join() which keeps only the rows in tibble one that have a matching key in tibble 2, as well as left_join() and right_join(). combined_data &lt;- full_join(my_longer_data, individual_diffs, by = &quot;ID&quot;) We now see that our dataset are combined as we’d expect. combined_data ## # A tibble: 128 × 5 ## ID Condition RT iq wm ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Condition1 487 100 9 ## 2 1 Condition2 492 100 9 ## 3 1 Condition3 499 100 9 ## 4 1 Condition4 488 100 9 ## 5 2 Condition1 502 108 8 ## 6 2 Condition2 494 108 8 ## 7 2 Condition3 517 108 8 ## 8 2 Condition4 508 108 8 ## 9 3 Condition1 510 116 9 ## 10 3 Condition2 483 116 9 ## # ℹ 118 more rows Left Join Of course, you may be thinking that we could just do a quick bit of Excel cut and paste of the columns we want from one dataset to the other. But what about the case where our individual differences file contains 10,000 participant IDs (in random order) and we’re only interested in combining the two datasets where there is a match? large_ind_diffs &lt;- read_csv(&quot;https://raw.githubusercontent.com/ajstewartlang/03_data_wrangling/master/data/large_ind_diffs.csv&quot;) head(large_ind_diffs) ## # A tibble: 6 × 3 ## ID iq wm ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 6057 93 7 ## 2 2723 86 7 ## 3 1088 97 9 ## 4 8687 87 8 ## 5 4223 77 11 ## 6 369 95 9 We can actually use another join function (left_join()) to combine these two datasets, but only where there is a match of ID with the first of the two datasets (my_longer_data) in the function call. left_join(my_longer_data, large_ind_diffs, by = &quot;ID&quot;) ## # A tibble: 128 × 5 ## ID Condition RT iq wm ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Condition1 487 100 9 ## 2 1 Condition2 492 100 9 ## 3 1 Condition3 499 100 9 ## 4 1 Condition4 488 100 9 ## 5 2 Condition1 502 108 8 ## 6 2 Condition2 494 108 8 ## 7 2 Condition3 517 108 8 ## 8 2 Condition4 508 108 8 ## 9 3 Condition1 510 116 9 ## 10 3 Condition2 483 116 9 ## # ℹ 118 more rows 3.1.8 Your challenge (complete this before the in-person session) Have a go at recreating the join above using RStudio Desktop. Use read_csv to create the my_wide_data tibble (as shown above) and use pivot_longer to make my my_longer_data. Then use read_csv to create large_ind_diffs and use left_join to combine it with my_longer_data 3.2 Summarising your data We’ll be using the mpg dataset that is built into the tidyverse for this workshop. This dataset contains information about cars (such as engine size, fuel economy) produced by a number of different manufacturers. Once a dataset has been tidied, often one of the first things we want to do is generate summary statistics, e.g. the means and standard deviations for one of our variables grouped by car manufacturer. Have a look at the following video where I walk you through this worksheet. Then I want you to work through the content by writing (and running) the script on your own machine.       Remember to set up a .Rproj file for this workshop before continuing. In your script, you’ll first need to load the tidyverse. library(tidyverse) 3.2.1 Using group_by() and summarise() We are going to use the group_by() function to group the dataset, and then the summarise() function to calculate the mean and sd of the hwy variable. The summarise() function can take a lot of different functions to give us summary statistics. To read more about the different options, type ?summarise in the Console window. Commonly used ones are mean(), median(), sd(). mpg %&gt;% group_by(manufacturer) %&gt;% summarise(mean_hwy = mean(hwy), sd_hwy = sd(hwy), number = n()) ## # A tibble: 15 × 4 ## manufacturer mean_hwy sd_hwy number ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 audi 26.4 2.18 18 ## 2 chevrolet 21.9 5.11 19 ## 3 dodge 17.9 3.57 37 ## 4 ford 19.4 3.33 25 ## 5 honda 32.6 2.55 9 ## 6 hyundai 26.9 2.18 14 ## 7 jeep 17.6 3.25 8 ## 8 land rover 16.5 1.73 4 ## 9 lincoln 17 1 3 ## 10 mercury 18 1.15 4 ## 11 nissan 24.6 5.09 13 ## 12 pontiac 26.4 1.14 5 ## 13 subaru 25.6 1.16 14 ## 14 toyota 24.9 6.17 34 ## 15 volkswagen 29.2 5.32 27 Note that this output is currently ordered alphabetically by the first column manufacturer. What if we wanted to order this out by mean highway fuel economy highest (best) to lowest (worst)? We can use the arrange function. Re-ordering the output with arrange() mpg %&gt;% group_by(manufacturer) %&gt;% summarise(mean_hwy = mean(hwy), sd_hwy = sd(hwy), number = n()) %&gt;% arrange(mean_hwy) ## # A tibble: 15 × 4 ## manufacturer mean_hwy sd_hwy number ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 land rover 16.5 1.73 4 ## 2 lincoln 17 1 3 ## 3 jeep 17.6 3.25 8 ## 4 dodge 17.9 3.57 37 ## 5 mercury 18 1.15 4 ## 6 ford 19.4 3.33 25 ## 7 chevrolet 21.9 5.11 19 ## 8 nissan 24.6 5.09 13 ## 9 toyota 24.9 6.17 34 ## 10 subaru 25.6 1.16 14 ## 11 pontiac 26.4 1.14 5 ## 12 audi 26.4 2.18 18 ## 13 hyundai 26.9 2.18 14 ## 14 volkswagen 29.2 5.32 27 ## 15 honda 32.6 2.55 9 Hmm, so that isn’t what we want - this is going from lowest to highest which is the default in R. We can change that by putting a - sign in from of the parameter we can to order by. mpg %&gt;% group_by(manufacturer) %&gt;% summarise(mean_hwy = mean(hwy), sd_hwy = sd(hwy), number = n()) %&gt;% arrange(-mean_hwy) ## # A tibble: 15 × 4 ## manufacturer mean_hwy sd_hwy number ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 honda 32.6 2.55 9 ## 2 volkswagen 29.2 5.32 27 ## 3 hyundai 26.9 2.18 14 ## 4 audi 26.4 2.18 18 ## 5 pontiac 26.4 1.14 5 ## 6 subaru 25.6 1.16 14 ## 7 toyota 24.9 6.17 34 ## 8 nissan 24.6 5.09 13 ## 9 chevrolet 21.9 5.11 19 ## 10 ford 19.4 3.33 25 ## 11 mercury 18 1.15 4 ## 12 dodge 17.9 3.57 37 ## 13 jeep 17.6 3.25 8 ## 14 lincoln 17 1 3 ## 15 land rover 16.5 1.73 4 This is looking better. The summarise_at() variant As well as using summarise(), you can use related functions such as summarise_at(). This is a scoped version of the summarise() function that can be applied across multiple columns. Note, when using summarise_at() you need to put the columns you want to summarise over in quotes. You also need to provide the summary function - in this case mean. Finally, in case our dataset contains any missing values (indicated by NA), we set the parameter na.rm = TRUE. This will ensure that missing data points are removed before the operation is applied. If we had missing data, but didn’t tell R what we wanted to do with it, it would have thrown an error. mpg %&gt;% group_by(manufacturer) %&gt;% summarise_at(c(&quot;displ&quot;, &quot;cty&quot;, &quot;hwy&quot;), mean, na.rm = TRUE) ## # A tibble: 15 × 4 ## manufacturer displ cty hwy ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 audi 2.54 17.6 26.4 ## 2 chevrolet 5.06 15 21.9 ## 3 dodge 4.38 13.1 17.9 ## 4 ford 4.54 14 19.4 ## 5 honda 1.71 24.4 32.6 ## 6 hyundai 2.43 18.6 26.9 ## 7 jeep 4.58 13.5 17.6 ## 8 land rover 4.3 11.5 16.5 ## 9 lincoln 5.4 11.3 17 ## 10 mercury 4.4 13.2 18 ## 11 nissan 3.27 18.1 24.6 ## 12 pontiac 3.96 17 26.4 ## 13 subaru 2.46 19.3 25.6 ## 14 toyota 2.95 18.5 24.9 ## 15 volkswagen 2.26 20.9 29.2 The summarise_if() variant Imagine we had a really big dataset and wanted to summarise all columns that were of a certain type. We can use the summarise_if() function to work out the mean for each of our car manufactures as follows: mpg %&gt;% group_by(manufacturer) %&gt;% summarise_if(is.numeric, mean, na.rm = TRUE) ## # A tibble: 15 × 6 ## manufacturer displ year cyl cty hwy ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 audi 2.54 2004. 5.22 17.6 26.4 ## 2 chevrolet 5.06 2005. 7.26 15 21.9 ## 3 dodge 4.38 2004. 7.08 13.1 17.9 ## 4 ford 4.54 2003. 7.2 14 19.4 ## 5 honda 1.71 2003 4 24.4 32.6 ## 6 hyundai 2.43 2004. 4.86 18.6 26.9 ## 7 jeep 4.58 2006. 7.25 13.5 17.6 ## 8 land rover 4.3 2004. 8 11.5 16.5 ## 9 lincoln 5.4 2002 8 11.3 17 ## 10 mercury 4.4 2004. 7 13.2 18 ## 11 nissan 3.27 2004. 5.54 18.1 24.6 ## 12 pontiac 3.96 2003. 6.4 17 26.4 ## 13 subaru 2.46 2004. 4 19.3 25.6 ## 14 toyota 2.95 2003. 5.12 18.5 24.9 ## 15 volkswagen 2.26 2003. 4.59 20.9 29.2 The first parameter in summarise_if() is the logical test applied to each column - in this case, if a column is numeric (i.e., a number) - then the test evaluates to TRUE and the second function, mean, is applied. Again, we tell R to ignore missing (NA) data values with the na.rm = TRUE parameter. R functions differ in terms of what arguments they take. I often forget them - if you start typing a function name, you’ll get a little bubble above where you’re typing to remind you what parameters are needed. And if you can’t remember the details, you can just type help(function_name) or ?function_name in the console for any function that you need help with. A lot of data analysis with R (or Python or any other language really) involves a fair bit of Googling. This is normal. There are some things I can never remember and am always having to look up! 3.2.2 Adding columns using mutate() We can add a new column that I’m calling mean_hwy using the mutate() function like this. mpg %&gt;% group_by(manufacturer) %&gt;% mutate(mean_hwy = mean(hwy), sd_hwy = sd(hwy)) ## # A tibble: 234 × 13 ## # Groups: manufacturer [15] ## manufacturer model displ year cyl trans drv cty hwy fl class mean_hwy sd_hwy ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 audi a4 1.8 1999 4 auto(l5) f 18 29 p compact 26.4 2.18 ## 2 audi a4 1.8 1999 4 manual(m5) f 21 29 p compact 26.4 2.18 ## 3 audi a4 2 2008 4 manual(m6) f 20 31 p compact 26.4 2.18 ## 4 audi a4 2 2008 4 auto(av) f 21 30 p compact 26.4 2.18 ## 5 audi a4 2.8 1999 6 auto(l5) f 16 26 p compact 26.4 2.18 ## 6 audi a4 2.8 1999 6 manual(m5) f 18 26 p compact 26.4 2.18 ## 7 audi a4 3.1 2008 6 auto(av) f 18 27 p compact 26.4 2.18 ## 8 audi a4 quattro 1.8 1999 4 manual(m5) 4 18 26 p compact 26.4 2.18 ## 9 audi a4 quattro 1.8 1999 4 auto(l5) 4 16 25 p compact 26.4 2.18 ## 10 audi a4 quattro 2 2008 4 manual(m6) 4 20 28 p compact 26.4 2.18 ## # ℹ 224 more rows We have too many columns to display on this page so we can drop a couple by using the select() function slightly differently. By putting a - sign in front of a column names in select() we end up dropping it. mpg %&gt;% group_by(manufacturer) %&gt;% mutate(mean_hwy = mean(hwy), sd_hwy = sd(hwy)) %&gt;% select(-class, -trans) ## # A tibble: 234 × 11 ## # Groups: manufacturer [15] ## manufacturer model displ year cyl drv cty hwy fl mean_hwy sd_hwy ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 audi a4 1.8 1999 4 f 18 29 p 26.4 2.18 ## 2 audi a4 1.8 1999 4 f 21 29 p 26.4 2.18 ## 3 audi a4 2 2008 4 f 20 31 p 26.4 2.18 ## 4 audi a4 2 2008 4 f 21 30 p 26.4 2.18 ## 5 audi a4 2.8 1999 6 f 16 26 p 26.4 2.18 ## 6 audi a4 2.8 1999 6 f 18 26 p 26.4 2.18 ## 7 audi a4 3.1 2008 6 f 18 27 p 26.4 2.18 ## 8 audi a4 quattro 1.8 1999 4 4 18 26 p 26.4 2.18 ## 9 audi a4 quattro 1.8 1999 4 4 16 25 p 26.4 2.18 ## 10 audi a4 quattro 2 2008 4 4 20 28 p 26.4 2.18 ## # ℹ 224 more rows Note that this doesn’t change the mpg dataset permanently - the changes won’t be saved unless we map the output of this code onto a new variable. Below I am doing this by using the assignment operator &lt;- to map it onto a new variable I’m calling mpg_with_mean. Note that we remove the grouping at the end as we don’t want our grouping rule to remain in our new data frame. mpg_with_mean &lt;- mpg %&gt;% group_by(manufacturer) %&gt;% mutate(mean_hwy = mean(hwy), sd_hyw = sd(hwy)) %&gt;% ungroup() %&gt;% select(-class, -trans) We can then inspect this new variable using head() and str(). head(mpg_with_mean) ## # A tibble: 6 × 11 ## manufacturer model displ year cyl drv cty hwy fl mean_hwy sd_hyw ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 audi a4 1.8 1999 4 f 18 29 p 26.4 2.18 ## 2 audi a4 1.8 1999 4 f 21 29 p 26.4 2.18 ## 3 audi a4 2 2008 4 f 20 31 p 26.4 2.18 ## 4 audi a4 2 2008 4 f 21 30 p 26.4 2.18 ## 5 audi a4 2.8 1999 6 f 16 26 p 26.4 2.18 ## 6 audi a4 2.8 1999 6 f 18 26 p 26.4 2.18 str(mpg_with_mean) ## tibble [234 × 11] (S3: tbl_df/tbl/data.frame) ## $ manufacturer: chr [1:234] &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; ... ## $ model : chr [1:234] &quot;a4&quot; &quot;a4&quot; &quot;a4&quot; &quot;a4&quot; ... ## $ displ : num [1:234] 1.8 1.8 2 2 2.8 2.8 3.1 1.8 1.8 2 ... ## $ year : int [1:234] 1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 ... ## $ cyl : int [1:234] 4 4 4 4 6 6 6 4 4 4 ... ## $ drv : chr [1:234] &quot;f&quot; &quot;f&quot; &quot;f&quot; &quot;f&quot; ... ## $ cty : int [1:234] 18 21 20 21 16 18 18 18 16 20 ... ## $ hwy : int [1:234] 29 29 31 30 26 26 27 26 25 28 ... ## $ fl : chr [1:234] &quot;p&quot; &quot;p&quot; &quot;p&quot; &quot;p&quot; ... ## $ mean_hwy : num [1:234] 26.4 26.4 26.4 26.4 26.4 ... ## $ sd_hyw : num [1:234] 2.18 2.18 2.18 2.18 2.18 ... 3.2.3 Your challenge (do this during the in-person session) The Tidyverse has a number of other built-in data sets. Another one is the starwars data set. You can have a look at it by typing starwars or by typing view(starwars). This second option will open the data set in a new window. Have a go playing around with it. Work out the mean height of humans in the Star Wars universe. There might be some missing data (indicated by NA). You can use the na.rm = TRUE parameter in your summarise() function to ignore these values when generating your summary statistics. Another way to filter out NA values is to use the filter() function in your pipeline. The function is.na() returns a logical value of TRUE of FALSE. The operator ! means NOT, so the expression !is.na(height) will return TRUE when the height value is present, and FALSE if absent. By combining this with filter() we have the line filter(!is.na(height)) which will filter only the cases where we have height data (i.e., !is.na(height) is TRUE). So your code might look like this: starwars %&gt;% filter(!is.na(height)) %&gt;% filter(species == &quot;Human&quot;) %&gt;% summarise(mean_height = mean(height)) Replace the word mean in the summarise() line with median. What other things can you replace it with? Hint: type ?summarise in the console. What other summary information can you extract from this dataset? End of workshop 3 materials "],["data-visualisation.html", "Workshop 4 Data Visualisation 4.1 The Basics of ggplot2 4.2 Your First Visualisation 4.3 Scatterplots 4.4 Plotting Histograms 4.5 The NHANES Dataset 4.6 Your Challenge (do this during the in-person session) 4.7 Additional Resources", " Workshop 4 Data Visualisation Being able to build clear visualisations is key to the successful communication of your data to your intended audience. There are a couple of great recent books focused on data visualisation that I suggest you have a look it. They both provide great perspectives on data visualisations and are full of wonderful examples of different kinds of data visualisations, some of which you’ll learn how to build in this workshop. If you click on the image of the Claus Wilke book, you’ll be taken to the online version of the book (written in R, obviously!)       First I’d like you to watch this brief video where I give some examples of the kinds of data visualisations you can build in R, and why you probably want to avoid building bar graphs.          4.1 The Basics of ggplot2 First we need to load the ggplot2 package. As it is part of the Tidyverse (and we’re likely to be using other Tidyverse packages alongside ggplot2), we load it into our library using the library(tidyverse) line of code. library(tidyverse) In the video, I mention that using ggplot2 requires us to specify some core information in order to build our visualisations. These include the raw data that you want to plot, geometries (or geoms) that are the geometric shapes that will represent the data, and the aesthetics of the geometric and objects, such as color, size, shape and position. 4.2 Your First Visualisation Below is an example where we’re using the mpg dataset (which is a dataset that contains information about cars) to build a visualisation that plots the points corresponding to city fuel economy (cty) on the y-axis and manufacturer on the x-axis. mpg %&gt;% ggplot(aes(x = manufacturer, y = cty)) + geom_point() So, this is not great. The x-axis labels are hard to read, and the individual points don’t look that pleasing. We can use the str_to_title() function to change the manufacturer labels to title case, and adjust the axis labels easily using the theme() function. Note, the + symbol between the lines of ggplot() code is equivalent to the %&gt;% operator. For historical reasons (basically, because ggplot() came before the other packages in the Tidyverse), you need to use the + when adding layers to your ggplot() visualisations. mpg %&gt;% mutate(manufacturer = str_to_title(manufacturer)) %&gt;% ggplot(aes(x = manufacturer, y = cty)) + geom_point() + theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust = .5)) 4.2.1 Improving the Plot So let’s do some more tidying - we’re going to jitter the points slightly (so they’re not stacked vertically) using the geom_jitter() function, and tidy up the axis titles using the labs() function to explicitly add axis labels (rather than just use the labels in our dataset). We’re also adding a few other tweaks - can you spot them? mpg %&gt;% mutate(manufacturer = str_to_title(manufacturer)) %&gt;% ggplot(aes(x = manufacturer, y = cty)) + geom_jitter(width = .2, alpha = .75, size = 2) + theme_minimal() + theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust = .5)) + theme(text = element_text(size = 13)) + labs(title = &quot;City Fuel Economy by Car Manufacturer&quot;, x = &quot;Manufacturer&quot;, y = &quot;City Fuel Economy (mpg)&quot;) It might be helpful for us to add summary statistical information such as the mean fuel economy and confidence intervals around the mean for each car manufacturer. 4.2.2 Adding Summary Statistics We need to add the Hmisc package to allow us to use the stat_summary() function. library(Hmisc) mpg %&gt;% mutate(manufacturer = str_to_title(manufacturer)) %&gt;% ggplot(aes(x = manufacturer, y = cty)) + stat_summary(fun.data = mean_cl_boot, colour = &quot;black&quot;, size = 1) + theme_minimal() + theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust = .5)) + theme(text = element_text(size = 13)) + labs(title = &quot;City Fuel Economy by Car Manufacturer&quot;, x = &quot;Manufacturer&quot;, y = &quot;City Fuel Economy (mpg)&quot;) 4.2.3 The Finished(?) Plot At the moment, the x-axis is ordered alphabetically. How about we order it so that it goes from manufacturer with the hightest mean fuel economy, to the lowest. Also, how about we flip the visualisation so that the axes swap and add a few other tweaks? mpg %&gt;% mutate(manufacturer = str_to_title(manufacturer)) %&gt;% ggplot(aes(x = fct_reorder(manufacturer, .fun = mean, cty), y = cty, colour = manufacturer)) + stat_summary(fun.data = mean_cl_boot, size = 1) + geom_jitter(alpha = .25) + theme_minimal() + theme(text = element_text(size = 13)) + labs(title = &quot;Manufacturer by City Fuel Economy&quot;, x = &quot;Manufacturer&quot;, y = &quot;City Fuel Economy (mpg)&quot;) + guides(colour = &#39;none&#39;) + coord_flip() This looks pretty good. Can you tell what the other bits of code do that I added? Have a go changing some of the numbers to see what happens. You can prevent a line of code being run by adding a # in front of it. So if you need to temporarily not run a line, just add a # rather than delete the line. Plots are rarely completely “finished” as you’ll often think of a minor aesthetic tweak that might make some improvement. 4.2.4 Using facet_wrap() We might think that fuel economy varies as a function of the type of vehicle (e.g., sports cars may be more fuel hungry than midsize cars) and by the number size of engine (e.g., cars with bigger engines may be more fuel hungry). In the visualisation below we’re going to use the facet_wrap() function to build a separate visualisation for each level of the factor we are facetting over (ignoring SUVs). mpg %&gt;% filter(class != &quot;suv&quot;) %&gt;% mutate(class = str_to_title(class)) %&gt;% ggplot(aes(x = displ, y = cty, colour = class)) + geom_jitter(width = .2) + theme_minimal() + theme(text = element_text(size = 13)) + labs(title = &quot;City Fuel Economy by Engine Displacement&quot;, x = &quot;Engine Displacement (litres)&quot;, y = &quot;City Fuel Economy (mpg)&quot;) + guides(colour = &#39;none&#39;) + facet_wrap(~ class) Can you tell what each bit of code is doing? Again, edit the numbers and put a # before lines you want to temporarily ignore to see what happens. 4.3 Scatterplots Above we focused on plotting a numerical variable on one axis, and a categorical variable on the other. There will be cases where we want to create scatterplots, allowing us to plot two numerical variables against each other - possibly to determine whether there might be a relationship between the two. Below we are plotting Engine Displacement on the y-axis, and City Fuel Economy on the x-axis. mpg %&gt;% mutate(class = str_to_upper(class)) %&gt;% ggplot(aes(x = cty, y = displ)) + geom_point(aes(colour = class)) + geom_smooth(se = FALSE) + theme(text = element_text(size = 13)) + theme_minimal() + labs(x = &quot;City Fuel Economy (mpg)&quot;, y = &quot;Engine Displacement (litres)&quot;, colour = &quot;Vehicle Class&quot;) In the above example, we used the geom_smooth() function to add a layer corresponding to fitting a curve to our data. We can see a fairly clear negative correlation between Engine Displacement and Fuel Economy for Fuel Economy values that are less than 25 mpg, but little relationship between the two for values that are great than 25 mpg. These seems to suggest there are some cars with relatively small engines that have great fuel economy, and others with similar engine sizes that have much worse fuel economy. 4.4 Plotting Histograms We might want to plot a histogram of engine sizes (measured in litres and captured in the variable displ in the mpg dataset) to get a feel for how this variable is distributed. mpg %&gt;% ggplot(aes(x = displ)) + geom_histogram(binwidth = .5, fill = &quot;grey&quot;) + labs(title = &quot;Histogram of Engine Displacement&quot;, x = &quot;Engine Displacement (litres)&quot;, y = &quot;Count&quot;) 4.4.1 The ggridges Package Given in a previous visualisation we saw that there seemed to be variability between vehicle classes, wouldn’t it be great if we could compare the distributions of engine size separated by vehicle class? We’re now going to use the ggridges package to do just that… library(ggridges) mpg %&gt;% mutate(class = str_to_title(class)) %&gt;% ggplot(aes(x = displ, y = fct_reorder(class, .fun = mean, displ))) + geom_density_ridges(height = .5, aes(fill = class)) + theme_minimal() + theme(text = element_text(size = 13)) + guides(fill = &#39;none&#39;) + labs(x = &quot;Engine Displacement (litres)&quot;, y = NULL) We can see from the above visualisation that SUVs seem to have quite a lot of variability in engine size while compact cars have relatively little variability. 4.5 The NHANES Dataset We’re now going to visualise aspects of the NHANES dataset.    This is survey data collected by the US National Center for Health Statistics (NCHS) which has conducted a series of health and nutrition surveys since the early 1960’s. Since 1999 approximately 5,000 individuals of all ages are interviewed in their homes every year and complete the health examination component of the survey. The health examination is conducted in a mobile examination centre (MEC). The NHANES target population is “the non-institutionalized civilian resident population of the United States”. NHANES, (American National Health and Nutrition Examination surveys), use complex survey designs (see http://www.cdc.gov/nchs/data/series/sr_02/sr02_162.pdf) that oversample certain subpopulations like racial minorities. Naive analysis of the original NHANES data can lead to mistaken conclusions. The percentages of people from each racial group in the data, for example, are quite different from the way they are in the population.    We need to load the NHANES package as this is where the dataset is contained. library(NHANES) If running the above command generated an error, is it because you haven’t installed the package on your machine with install.packages(\"NHANES\")? First we’re going to explore the NHANES dataset. ncol(NHANES) ## [1] 76 nrow(NHANES) ## [1] 10000 We see there are 76 columns and 10,000 rows. If we use the function head() we can see the first few rows of the dataframe. head(NHANES) ## # A tibble: 6 × 76 ## ID SurveyYr Gender Age AgeDecade AgeMonths Race1 Race3 Education MaritalStatus HHIncome HHIncomeMid Poverty HomeRooms ## &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 51624 2009_10 male 34 &quot; 30-39&quot; 409 White &lt;NA&gt; High Scho… Married 25000-3… 30000 1.36 6 ## 2 51624 2009_10 male 34 &quot; 30-39&quot; 409 White &lt;NA&gt; High Scho… Married 25000-3… 30000 1.36 6 ## 3 51624 2009_10 male 34 &quot; 30-39&quot; 409 White &lt;NA&gt; High Scho… Married 25000-3… 30000 1.36 6 ## 4 51625 2009_10 male 4 &quot; 0-9&quot; 49 Other &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 20000-2… 22500 1.07 9 ## 5 51630 2009_10 female 49 &quot; 40-49&quot; 596 White &lt;NA&gt; Some Coll… LivePartner 35000-4… 40000 1.91 5 ## 6 51638 2009_10 male 9 &quot; 0-9&quot; 115 White &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 75000-9… 87500 1.84 6 ## # ℹ 62 more variables: HomeOwn &lt;fct&gt;, Work &lt;fct&gt;, Weight &lt;dbl&gt;, Length &lt;dbl&gt;, HeadCirc &lt;dbl&gt;, Height &lt;dbl&gt;, BMI &lt;dbl&gt;, ## # BMICatUnder20yrs &lt;fct&gt;, BMI_WHO &lt;fct&gt;, Pulse &lt;int&gt;, BPSysAve &lt;int&gt;, BPDiaAve &lt;int&gt;, BPSys1 &lt;int&gt;, BPDia1 &lt;int&gt;, ## # BPSys2 &lt;int&gt;, BPDia2 &lt;int&gt;, BPSys3 &lt;int&gt;, BPDia3 &lt;int&gt;, Testosterone &lt;dbl&gt;, DirectChol &lt;dbl&gt;, TotChol &lt;dbl&gt;, ## # UrineVol1 &lt;int&gt;, UrineFlow1 &lt;dbl&gt;, UrineVol2 &lt;int&gt;, UrineFlow2 &lt;dbl&gt;, Diabetes &lt;fct&gt;, DiabetesAge &lt;int&gt;, ## # HealthGen &lt;fct&gt;, DaysPhysHlthBad &lt;int&gt;, DaysMentHlthBad &lt;int&gt;, LittleInterest &lt;fct&gt;, Depressed &lt;fct&gt;, ## # nPregnancies &lt;int&gt;, nBabies &lt;int&gt;, Age1stBaby &lt;int&gt;, SleepHrsNight &lt;int&gt;, SleepTrouble &lt;fct&gt;, PhysActive &lt;fct&gt;, ## # PhysActiveDays &lt;int&gt;, TVHrsDay &lt;fct&gt;, CompHrsDay &lt;fct&gt;, TVHrsDayChild &lt;int&gt;, CompHrsDayChild &lt;int&gt;, … 4.5.1 Tidying the Data It looks like some participants appear more than once in the dataset - this could be due to the oversampling mentioned in the description - the first few rows are all for participant ID 51624. We can use the select() function alongwith the n_distinct() function to tell us the unique number of IDs in the dataset. NHANES %&gt;% select(ID) %&gt;% n_distinct() ## [1] 6779 We see we have 6,779 unique individuals. Let’s tidy our data to remove duplicate IDs. Note that below we’re using the pipe operator %&gt;% You can read it as ‘and then’ so it means we’re taking the NHANES dataset and then filtering it keeping just rows with distinct ID numbers. The pipe operator really helps with the readability of your data wrangling code and is an integral part of the tidyverse philosophy - tidy data and tidy code. NHANES_tidied &lt;- NHANES %&gt;% distinct(ID, .keep_all = TRUE) ncol(NHANES_tidied) ## [1] 76 nrow(NHANES_tidied) ## [1] 6779 OK, so our tidied dataset is assigned to the variable NHANES_tidied and has 6,779 rows (but still 76 columns) - as we’d expect given we have 6,779 unique individuals. 4.5.2 Plotting a Histogram Let’s start exploring the data. We have lots of potential variables and relationships to explore. I see we have one labelled Education which is coded as a factor. We also have information related to health such as BMI - first of all lets plot a histogram of BMI. NHANES_tidied %&gt;% ggplot(aes(x = BMI)) + geom_histogram(bins = 100, na.rm = TRUE) We see a pretty right skewed distribution here. Note our use of the na.rm parameter - this parameter appears in many tidyverse functions and by setting it to TRUE we tell R to ignore any parts of our data frame where we have missing data (which is indicated by NA). 4.5.3 Summary Statistics Does BMI vary as a function of Education level? In the code below we are using the data stored in the variable NHANES_tidied, grouping it by Education, then summarising to generate the median BMI for each of our groups. Again, we use the na.rm = TRUE parameter with the summarise() function this time to remove any missing values (NA) from our calculation. NHANES_tidied %&gt;% group_by(Education) %&gt;% summarise(median = median(BMI, na.rm = TRUE)) ## # A tibble: 6 × 2 ## Education median ## &lt;fct&gt; &lt;dbl&gt; ## 1 8th Grade 28.6 ## 2 9 - 11th Grade 28.2 ## 3 High School 28.2 ## 4 Some College 28.4 ## 5 College Grad 26.5 ## 6 &lt;NA&gt; 18.9 So it looks like those with College eduction have the lowest median BMI (ignoring the NA category which corresponds to cases where we don’t have Education level recorded). 4.5.4 geom_violin() Let’s graph it! Note here we’re filtering out cases where we don’t have BMI value recorded. The function is.na() returns TRUE when applied to a case of missing data (NA) - we use the ! operator to negate this and combine several of these expressions together using the logical AND operator &amp;. The line of code below starting with filter() means filter cases where Education is not missing AND BMI is not missing. This means that the NHANES_tidied data that gets passed to the ggplot() call has no missing data for the key variables we’re interested in. I then add a geom_violin() layer to capture the shape of the distribution for each level of Education and geom_boxplot() layer to create a boxplot for each level of our Education factor. The guides(colour = 'none') call suppresses displaying the colour legend - place a # in front of it and rerun the code to see what changes. NHANES_tidied %&gt;% filter(!is.na(Education) &amp; !is.na(BMI)) %&gt;% ggplot(aes(x = Education, y = BMI, colour = Education)) + geom_violin() + geom_jitter(alpha = .2, width = .1) + geom_boxplot(alpha = .5) + guides(colour = &#39;none&#39;) + labs(title = &quot;Examining the effect of education level on BMI&quot;, x = &quot;Education Level&quot;, y = &quot;BMI&quot;) 4.5.5 Plotting Interaction Effects We can also plot how two factors interact with each other. For the plot above, we’ll now add the factor Diabetes (which has two levels - Yes vs. No) to see how that might interact with Education level. To capture the nature of this interaction, we use the expression Education:Diabetes when we specify the x-axis aesthetic. Note, I have rotated the x-axis labels 45 degrees to make them easier to read. NHANES_tidied %&gt;% filter(!is.na(Education) &amp; !is.na(BMI) &amp; !is.na(Diabetes)) %&gt;% ggplot(aes(x = Education:Diabetes, y = BMI, colour = Education)) + geom_violin() + geom_jitter(alpha = .2, width = .1) + geom_boxplot(alpha = .5) + guides(colour = &#39;none&#39;) + theme(axis.text.x = element_text(angle = 45, vjust = 0.5)) + labs(title = &quot;Examining the effect of education level and diabetes on BMI&quot;, x = &quot;Education Level x Diabetes&quot;, y = &quot;BMI&quot;) We can see from the above plot that those with Diabetes seem to also have higher BMI scores for each level of Education. 4.5.6 Histograms with facet_wrap() We can also plot histograms of BMI separately for each Education level - we use the facet_wrap() function to do this. NHANES_tidied %&gt;% filter(!is.na(Education) &amp; !is.na(BMI)) %&gt;% group_by(Education) %&gt;% ggplot(aes(x = BMI, fill = Education)) + geom_histogram() + guides(fill = &#39;none&#39;) + labs(title = &quot;Examining the effect of education level on BMI&quot;, x = &quot;BMI&quot;, y = &quot;Number of cases&quot;) + facet_wrap(~ Education) In the above graph, notice that the same y-axis scale is used for each plot - this makes comparisons a little tricky as there are different numbers of cases for each Eduction level. Add the following scales = \"free\" after Education in the facet_wrap() line. What changes? Instead of generating the histograms using a count, we could generate them using a density function. Let’s also add a density curve. NHANES_tidied %&gt;% filter(!is.na(Education) &amp; !is.na(BMI)) %&gt;% group_by(Education) %&gt;% ggplot(aes(x = BMI, fill = Education)) + geom_histogram(aes(y = ..density..)) + geom_density(aes(y = ..density..)) + guides(fill = &#39;none&#39;) + labs( title = &quot;Examining the effect of education level on BMI&quot;, x = &quot;BMI&quot;, y = &quot;Density&quot;) + facet_wrap(~ Education) 4.6 Your Challenge (do this during the in-person session) Create some new visualisations with either the mpg or NHANES datasets. There are many possible visualisations you could build with either dataset. 4.7 Additional Resources If you are interested in understanding more about ggplot2, you might be interested in watching these two (very) detailed webinars by Thomas Lin Pedersen, one of the developers working on ggplot2 and related packages. End of workshop 4 materials "],["r-markdown.html", "Workshop 5 R Markdown 5.1 Overview 5.2 Your Challenge (do this during the in-person session)", " Workshop 5 R Markdown In this workshop we will see how to generate a report in .html format using R Markdown. Reports written using R Markdown allow you to combine narrative that you’ve written along with R code chunks, and the output associated with those code chunks all in one knitted document. The assignments for this unit need to be produced using R Markdown. 5.1 Overview In the following video I will give you a brief overview of how you can turn a script you have written in R into an R Markdown document that you can ‘knit’ and share with others.       There are many resources available to help you explore the full range of possibilities in R Markdown. A good starting point is the “R Markdown: The Definitive Guide” by Yihui Xie, J. J. Allaire, and Garrett Grolemund. Just click on the image below to be taken to the online version of the book.       You may also be interested in the “R Markdown Cookbook” by Yihui Xie, Christophe Dervieux, and Emily Riederer. Again, just click on the image below to be taken to the online version of the book.       5.2 Your Challenge (do this during the in-person session) Take one of the scripts that you’ve already written - maybe one of the visualisation scripts that you’ve developed - and use it to create an R Markdown document. Add the code from your script to a new R Markdown document in meaningful small code chunks (maybe just a few lines each) - you might have a code chunk that loads your libraries, a chunk that reads in your data, another chunk for wrangling your data, and then separate chunks for each of the visualisations you have written. Before each small chunk of code, add some narrative explaining what the following code chunk does. Tweak the message and warning parameters at the start of each code chunk so that warnings and messages aren’t displayed in the final R Markdown generated .html file. Remember that when you’re knitting an R Markdown document it is working in its own R session - and so can’t access anything in the main R session in which you’ve been writing the R Markdown document itself. This means that your R Markdown must load the libraries needed and read in the data within the document itself. End of workshop 5 materials "],["technical-details-and-credits.html", "Technical Details and Credits", " Technical Details and Credits This course was originally developed and made open access by Andrew Stewart. Updates since 2023 by George Farmer. The workshops and this website were all written using R Markdown and Bookdown. The website is hosted on GitHub. The lecture content is licensed under CC-BY. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
